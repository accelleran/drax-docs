{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Accelleran CU Install Guide \u00b6 Introduction \u00b6 This guide describes the installation of the Accelleran dRAX base, 4G and 5G components, the Effnet DU, Phluido L1 and optionally a core network on a single server machine, however separating the RIC/CU (on a VM) and the DU/L1 (on the server) to increase stability and performances. This first page is the most important page of the install. This first pages holds all the elements needed to do the install in a minimum of time. Duration \u00b6 This install can be done in 1 day up to 1 week depending on the experience of the engineer. This is an estimation done from experience. Why this is the case is not explained here. However a few points listed here without any more details. Slight differences in hardware/firmware of each component. The network the platform resides in. The experience and view points of the engineer itself. Network stability in which the platform resides. Access to the platform. ... Because the install is done manually by an engineer it might be necessary that this engineer needs some support from accelleran. Releases \u00b6 This document is released together with the system release 2022.3.0. This system release contains component version RIC 6.0.0 CU CHART 5.0.0 CU APP R3.3.0_hoegaarden DU 2022-08-26-q2-release-0.4 L1 8.4.2 BNTL650 0.5.2 BNTL550 0.6.0 cell wrapper 1.0.0 During the installation following variables will be used. These are the correct values they are set to for this release. export INSTALL_VERSION = 2022 .3.0 export RIC_VERSION = 6 .0.0 export CU_VERSION = R3.3.0_hoegaarden # build tag export L1_VERSION = v8.7.1 export DU_VERSION = 2022 -08-26-q2-release-0.4 export RU_VERSION = RAN650-2V0.5.2 # shipped with the UNIT. Prerequisites / Preperations \u00b6 This installation guide assumes that that the following are to be taken as prerequisites and made available before proceding further: Hardware: Server with at least the following specifications: Intel Xeon D-1541 or stronger 64-bit processor 64 GB DDR4 RAM 800 GB Hard Disk BIOS settings to most performance mode ( powersaving off, ) VM within the Server, in the same subnet with at least : 8 assigned cores 32 GB assigned RAM 200 GB assigned Disk space NOTE: the VM shall be created using KVM/Virsh, this allows to have easy access to its libvirt XML configuration when needed, ex. to perform the CPU pinning. The User can alternately choose other VM management tools, however without further support from Accelleran. Licenses: A dRAX license file: license.crt A Phluido license key (see the chapter on installing the DU on how to get one) Effnet YubiKey Effnet yubikey license activation file: effnet-license-activation-2022-07-01.zip an active github account that has been enabled to access the necessary software images on accelleran github repository 4G Only: A server certificate: server.crt (see the chapter on installing dRax on how to get one) 4G Only: A CA certificate: ca.crt (see the chapter on installing dRax on how to get one) Software: Ubuntu Server 20.04 OS both on the VM and on the Server ( ubuntu-20.04.4-live-server-amd64.iso ) Effnet DU: accelleran-du-phluido-%Y-%m-%d-pre-release.zip Phluido L1: phluido_docker_xxxxx.tar effnet-license-activation-%Y-%m-%d.zip sysTest executable Linux Configuration: Linux bridge br0 virsh installed 5G configuration : plmn_identity [ eg 235 88 ] nr_cell_identity [ eg 1 any number ] nr_pci [ eg 1 not any number. Make sure to do the correct PCI planning in case of multiple cells. ] 5gs_tac [ eg 1 ] center_frequency_band [ eg 3751.680 ] point_a_arfcn [ eg 648840 consistent with center freq, scs 30khz ] band [ eg 77 consistent with center frequency ] NOTE: while taking almost no active time to obtain the Phluido license code and the Effnet activation bundle, in order to do so we need to contact our technical partners and this may require up to a couple of working days so it is recommended to take the necessary actions to complete these steps first of all. Similarly, we must enable your dockerhub account to access and download the Accelleran software images, this also takes some time and can be done upfront Know the ip addresses, interfaces, user account \u00b6 Make sure Ubuntu (Server) 20.04 is installed as said both on the physical server and on the virtual machine and that both have access to the internet. They both must have a static IP address on a fixed port, in the same subnet This guide will refer to the VM static IP address as $NODE_IP and the interface it belongs to as $NODE_INT , and to $SERVER_IP for the server static IP address. Furthermore this guide will refer to the IP address of the gateway as $GATEWAY_IP , the IP address of the core (see the section on Core Installation ) as $CORE_IP and the IP address of the CU (see the section on DU Installation ) as $CU_IP . In order to be able to execute the commands in this guide as-is you should add these variables to the environment as soon as they are known. Alternatively you can edit the configurations to set the correct IP addresses. NOTE : All IP's need to be in the same subnet. The ip's and values used in the variables depicted below are example values. NOTE : the USER of the CU VM and the bare metal HOST is assumed to be the same. export NODE_IP = 192 .168.88.4 # replace 192.168.88.4 by the IP address of the node. ( The IP of the eth0 in the CU VM ) export NODE_SUBNET = 192 .168.88.0/24 # the subnet that contains the $NODE_IP export NODE_INT = br0 # he name of the network interface that has IP $NODE_IP export SERVER_IP = 192 .168.88.3 # The IP address of the linux bridge ( br0 ) export SERVER_INT = eno # The physical interface the server connects to the LAN. export GATEWAY_IP = 192 .168.88.1 # replace 192.168.88.1 by the IP address of the gateway export CORE_IP = 192 .168.88.5 # replace 192.168.88.5 by the IP address of the core export E1_CU_IP = 192 .168.88.170 # E1 ip address the CU listens on. Good practice to take the second last in the LOADBALANCER_IP_RANGE and anding with an even byte. export F1_CU_IP = 192 .168.88.171 # F1 ip address the CU listens on. Good practice to take the last in the LOADBALANCER_IP_RANGE and ending with an odd byte. export LOADBALANCER_IP_RANGE = 192 .168.88.160-192.168.88.171 export USER = sj5g # username to log into linux of HOST SERVER and CU VM export CU_HOSTNAME = cu-cab3 # the hostname the CU VM will get. export CU_VM_NAME = cu-cab3 # the hostname the CU VM will get. export OPEN5GS_HOSTNAME = open5gs-cab3 # the hostname the CU VM will get. export OPEN5GS_VM_NAME = open5gs-cab3 # the hostname the oCU VM will get. export L1_PHLUIDO_KEY = \"xxxx.xxxx.xxxx.xxxx.xxxx\" export POD_NETWORK = \"10.244.0.0/16\" In case a Benetel650 RU connected to the server with a fiber export SERVER_RU_INT = enp1s0f0 # interface of the server to the RU. Fiber interface. export MAC_DU = 11 :22:33:44:55:66 # mac of the server interface to RU. export MAC_RU = aa:bb:cc:dd:ee:ff # mac of the RU for ip 10.0.0.2. Use tcpdump to find. Kubernetes namespaces, listed here for completeness. We will use default. export NS_DRAX = default export NS_4G_CU = default export NS_5G_CU = default Docker hub account. Request these 3 values to Accelleran. export DOCKER_USER = export DOCKER_PASS = export DOCKER_EMAIL = In order to perform many of the commands in this installation manual you need root privileges. Whenever a command has to be executed with root privileges it will be prefixed with sudo . Know the 5G configuration \u00b6 export PLMN_ID = 001f01 export PCI_ID = 201 export ARFCN_POINT_A = 662664 export FREQ_BAND = 77 export FREQ_CENTER = NOTE : for multiple cells bare in mind a correct PCI ID planning. know which cores and cpu you will be using. \u00b6 Depending on the server you will be using assign the cores to the DU and CU. IMPORTANT : only comma seperated list is allowed. ( virt-install will be using it ) In case of dual CPU \u00b6 ubuntu@bbu3:~$ numactl --hardware available: 2 nodes ( 0 -1 ) node 0 cpus: 0 2 4 6 8 10 12 14 node 0 size: 64037 MB node 0 free: 593 MB node 1 cpus: 1 3 5 7 9 11 13 15 node 1 size: 64509 MB node 1 free: 138 MB node distances: node 0 1 0 : 10 21 1 : 21 10 assign all cores of 1 CPU to DU. The cores of the other CPU to CU VM). export CORE_SET_DU = 0 ,2,4,6,8,10,12,14 export CORE_SET_CU = 1 ,3,5,7,9,11,13,15 export CORE_AMOUNT_CU = 8 In case of 1 CPU server \u00b6 $ numactl --hardware available: 1 nodes ( 0 ) node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 node 0 size: 31952 MB node 0 free: 4294 MB node distances: node 0 0 : 10 Assign first half to DU and last half to CU export CORE_SET_DU = 0 ,1,2,3,4,5,6,7,8,9 export CORE_SET_CU = 10 ,11,12,13,14,15,16,17,18,19 export CORE_AMOUNT_CU = 10 Prepare install directory and scripts \u00b6 We'll create an install directory in which we'll be working through this whole install procedure. cd ; mkdir -p ~/install/ ; cd $_ Get scripts \u00b6 cd ~/install git clone --branch $INSTALL_VERSION https://github.com/accelleran/drax-install Fill in variable values. \u00b6 All the variables mentioned above can be found in the file vi ~/install/drax-install/install-vars.sh Update this file with the correct values. They values have been prepared in the section above. Making variables boot safe \u00b6 After having done that run the install.sh script to make the values available in your shell. They values are boot safe so you only have to run them once. . ~/install/drax-install/install.sh Verify the variables \u00b6 log out and in again in this server and try env | grep VERSION The variables will show containing the VERSION info. network components overview \u00b6 Here a simplified diagram of all network components and the related ip addresses. Before you continue installing fill in this simplified drawing with the ip address that apply for the configuration. NOTE : the CORE needs to be able to reach the GTP-0 and GTP-1 ips. In this example they are in the same subnet. NOTE : subnet in below example is 255.255.255.0 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 server \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 VM CORE \u2502 \u2502 \u2502 \u2502 CORE_IP 192.168.88.5 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 enp1s0 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 internet access \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 GATEWAY_IP 192.168.88.1 \u2502 \u25bc \u2502 \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 eno1 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba ( linux bridge br0 192.168.88.3 ) \u2502 \u2502 \u25b2 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 eth0 \u2502 \u2502 \u2502 \u2502 VM CU \u2502 \u2502 \u2502 \u2502 NODE_IP=192.168.88.4 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 E1 F1 GTP-0 GTP-1 \u2502 \u2502 \u2502 \u2502 E1_CU_IP F1_CU_IP \u2502 \u2502 \u2502 \u2502192.168.88.170 192.168.88.171 192.168.88.172 192.168.88.173 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502DU effnet container \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502L1 phluido container\u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 enp1s0f0 10.10.0.1/24 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 fiber \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 RU eth0 10.10.0.100/24 mgmt \u2502 \u2502 10.10.0.2/24 traffic\u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Steps to take \u00b6 The installation process is divided in a number of steps. Each of these steps is described in its own chapter. It is recommended to execute these steps in the following order as there are dependencies from one chapter to another. Optional: Open5GS Installation Kubernetes Installation dRax Installation DU Installation","title":"Accelleran CU Install Guide"},{"location":"#accelleran-cu-install-guide","text":"","title":"Accelleran CU Install Guide"},{"location":"#introduction","text":"This guide describes the installation of the Accelleran dRAX base, 4G and 5G components, the Effnet DU, Phluido L1 and optionally a core network on a single server machine, however separating the RIC/CU (on a VM) and the DU/L1 (on the server) to increase stability and performances. This first page is the most important page of the install. This first pages holds all the elements needed to do the install in a minimum of time.","title":"Introduction"},{"location":"#duration","text":"This install can be done in 1 day up to 1 week depending on the experience of the engineer. This is an estimation done from experience. Why this is the case is not explained here. However a few points listed here without any more details. Slight differences in hardware/firmware of each component. The network the platform resides in. The experience and view points of the engineer itself. Network stability in which the platform resides. Access to the platform. ... Because the install is done manually by an engineer it might be necessary that this engineer needs some support from accelleran.","title":"Duration"},{"location":"#releases","text":"This document is released together with the system release 2022.3.0. This system release contains component version RIC 6.0.0 CU CHART 5.0.0 CU APP R3.3.0_hoegaarden DU 2022-08-26-q2-release-0.4 L1 8.4.2 BNTL650 0.5.2 BNTL550 0.6.0 cell wrapper 1.0.0 During the installation following variables will be used. These are the correct values they are set to for this release. export INSTALL_VERSION = 2022 .3.0 export RIC_VERSION = 6 .0.0 export CU_VERSION = R3.3.0_hoegaarden # build tag export L1_VERSION = v8.7.1 export DU_VERSION = 2022 -08-26-q2-release-0.4 export RU_VERSION = RAN650-2V0.5.2 # shipped with the UNIT.","title":"Releases"},{"location":"#prerequisites-preperations","text":"This installation guide assumes that that the following are to be taken as prerequisites and made available before proceding further: Hardware: Server with at least the following specifications: Intel Xeon D-1541 or stronger 64-bit processor 64 GB DDR4 RAM 800 GB Hard Disk BIOS settings to most performance mode ( powersaving off, ) VM within the Server, in the same subnet with at least : 8 assigned cores 32 GB assigned RAM 200 GB assigned Disk space NOTE: the VM shall be created using KVM/Virsh, this allows to have easy access to its libvirt XML configuration when needed, ex. to perform the CPU pinning. The User can alternately choose other VM management tools, however without further support from Accelleran. Licenses: A dRAX license file: license.crt A Phluido license key (see the chapter on installing the DU on how to get one) Effnet YubiKey Effnet yubikey license activation file: effnet-license-activation-2022-07-01.zip an active github account that has been enabled to access the necessary software images on accelleran github repository 4G Only: A server certificate: server.crt (see the chapter on installing dRax on how to get one) 4G Only: A CA certificate: ca.crt (see the chapter on installing dRax on how to get one) Software: Ubuntu Server 20.04 OS both on the VM and on the Server ( ubuntu-20.04.4-live-server-amd64.iso ) Effnet DU: accelleran-du-phluido-%Y-%m-%d-pre-release.zip Phluido L1: phluido_docker_xxxxx.tar effnet-license-activation-%Y-%m-%d.zip sysTest executable Linux Configuration: Linux bridge br0 virsh installed 5G configuration : plmn_identity [ eg 235 88 ] nr_cell_identity [ eg 1 any number ] nr_pci [ eg 1 not any number. Make sure to do the correct PCI planning in case of multiple cells. ] 5gs_tac [ eg 1 ] center_frequency_band [ eg 3751.680 ] point_a_arfcn [ eg 648840 consistent with center freq, scs 30khz ] band [ eg 77 consistent with center frequency ] NOTE: while taking almost no active time to obtain the Phluido license code and the Effnet activation bundle, in order to do so we need to contact our technical partners and this may require up to a couple of working days so it is recommended to take the necessary actions to complete these steps first of all. Similarly, we must enable your dockerhub account to access and download the Accelleran software images, this also takes some time and can be done upfront","title":"Prerequisites / Preperations"},{"location":"#know-the-ip-addresses-interfaces-user-account","text":"Make sure Ubuntu (Server) 20.04 is installed as said both on the physical server and on the virtual machine and that both have access to the internet. They both must have a static IP address on a fixed port, in the same subnet This guide will refer to the VM static IP address as $NODE_IP and the interface it belongs to as $NODE_INT , and to $SERVER_IP for the server static IP address. Furthermore this guide will refer to the IP address of the gateway as $GATEWAY_IP , the IP address of the core (see the section on Core Installation ) as $CORE_IP and the IP address of the CU (see the section on DU Installation ) as $CU_IP . In order to be able to execute the commands in this guide as-is you should add these variables to the environment as soon as they are known. Alternatively you can edit the configurations to set the correct IP addresses. NOTE : All IP's need to be in the same subnet. The ip's and values used in the variables depicted below are example values. NOTE : the USER of the CU VM and the bare metal HOST is assumed to be the same. export NODE_IP = 192 .168.88.4 # replace 192.168.88.4 by the IP address of the node. ( The IP of the eth0 in the CU VM ) export NODE_SUBNET = 192 .168.88.0/24 # the subnet that contains the $NODE_IP export NODE_INT = br0 # he name of the network interface that has IP $NODE_IP export SERVER_IP = 192 .168.88.3 # The IP address of the linux bridge ( br0 ) export SERVER_INT = eno # The physical interface the server connects to the LAN. export GATEWAY_IP = 192 .168.88.1 # replace 192.168.88.1 by the IP address of the gateway export CORE_IP = 192 .168.88.5 # replace 192.168.88.5 by the IP address of the core export E1_CU_IP = 192 .168.88.170 # E1 ip address the CU listens on. Good practice to take the second last in the LOADBALANCER_IP_RANGE and anding with an even byte. export F1_CU_IP = 192 .168.88.171 # F1 ip address the CU listens on. Good practice to take the last in the LOADBALANCER_IP_RANGE and ending with an odd byte. export LOADBALANCER_IP_RANGE = 192 .168.88.160-192.168.88.171 export USER = sj5g # username to log into linux of HOST SERVER and CU VM export CU_HOSTNAME = cu-cab3 # the hostname the CU VM will get. export CU_VM_NAME = cu-cab3 # the hostname the CU VM will get. export OPEN5GS_HOSTNAME = open5gs-cab3 # the hostname the CU VM will get. export OPEN5GS_VM_NAME = open5gs-cab3 # the hostname the oCU VM will get. export L1_PHLUIDO_KEY = \"xxxx.xxxx.xxxx.xxxx.xxxx\" export POD_NETWORK = \"10.244.0.0/16\" In case a Benetel650 RU connected to the server with a fiber export SERVER_RU_INT = enp1s0f0 # interface of the server to the RU. Fiber interface. export MAC_DU = 11 :22:33:44:55:66 # mac of the server interface to RU. export MAC_RU = aa:bb:cc:dd:ee:ff # mac of the RU for ip 10.0.0.2. Use tcpdump to find. Kubernetes namespaces, listed here for completeness. We will use default. export NS_DRAX = default export NS_4G_CU = default export NS_5G_CU = default Docker hub account. Request these 3 values to Accelleran. export DOCKER_USER = export DOCKER_PASS = export DOCKER_EMAIL = In order to perform many of the commands in this installation manual you need root privileges. Whenever a command has to be executed with root privileges it will be prefixed with sudo .","title":"Know the ip addresses, interfaces, user account"},{"location":"#know-the-5g-configuration","text":"export PLMN_ID = 001f01 export PCI_ID = 201 export ARFCN_POINT_A = 662664 export FREQ_BAND = 77 export FREQ_CENTER = NOTE : for multiple cells bare in mind a correct PCI ID planning.","title":"Know the 5G configuration"},{"location":"#know-which-cores-and-cpu-you-will-be-using","text":"Depending on the server you will be using assign the cores to the DU and CU. IMPORTANT : only comma seperated list is allowed. ( virt-install will be using it )","title":"know which cores and cpu you will be using."},{"location":"#in-case-of-dual-cpu","text":"ubuntu@bbu3:~$ numactl --hardware available: 2 nodes ( 0 -1 ) node 0 cpus: 0 2 4 6 8 10 12 14 node 0 size: 64037 MB node 0 free: 593 MB node 1 cpus: 1 3 5 7 9 11 13 15 node 1 size: 64509 MB node 1 free: 138 MB node distances: node 0 1 0 : 10 21 1 : 21 10 assign all cores of 1 CPU to DU. The cores of the other CPU to CU VM). export CORE_SET_DU = 0 ,2,4,6,8,10,12,14 export CORE_SET_CU = 1 ,3,5,7,9,11,13,15 export CORE_AMOUNT_CU = 8","title":"In case of dual CPU"},{"location":"#in-case-of-1-cpu-server","text":"$ numactl --hardware available: 1 nodes ( 0 ) node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 node 0 size: 31952 MB node 0 free: 4294 MB node distances: node 0 0 : 10 Assign first half to DU and last half to CU export CORE_SET_DU = 0 ,1,2,3,4,5,6,7,8,9 export CORE_SET_CU = 10 ,11,12,13,14,15,16,17,18,19 export CORE_AMOUNT_CU = 10","title":"In case of 1 CPU server"},{"location":"#prepare-install-directory-and-scripts","text":"We'll create an install directory in which we'll be working through this whole install procedure. cd ; mkdir -p ~/install/ ; cd $_","title":"Prepare install directory and scripts"},{"location":"#get-scripts","text":"cd ~/install git clone --branch $INSTALL_VERSION https://github.com/accelleran/drax-install","title":"Get scripts"},{"location":"#fill-in-variable-values","text":"All the variables mentioned above can be found in the file vi ~/install/drax-install/install-vars.sh Update this file with the correct values. They values have been prepared in the section above.","title":"Fill in variable values."},{"location":"#making-variables-boot-safe","text":"After having done that run the install.sh script to make the values available in your shell. They values are boot safe so you only have to run them once. . ~/install/drax-install/install.sh","title":"Making variables boot safe"},{"location":"#verify-the-variables","text":"log out and in again in this server and try env | grep VERSION The variables will show containing the VERSION info.","title":"Verify the variables"},{"location":"#network-components-overview","text":"Here a simplified diagram of all network components and the related ip addresses. Before you continue installing fill in this simplified drawing with the ip address that apply for the configuration. NOTE : the CORE needs to be able to reach the GTP-0 and GTP-1 ips. In this example they are in the same subnet. NOTE : subnet in below example is 255.255.255.0 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 server \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 VM CORE \u2502 \u2502 \u2502 \u2502 CORE_IP 192.168.88.5 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 enp1s0 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 internet access \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 GATEWAY_IP 192.168.88.1 \u2502 \u25bc \u2502 \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 eno1 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba ( linux bridge br0 192.168.88.3 ) \u2502 \u2502 \u25b2 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 eth0 \u2502 \u2502 \u2502 \u2502 VM CU \u2502 \u2502 \u2502 \u2502 NODE_IP=192.168.88.4 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 E1 F1 GTP-0 GTP-1 \u2502 \u2502 \u2502 \u2502 E1_CU_IP F1_CU_IP \u2502 \u2502 \u2502 \u2502192.168.88.170 192.168.88.171 192.168.88.172 192.168.88.173 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502DU effnet container \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502L1 phluido container\u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 enp1s0f0 10.10.0.1/24 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 fiber \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 RU eth0 10.10.0.100/24 mgmt \u2502 \u2502 10.10.0.2/24 traffic\u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"network components overview"},{"location":"#steps-to-take","text":"The installation process is divided in a number of steps. Each of these steps is described in its own chapter. It is recommended to execute these steps in the following order as there are dependencies from one chapter to another. Optional: Open5GS Installation Kubernetes Installation dRax Installation DU Installation","title":"Steps to take"},{"location":"publish/","text":"How to publish \u00b6 site with documentation https://squidfunk.github.io/mkdocs-material/setup/changing-the-logo-and-icons/ How to install mkdocs \u00b6 on any server clone the repo containing the .md files git clone https://github.com/accelleran/drax-docs install the tools pip3 install mkdocs-material pip3 install mike pip3 install mkdocs-section-index create a new site cd drax-docs mkdocs new site Then start the webservice on the ip address of this server. In our example 10.22.11.147 mkdocs serve -a 10.22.11.147:8000 now you can browse using http:10.22.11.147:8000","title":"How to publish"},{"location":"publish/#how-to-publish","text":"site with documentation https://squidfunk.github.io/mkdocs-material/setup/changing-the-logo-and-icons/","title":"How to publish"},{"location":"publish/#how-to-install-mkdocs","text":"on any server clone the repo containing the .md files git clone https://github.com/accelleran/drax-docs install the tools pip3 install mkdocs-material pip3 install mike pip3 install mkdocs-section-index create a new site cd drax-docs mkdocs new site Then start the webservice on the ip address of this server. In our example 10.22.11.147 mkdocs serve -a 10.22.11.147:8000 now you can browse using http:10.22.11.147:8000","title":"How to install mkdocs"},{"location":"benetel650-install/","text":"Summary \u00b6 Drawing \u00b6 10.10.0.100:ssh +-------------+ | | | | +-----------+ +-----------+ | | | | | | | RRU +----fiber----+ L1 | | DU | | | | | | | | | +-----------+ +-----------+ | | +-------------+ aa:bb:cc:dd:ee:ff 11:22:33:44:55:66 10.10.0.2:44000 10.10.0.1:44000 eth0 enp45s0f0 port FIBER1 Configure the DU ( under construction ) \u00b6 This chapter on only focuses on what to configure to get the Benetel650 working. The complete installation of the DU can be found in the du-install doc itself. Other then the B210, benetel uses 2 software components. 2 Containers, a effnet du and phluido l1. The phluido rru is not needed. This component resides in the benetel equipment. adapt the PhluidoL1_NR_Benetel.cfg file delivered by effnet Make sure to set the value LicenseKey option to the received Phluido license key: to be tested which .cfg to use Create a configuration file for the Effnet DU: to be tested which .json to use Before creating the docker-compose.yml file, make sure to set the $CU_IP environment variable. This IP address can be determined by executing the following command. The CU address is the second IP address. This IP address should be in the IP pool that was assigned to MetalLB in dRax Installation . kubectl get services | grep 'acc-5g-cu-cp-.*-sctp-f1' Now, create a docker-compose configuration file: tee accelleran-du-phluido/accelleran-du-phluido-2021-06-30/docker-compose.yml <<EOF version: \"3\" services: phluido_l1: image: phluido_l1 container_name: phluido_l1_cn tty: true privileged: true ipc: shareable shm_size: 2gb command: /config.cfg volumes: - \"$PWD/phluido/PhluidoL1_NR_B210.cfg:/config.cfg:ro\" - \"/run/logs-du/l1:/workdir\" - \"/etc/machine-id:/etc/machine-id:ro\" working_dir: \"/workdir\" network_mode: host du: image: gnb_du_main_phluido volumes: - \"$PWD/b210_config_20mhz.json:/config.json:ro\" - \"/run/logs-du/du:/workdir\" - /run/pcscd/pcscd.comm:/run/pcscd/pcscd.comm ipc: container:phluido_l1_cn tty: true cap_add: - CAP_SYS_NICE - CAP_IPC_LOCK depends_on: - phluido_l1 entrypoint: [\"/bin/sh\", \"-c\", \"sleep 4 && exec /gnb_du_main_phluido /config.json\"] working_dir: \"/workdir\" extra_hosts: - \"cu:$F1_CU_IP\" EOF Prepare the server for the Benetel 650 \u00b6 The benetel is connected with a fiber to the server. 1. The port on the RRU is labeled port FIBER1 2. The port on the server is one of these listed below. :ad@5GCN:~$ lshw | grep SFP -C 5 WARNING: you should run this program as super-user. capabilities: pci normal_decode bus_master cap_list configuration: driver = pcieport resources: irq:29 ioport:f000 ( size = 4096 ) memory:f8000000-f86fffff *-network:0 DISABLED description: Ethernet interface product: 82599ES 10 -Gigabit SFI/SFP+ Network Connection vendor: Intel Corporation physical id: 0 bus info: pci@0000:2d:00.0 logical name: enp45s0f0 version: 01 -- capabilities: bus_master cap_list rom ethernet physical fibre 10000bt-fd configuration: autonegotiation = off broadcast = yes driver = ixgbe driverversion = 5 .1.0-k firmware = 0x2b2c0001 latency = 0 link = no multicast = yes resources: irq:202 memory:f8000000-f807ffff ioport:f020 ( size = 32 ) memory:f8200000-f8203fff memory:f8080000-f80fffff memory:f8204000-f8303fff memory:f8304000-f8403fff *-network:1 DISABLED description: Ethernet interface product: 82599ES 10 -Gigabit SFI/SFP+ Network Connection vendor: Intel Corporation physical id: 0 .1 bus info: pci@0000:2d:00.1 logical name: enp45s0f1 version: 01 by setting both network devices to UP you find out which one is connected. In our case its enp45s0f0. This port is the one we connected the fiber with. :ad@5GCN:~$ sudo ip link set dev enp45s0f0 up :ad@5GCN:~$ sudo ip link set dev enp45s0f1 up :ad@5GCN:~$ ip -br a : enp45s0f0 UP fe80::6eb3:11ff:fe08:a4e0/64 enp45s0f1 DOWN : configuring the static ip of enp45s0f0 is done via netplan. add this part to /etc/netplan/50-cloud-init.yaml . In some installation it might be anot network: ethernets: enp45s0f0: dhcp4: false dhcp6: false optional: true addresses: - 10 .10.0.1/24 mtu: 9000 To apply this configuration you can use sudo netplan apply This needs to be the result $ ip -br a | grep enp45 enp45s0f0 UP 10 .10.0.1/24 fe80::6eb3:11ff:fe08:a4e0/64 enp45s0f1 DOWN The ip benetel is configured with is 10.10.0.100 . This is the MGMT ip. We can ssh to it. We found out using nmap this way. $ nmap 10 .10.0.0/24 Starting Nmap 7 .60 ( https://nmap.org ) at 2021 -09-21 10 :15 CEST Nmap scan report for 10 .10.0.1 Host is up ( 0 .000040s latency ) . Not shown: 996 closed ports PORT STATE SERVICE 22 /tcp open ssh 111 /tcp open rpcbind 5900 /tcp open vnc 9100 /tcp open jetdirect Nmap scan report for 10 .10.0.100 Host is up ( 0 .0053s latency ) . Not shown: 998 closed ports PORT STATE SERVICE 22 /tcp open ssh 111 /tcp open rpcbind Nmap done : 256 IP addresses ( 2 hosts up ) scanned in 3 .10 seconds A route is added also in the route table automatically $ route -n | grep 10 .10.0.0 10 .10.0.0 0 .0.0.0 255 .255.255.0 U 0 0 0 enp45s0f0 ```` now you can ssh to the benetel ``` bash $ ssh root@10.10.0.100 Last login: Fri Feb 7 16 :45:59 2020 from 10 .10.0.1 root@benetelru:~# ls -l -rwxrwxrwx 1 root root 1572 Sep 10 2021 DPLL3_1PPS_REGISTER_PATCH.txt drwxrwxrwx 2 root root 0 Feb 7 16 :44 adrv9025 -rwxrwxrwx 1 root root 1444 Feb 7 16 :40 dev_struct.dat -rwxrwxrwx 1 root root 17370 Sep 10 2021 dpdModelReadback.txt -rwxrwxrwx 1 root root 5070 Feb 7 17 :00 dpdModelcoefficients.txt -rwxrwxrwx 1 root root 24036 Sep 10 2021 eeprog_cp60 -rwxrwxrwx 1 root root 1825062 Feb 7 15 :58 madura_log_file.txt -rw------- 1 root root 1230 Feb 7 2020 nohup.out -rwxr-xr-x 1 root root 57 Feb 7 2020 nohup_handshake -rwxrwxrwx 1 root root 571 Feb 7 2020 progBenetelDuMAC_CATB -rwxr-xr-x 1 root root 1121056 Feb 7 16 :24 quickRadioControl -rwxrwxrwx 1 root root 1151488 Sep 10 2021 radiocontrol_prv-nk-cliupdate -rwxrwxrwx 1 root root 22904 Aug 24 2021 registercontrol -rwxrwxrwx 1 root root 164 Feb 7 16 :35 removeResetRU_CATB -rwxrwxrwx 1 root root 163 Feb 7 2020 reportRuStatus -rwxrwxrwx 1 root root 162 Feb 7 16 :35 resetRU_CATB -rwxr-xr-x 1 root root 48 Feb 7 15 :57 runSync -rwxrwxrwx 1 root root 21848 Sep 10 2021 smuconfig -rwxrwxrwx 1 root root 17516 Sep 10 2021 statmon -rwxrwxrwx 1 root root 23248 Sep 10 2021 syncmon -rwxr-xr-x 1 root root 182 Feb 7 16 :41 trialHandshake root@benetelru:~# The mac address of the benetel data interface can be found like this to be done Add mac entry script in routable.d. $ cat /etc/networkd-dispatcher/routable.d/macs.sh #!/bin/sh sudo arp -s 10.10.0.2 aa:bb:cc:dd:ee:ff -i enp45s0f0 chmod 777 /etc/networkd-dispatcher/routable.d/macs.sh Benetel650 does not answer arp requests. With this apr entry in the arp table the server knows to which mac address it needs to sent the ip packet to. The ip packet towards the RRU with ip 10.10.0.2. Test the script by running it and checking the arp -a table like this $ arp -a | grep 10.10.0.2 ? (10.10.0.2) at 02:00:5e:01:01:01 [ether] PERM on enp45s0f0 Version Check \u00b6 finding out the version and commit hash of the benetel650 commit hash root@benetelru:~# registercontrol -v Lightweight HPS-to-FPGA Control Program Version : V1.2.0 ****BENETEL PRODUCT VERSIONING BLOCK**** This Build Was Created Locally. Please Use Git Pipeline! Project ID NUMBER: 0 Git # Number: f6366d7adf84933ab2b242a345bd63c07fedb9e5 Build ID: 0 Version Number: 0.0.1 Build Date: 2/12/2021 Build Time H:M:S: 18:20:3 ****BENETEL PRODUCT VERSIONING BLOCK END**** The version which is referred to. This is version 0.4. Depending on the version different configuration commands apply. root@benetelru:~# cat /etc/benetel-rootfs-version RAN650-2V0.4 Configure the RRU release V0.3 \u00b6 Set DU mac address for version V0.3 \u00b6 Inside the file /etc/radio_init.sh we program the mac. Example for MAC address 00:1E:67:FD:F5:51 you will find in the file: registercontrol -w c0315 -x 0x67FDF551 >> /home/root/radio_boot_response registercontrol -w c0316 -x 0x001E >> /home/root/radio_boot_response echo \"Configure the MAC address of the O-DU: 00:1E:67:FD:F5:51 \" >> /home/root/radio_status Make sure to edit those as MAC address of the fiber port. Reboot the BNTL650 It is possible to run the commands manually to speed up things. registercontrol -w c0315 -x 0x67FDF551 >> /home/root/radio_boot_response registercontrol -w c0316 -x 0x001E >> /home/root/radio_boot_response Set the Frequency for version V0.3 \u00b6 This file /etc/systemd/system/multi-user.target.wants/autoconfig.service is called during boot that sets the frequency. change the frequency here. [Service] ExecStart =/bin/sh /etc/radio_init.sh 3751.680 Example for frequency 3751.68MHz (ARFCN=650112) you will find in the file: Make sure to edit the pointA frequency ARFCN value in the DU config (in this example PointA_ARFCN=648840). Reboot the Benetel. Configure the RRU release V0.4 \u00b6 Set DU mac address in the RRU \u00b6 Create this script to program the mac address of the DU inside the RRU. Remember the RRU does not request arp, so we have to manually configure that. root@benetelru:~# cat progDuMAC-5GCN-enp45s0f0 # 11:22:33:44:55:66 5GCN-itf registercontrol -w 0xC036B -x 0x88000088 # don't touch file eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1A:0x01:0x11 # first byte of mac address is 0x11 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1B:0x01:0x22 # etc ... eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1C:0x01:0x33 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1D:0x01:0x44 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1E:0x01:0x55 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1F:0x01:0x66 Set the Frequency for version v0.4 \u00b6 This file /etc/systemd/system/multi-user.target.wants/autoconfig.service is called during boot that sets the frequency. [Service] ExecStart =/bin/sh /etc/radio_init.sh $(read_default_tx_frequency) In this version the frequency is read from the eeprom. So we program the eeprom with the correct center frequency. Programming the eeprom with the center frequency we do with this script. root@benetelru:~# cat progFreq registercontrol -w 0xC036B -x 0x88000088 # don't touch file eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x174:0x01:0x33 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x175:0x01:0x37 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x176:0x01:0x35 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x177:0x01:0x31 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x178:0x01:0x2E eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x179:0x01:0x36 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17A:0x01:0x38 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17B:0x01:0x30 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17C:0x01:0x33 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17D:0x01:0x37 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17E:0x01:0x35 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17F:0x01:0x31 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x180:0x01:0x2E eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x181:0x01:0x36 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x182:0x01:0x38 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x183:0x01:0x30 eeprog_cp60 -f -16 /dev/i2c-0 0x57 -r 0x174:8 eeprog_cp60 -f -16 /dev/i2c-0 0x57 -r 0x17C:8 Each byte 0x33,0x37,0x35, ... is the ascii value of a numbers 3751,680 Set attenuation level \u00b6 read current attenuations ~# radiocontrol -o G a Benetel radiocontrol Version : 0.9.0 Madura API Version : 5.1.0.21 Madura ARM FW version : 5.0.0.32 Madura ARM DPD FW version : 5.0.0.32 Madura Stream version : 8.0.0.5 Madura Product ID : 0x84 Madura Device Revision : 0xb0 Tx1 Attenuation (mdB) : 20000 Tx2 Attenuation (mdB) : 40000 Tx3 Attenuation (mdB) : 21000 Tx4 Attenuation (mdB) : 40000 PLL1 Frequency (Hz) : 0 PLL2 Frequency (Hz) : 3751680000 Front-end Control : 0x2aa491 Madura Deframer 0 : 0x87 Madura Framer 0 : 0xa Internal Temperature (degC) : 47 External Temperature (degC) : 42.789063 RX1 Power Level (dBFS) : -60.750000 RX2 Power Level (dBFS) : -60.750000 RX3 Power Level (dBFS) : -60.750000 RX4 Power Level (dBFS) : -60.750000 ORX1 Peak/Mean Power Level (dBFS) : -10.839418/-22.709361 ORX2 Peak/Mean Power Level (dBFS) : -inf/-inf ORX3 Peak/Mean Power Level (dBFS) : -10.748048/-21.656226 ORX4 Peak/Mean Power Level (dBFS) : -inf/-inf set attenuation for antenna 1 /usr/bin/radiocontrol -o A 20000 1 set attenuation for antenna 3 /usr/bin/radiocontrol -o A 21000 4 yes the 4 at the end seems to be correct. Configure for any RRU release \u00b6 The frequency in the RRU \u00b6 The frequency set in in the RRU is the center frequency. The center frequency has to be divisable by 3.84 Mhz. Ths is an example of a configuration * point A frequency : 3732.60 ( arfcn : 648840 ) - du configuration * center Frequency : 3751.68 ( arfcn : 650112 ) - rru configuration 3751.68 / 3.84 = 977 . It is divisable. Some things you can check in the logging to see if everything is set correctly. The same info in more detail can be found here Link setting frequency benetel650 root@benetelru:~# radiocontrol -o G a PLL2 Frequency (Hz) : 3751680000 cat du-config.json| grep -e bandwidth_mhz -e scs_khz -e coreset_zero -e frequency_band -e arfcn -e nrb \"nr_arfcn\": 648840, \"frequency_band_list\": [ \"nr_frequency_band\": 78 \"transmission_bandwidth\": { \"bandwidth_mhz\": 40, \"scs_khz\": 30, \"nrb\": 106 \"coreset_zero_index\": 3, cat eff_log_bin | grep -i -e freq -e scs -e band 00001261908350886288 info add_coreset_zero: 0xff BWP{id=0, start_crb=0, num_rb=24, Scs::KHZ_30, CyclicPrefix::NORMAL} 00001261908350897548 info add_coreset_zero: 0x00 BWP{id=0, start_crb=0, num_rb=106, Scs::KHZ_30, CyclicPrefix::NORMAL} 00001261908387312201 info dlChannelBandwidth: 106 00001261908387312831 info ulChannelBandwidth_present: 0 00001261908387313431 info band: 78 00001261908387313991 info absoluteFrequencyPointA: 648840 00001261908387314601 info ulCenterFrequency_present: 0 00001261908387320821 info ssbConfig.absoluteFrequencySsb: 649152 00001261908387328381 info prachConfig.msg1_FrequencyStart: 0 Set RRU mac address in DU server \u00b6 Throubleshoot \u00b6 GPS \u00b6 See if GPS is locked root@benetelru:~# syncmon DPLL0 State (SyncE/Ethernet clock): LOCKED DPLL1 State (FPGA clocks): FREERUN DPLL2 State (FPGA clocks): FREERUN DPLL3 State (RF/PTP clock): LOCKED CLK0 SyncE LIVE: OK CLK0 SyncE STICKY: LOS + No Activity CLK2 10MHz LIVE: LOS + No Activity CLK2 10MHz STICKY: LOS + No Activity CLK5 GPS LIVE: OK CLK5 GPS STICKY: LOS and Frequency Offset CLK6 EXT 1PPS LIVE: LOS and Frequency Offset CLK6 EXT 1PPS STICKY: LOS and Frequency Offset To be noted \u00b6 some important registers root@benetelru:~# reportRuStatus [INFO] Sync status is: Register 0xc0367, Value : 0x1 ------------------------------- [INFO] RU Status information is: Register 0xc0306, Value : 0x470800 ------------------------------- [INFO] Fill level of Reception Window is: Register 0xc0308, Value : 0x6c12 ------------------------------- [INFO] Sample Count is: Register 0xc0311, Value : 0x56f49 ------------------------------- ============================================================ RU Status Register description: ============================================================ [31:19] not used [18] set to 1 if handshake is successful [17] set to 1 when settling time (fronthaul) has completed [16] set to 1 if symbolndex=0 was captured [15] set to 1 if payload format is invalid [14] set to 1 if symbol index error has been detected [13:12] not used [11] set to 1 if DU MAC address is correct [10:2] not used [1] Reception Window Buffer is empty [0] Reception Window Buffer is full ------------------------------------------------------------ =========================================================== [NOTE] Max buffer depth is 53424 (112 symbols, 2 antennas) =========================================================== trace traffic between RRU and L1. Also the mac can be read from this trace. Packet lengths are 3874. Remember we increased the MTU size to 9000. Without increasing the L1 would crash on the fragmented udp packets. $ tcpdump -i enp45s0f0 -c 5 port 44000 -en tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on enp45s0f0, link-type EN10MB (Ethernet), capture size 262144 bytes 19:22:47.096453 02:00:5e:01:01:01 > 6c:b3:11:08:a4:e0, ethertype IPv4 (0x0800), length 64: 10.10.0.2.44000 > 10.10.0.1.44000: UDP, length 20 19:22:47.106677 6c:b3:11:08:a4:e0 > 02:00:5e:01:01:01, ethertype IPv4 (0x0800), length 54: 10.10.0.1.44000 > 10.10.0.2.44000: UDP, length 12 19:23:14.596247 02:00:5e:01:01:01 > 6c:b3:11:08:a4:e0, ethertype IPv4 (0x0800), length 64: 10.10.0.2.44000 > 10.10.0.1.44000: UDP, length 12 19:23:14.596621 6c:b3:11:08:a4:e0 > 02:00:5e:01:01:01, ethertype IPv4 (0x0800), length 3874: 10.10.0.1.44000 > 10.10.0.2.44000: UDP, length 3832 19:23:14.596631 6c:b3:11:08:a4:e0 > 02:00:5e:01:01:01, ethertype IPv4 (0x0800), length 3874: 10.10.0.1.44000 > 10.10.0.2.44000: UDP, length 3832 5 packets captured Check if the L1 is listening $ while true ; do sleep 1 ; netstat -ano | grep 44000 ;echo $RANDOM; done udp 0 118272 10.10.0.1:44000 0.0.0.0:* off (0.00/0/0) 1427 udp 0 16896 10.10.0.1:44000 0.0.0.0:* off (0.00/0/0) 11962 udp 0 42240 10.10.0.1:44000 0.0.0.0:* off (0.00/0/0) 16780 udp 0 0 10.10.0.1:44000 0.0.0.0:* off (0.00/0/0) 502 Show the traffic between rru and l1 $ ifstat -i enp45s0f0 enp45s0f0 KB/s in KB/s out 71320.01 105959.7 71313.36 105930.1 Troubleshooting Fiber Port not showing up \u00b6 https://www.serveradminz.com/blog/unsupported-sfp-linux/ fiber port should always be like this enp1s0f1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 9000 enp1s0f1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 9000 enp1s0f1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 9000 enp1s0f1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 9000 enp1s0f1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 9000 There should exist an arp entry like this $ arp -an | grep 10.10 ? (10.10.0.2) at 02:00:5e:01:01:01 [ether] PERM on enp1s0f1 Starting RRU Benetel 650 \u00b6 Perform these steps to get a running active cell. 1) Start L1 2) Start DU At this point following sequence is DU CU | F1SetupRequest---> | | <---F1SetupResponse | | | | <---GNBCUConfigurationUpdate | | | The L1 starts listening on ip:port 10.10.0.1:44000 3) type ssh root@10.10.0.100 handshake After less than 30 seconds communication between rru and du starts. around 100 Mbytes/second DU CU | GNBCUConfigurationUpdateAck---> | | | 5) type ssh root@10.10.0.100 handshake again to stop the traffic. ( If it does not stop use ssh_rru \"registercontrol -w c0310 -x 0 but be carefull ) Fiber port not coming up. \u00b6 It has been seen on a rare occasion that the B650 is not pingable although the interface is UP. pinging the server has this result $ ping 10.10.0.100 PING 10.10.0.100 (10.10.0.100) 56(84) bytes of data. From 10.10.0.1 icmp_seq=1 Destination Host Unreachable From 10.10.0.1 icmp_seq=2 Destination Host Unreachable From 10.10.0.1 icmp_seq=5 Destination Host Unreachable $ ip -br a enp43s0f0 UP 10.10.0.1/24 fe80::21b:21ff:fec1:38a8/64 in that case check the arp table $ arp -an ? (10.10.0.100) at <incomplete> on enp43s0f0 powercycling the B650 a few times fixes this issue. Also I connect and reconnect the fiber SFP+ at the server side.","title":"Summary"},{"location":"benetel650-install/#summary","text":"","title":"Summary"},{"location":"benetel650-install/#drawing","text":"10.10.0.100:ssh +-------------+ | | | | +-----------+ +-----------+ | | | | | | | RRU +----fiber----+ L1 | | DU | | | | | | | | | +-----------+ +-----------+ | | +-------------+ aa:bb:cc:dd:ee:ff 11:22:33:44:55:66 10.10.0.2:44000 10.10.0.1:44000 eth0 enp45s0f0 port FIBER1","title":"Drawing"},{"location":"benetel650-install/#configure-the-du-under-construction","text":"This chapter on only focuses on what to configure to get the Benetel650 working. The complete installation of the DU can be found in the du-install doc itself. Other then the B210, benetel uses 2 software components. 2 Containers, a effnet du and phluido l1. The phluido rru is not needed. This component resides in the benetel equipment. adapt the PhluidoL1_NR_Benetel.cfg file delivered by effnet Make sure to set the value LicenseKey option to the received Phluido license key: to be tested which .cfg to use Create a configuration file for the Effnet DU: to be tested which .json to use Before creating the docker-compose.yml file, make sure to set the $CU_IP environment variable. This IP address can be determined by executing the following command. The CU address is the second IP address. This IP address should be in the IP pool that was assigned to MetalLB in dRax Installation . kubectl get services | grep 'acc-5g-cu-cp-.*-sctp-f1' Now, create a docker-compose configuration file: tee accelleran-du-phluido/accelleran-du-phluido-2021-06-30/docker-compose.yml <<EOF version: \"3\" services: phluido_l1: image: phluido_l1 container_name: phluido_l1_cn tty: true privileged: true ipc: shareable shm_size: 2gb command: /config.cfg volumes: - \"$PWD/phluido/PhluidoL1_NR_B210.cfg:/config.cfg:ro\" - \"/run/logs-du/l1:/workdir\" - \"/etc/machine-id:/etc/machine-id:ro\" working_dir: \"/workdir\" network_mode: host du: image: gnb_du_main_phluido volumes: - \"$PWD/b210_config_20mhz.json:/config.json:ro\" - \"/run/logs-du/du:/workdir\" - /run/pcscd/pcscd.comm:/run/pcscd/pcscd.comm ipc: container:phluido_l1_cn tty: true cap_add: - CAP_SYS_NICE - CAP_IPC_LOCK depends_on: - phluido_l1 entrypoint: [\"/bin/sh\", \"-c\", \"sleep 4 && exec /gnb_du_main_phluido /config.json\"] working_dir: \"/workdir\" extra_hosts: - \"cu:$F1_CU_IP\" EOF","title":"Configure the DU ( under construction )"},{"location":"benetel650-install/#prepare-the-server-for-the-benetel-650","text":"The benetel is connected with a fiber to the server. 1. The port on the RRU is labeled port FIBER1 2. The port on the server is one of these listed below. :ad@5GCN:~$ lshw | grep SFP -C 5 WARNING: you should run this program as super-user. capabilities: pci normal_decode bus_master cap_list configuration: driver = pcieport resources: irq:29 ioport:f000 ( size = 4096 ) memory:f8000000-f86fffff *-network:0 DISABLED description: Ethernet interface product: 82599ES 10 -Gigabit SFI/SFP+ Network Connection vendor: Intel Corporation physical id: 0 bus info: pci@0000:2d:00.0 logical name: enp45s0f0 version: 01 -- capabilities: bus_master cap_list rom ethernet physical fibre 10000bt-fd configuration: autonegotiation = off broadcast = yes driver = ixgbe driverversion = 5 .1.0-k firmware = 0x2b2c0001 latency = 0 link = no multicast = yes resources: irq:202 memory:f8000000-f807ffff ioport:f020 ( size = 32 ) memory:f8200000-f8203fff memory:f8080000-f80fffff memory:f8204000-f8303fff memory:f8304000-f8403fff *-network:1 DISABLED description: Ethernet interface product: 82599ES 10 -Gigabit SFI/SFP+ Network Connection vendor: Intel Corporation physical id: 0 .1 bus info: pci@0000:2d:00.1 logical name: enp45s0f1 version: 01 by setting both network devices to UP you find out which one is connected. In our case its enp45s0f0. This port is the one we connected the fiber with. :ad@5GCN:~$ sudo ip link set dev enp45s0f0 up :ad@5GCN:~$ sudo ip link set dev enp45s0f1 up :ad@5GCN:~$ ip -br a : enp45s0f0 UP fe80::6eb3:11ff:fe08:a4e0/64 enp45s0f1 DOWN : configuring the static ip of enp45s0f0 is done via netplan. add this part to /etc/netplan/50-cloud-init.yaml . In some installation it might be anot network: ethernets: enp45s0f0: dhcp4: false dhcp6: false optional: true addresses: - 10 .10.0.1/24 mtu: 9000 To apply this configuration you can use sudo netplan apply This needs to be the result $ ip -br a | grep enp45 enp45s0f0 UP 10 .10.0.1/24 fe80::6eb3:11ff:fe08:a4e0/64 enp45s0f1 DOWN The ip benetel is configured with is 10.10.0.100 . This is the MGMT ip. We can ssh to it. We found out using nmap this way. $ nmap 10 .10.0.0/24 Starting Nmap 7 .60 ( https://nmap.org ) at 2021 -09-21 10 :15 CEST Nmap scan report for 10 .10.0.1 Host is up ( 0 .000040s latency ) . Not shown: 996 closed ports PORT STATE SERVICE 22 /tcp open ssh 111 /tcp open rpcbind 5900 /tcp open vnc 9100 /tcp open jetdirect Nmap scan report for 10 .10.0.100 Host is up ( 0 .0053s latency ) . Not shown: 998 closed ports PORT STATE SERVICE 22 /tcp open ssh 111 /tcp open rpcbind Nmap done : 256 IP addresses ( 2 hosts up ) scanned in 3 .10 seconds A route is added also in the route table automatically $ route -n | grep 10 .10.0.0 10 .10.0.0 0 .0.0.0 255 .255.255.0 U 0 0 0 enp45s0f0 ```` now you can ssh to the benetel ``` bash $ ssh root@10.10.0.100 Last login: Fri Feb 7 16 :45:59 2020 from 10 .10.0.1 root@benetelru:~# ls -l -rwxrwxrwx 1 root root 1572 Sep 10 2021 DPLL3_1PPS_REGISTER_PATCH.txt drwxrwxrwx 2 root root 0 Feb 7 16 :44 adrv9025 -rwxrwxrwx 1 root root 1444 Feb 7 16 :40 dev_struct.dat -rwxrwxrwx 1 root root 17370 Sep 10 2021 dpdModelReadback.txt -rwxrwxrwx 1 root root 5070 Feb 7 17 :00 dpdModelcoefficients.txt -rwxrwxrwx 1 root root 24036 Sep 10 2021 eeprog_cp60 -rwxrwxrwx 1 root root 1825062 Feb 7 15 :58 madura_log_file.txt -rw------- 1 root root 1230 Feb 7 2020 nohup.out -rwxr-xr-x 1 root root 57 Feb 7 2020 nohup_handshake -rwxrwxrwx 1 root root 571 Feb 7 2020 progBenetelDuMAC_CATB -rwxr-xr-x 1 root root 1121056 Feb 7 16 :24 quickRadioControl -rwxrwxrwx 1 root root 1151488 Sep 10 2021 radiocontrol_prv-nk-cliupdate -rwxrwxrwx 1 root root 22904 Aug 24 2021 registercontrol -rwxrwxrwx 1 root root 164 Feb 7 16 :35 removeResetRU_CATB -rwxrwxrwx 1 root root 163 Feb 7 2020 reportRuStatus -rwxrwxrwx 1 root root 162 Feb 7 16 :35 resetRU_CATB -rwxr-xr-x 1 root root 48 Feb 7 15 :57 runSync -rwxrwxrwx 1 root root 21848 Sep 10 2021 smuconfig -rwxrwxrwx 1 root root 17516 Sep 10 2021 statmon -rwxrwxrwx 1 root root 23248 Sep 10 2021 syncmon -rwxr-xr-x 1 root root 182 Feb 7 16 :41 trialHandshake root@benetelru:~# The mac address of the benetel data interface can be found like this to be done Add mac entry script in routable.d. $ cat /etc/networkd-dispatcher/routable.d/macs.sh #!/bin/sh sudo arp -s 10.10.0.2 aa:bb:cc:dd:ee:ff -i enp45s0f0 chmod 777 /etc/networkd-dispatcher/routable.d/macs.sh Benetel650 does not answer arp requests. With this apr entry in the arp table the server knows to which mac address it needs to sent the ip packet to. The ip packet towards the RRU with ip 10.10.0.2. Test the script by running it and checking the arp -a table like this $ arp -a | grep 10.10.0.2 ? (10.10.0.2) at 02:00:5e:01:01:01 [ether] PERM on enp45s0f0","title":"Prepare the server for the Benetel 650"},{"location":"benetel650-install/#version-check","text":"finding out the version and commit hash of the benetel650 commit hash root@benetelru:~# registercontrol -v Lightweight HPS-to-FPGA Control Program Version : V1.2.0 ****BENETEL PRODUCT VERSIONING BLOCK**** This Build Was Created Locally. Please Use Git Pipeline! Project ID NUMBER: 0 Git # Number: f6366d7adf84933ab2b242a345bd63c07fedb9e5 Build ID: 0 Version Number: 0.0.1 Build Date: 2/12/2021 Build Time H:M:S: 18:20:3 ****BENETEL PRODUCT VERSIONING BLOCK END**** The version which is referred to. This is version 0.4. Depending on the version different configuration commands apply. root@benetelru:~# cat /etc/benetel-rootfs-version RAN650-2V0.4","title":"Version Check"},{"location":"benetel650-install/#configure-the-rru-release-v03","text":"","title":"Configure the RRU release V0.3"},{"location":"benetel650-install/#set-du-mac-address-for-version-v03","text":"Inside the file /etc/radio_init.sh we program the mac. Example for MAC address 00:1E:67:FD:F5:51 you will find in the file: registercontrol -w c0315 -x 0x67FDF551 >> /home/root/radio_boot_response registercontrol -w c0316 -x 0x001E >> /home/root/radio_boot_response echo \"Configure the MAC address of the O-DU: 00:1E:67:FD:F5:51 \" >> /home/root/radio_status Make sure to edit those as MAC address of the fiber port. Reboot the BNTL650 It is possible to run the commands manually to speed up things. registercontrol -w c0315 -x 0x67FDF551 >> /home/root/radio_boot_response registercontrol -w c0316 -x 0x001E >> /home/root/radio_boot_response","title":"Set DU mac address for version V0.3"},{"location":"benetel650-install/#set-the-frequency-for-version-v03","text":"This file /etc/systemd/system/multi-user.target.wants/autoconfig.service is called during boot that sets the frequency. change the frequency here. [Service] ExecStart =/bin/sh /etc/radio_init.sh 3751.680 Example for frequency 3751.68MHz (ARFCN=650112) you will find in the file: Make sure to edit the pointA frequency ARFCN value in the DU config (in this example PointA_ARFCN=648840). Reboot the Benetel.","title":"Set the Frequency for version V0.3"},{"location":"benetel650-install/#configure-the-rru-release-v04","text":"","title":"Configure the RRU release V0.4"},{"location":"benetel650-install/#set-du-mac-address-in-the-rru","text":"Create this script to program the mac address of the DU inside the RRU. Remember the RRU does not request arp, so we have to manually configure that. root@benetelru:~# cat progDuMAC-5GCN-enp45s0f0 # 11:22:33:44:55:66 5GCN-itf registercontrol -w 0xC036B -x 0x88000088 # don't touch file eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1A:0x01:0x11 # first byte of mac address is 0x11 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1B:0x01:0x22 # etc ... eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1C:0x01:0x33 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1D:0x01:0x44 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1E:0x01:0x55 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1F:0x01:0x66","title":"Set DU mac address in the RRU"},{"location":"benetel650-install/#set-the-frequency-for-version-v04","text":"This file /etc/systemd/system/multi-user.target.wants/autoconfig.service is called during boot that sets the frequency. [Service] ExecStart =/bin/sh /etc/radio_init.sh $(read_default_tx_frequency) In this version the frequency is read from the eeprom. So we program the eeprom with the correct center frequency. Programming the eeprom with the center frequency we do with this script. root@benetelru:~# cat progFreq registercontrol -w 0xC036B -x 0x88000088 # don't touch file eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x174:0x01:0x33 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x175:0x01:0x37 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x176:0x01:0x35 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x177:0x01:0x31 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x178:0x01:0x2E eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x179:0x01:0x36 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17A:0x01:0x38 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17B:0x01:0x30 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17C:0x01:0x33 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17D:0x01:0x37 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17E:0x01:0x35 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17F:0x01:0x31 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x180:0x01:0x2E eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x181:0x01:0x36 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x182:0x01:0x38 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x183:0x01:0x30 eeprog_cp60 -f -16 /dev/i2c-0 0x57 -r 0x174:8 eeprog_cp60 -f -16 /dev/i2c-0 0x57 -r 0x17C:8 Each byte 0x33,0x37,0x35, ... is the ascii value of a numbers 3751,680","title":"Set the Frequency for version v0.4"},{"location":"benetel650-install/#set-attenuation-level","text":"read current attenuations ~# radiocontrol -o G a Benetel radiocontrol Version : 0.9.0 Madura API Version : 5.1.0.21 Madura ARM FW version : 5.0.0.32 Madura ARM DPD FW version : 5.0.0.32 Madura Stream version : 8.0.0.5 Madura Product ID : 0x84 Madura Device Revision : 0xb0 Tx1 Attenuation (mdB) : 20000 Tx2 Attenuation (mdB) : 40000 Tx3 Attenuation (mdB) : 21000 Tx4 Attenuation (mdB) : 40000 PLL1 Frequency (Hz) : 0 PLL2 Frequency (Hz) : 3751680000 Front-end Control : 0x2aa491 Madura Deframer 0 : 0x87 Madura Framer 0 : 0xa Internal Temperature (degC) : 47 External Temperature (degC) : 42.789063 RX1 Power Level (dBFS) : -60.750000 RX2 Power Level (dBFS) : -60.750000 RX3 Power Level (dBFS) : -60.750000 RX4 Power Level (dBFS) : -60.750000 ORX1 Peak/Mean Power Level (dBFS) : -10.839418/-22.709361 ORX2 Peak/Mean Power Level (dBFS) : -inf/-inf ORX3 Peak/Mean Power Level (dBFS) : -10.748048/-21.656226 ORX4 Peak/Mean Power Level (dBFS) : -inf/-inf set attenuation for antenna 1 /usr/bin/radiocontrol -o A 20000 1 set attenuation for antenna 3 /usr/bin/radiocontrol -o A 21000 4 yes the 4 at the end seems to be correct.","title":"Set attenuation level"},{"location":"benetel650-install/#configure-for-any-rru-release","text":"","title":"Configure for any RRU release"},{"location":"benetel650-install/#the-frequency-in-the-rru","text":"The frequency set in in the RRU is the center frequency. The center frequency has to be divisable by 3.84 Mhz. Ths is an example of a configuration * point A frequency : 3732.60 ( arfcn : 648840 ) - du configuration * center Frequency : 3751.68 ( arfcn : 650112 ) - rru configuration 3751.68 / 3.84 = 977 . It is divisable. Some things you can check in the logging to see if everything is set correctly. The same info in more detail can be found here Link setting frequency benetel650 root@benetelru:~# radiocontrol -o G a PLL2 Frequency (Hz) : 3751680000 cat du-config.json| grep -e bandwidth_mhz -e scs_khz -e coreset_zero -e frequency_band -e arfcn -e nrb \"nr_arfcn\": 648840, \"frequency_band_list\": [ \"nr_frequency_band\": 78 \"transmission_bandwidth\": { \"bandwidth_mhz\": 40, \"scs_khz\": 30, \"nrb\": 106 \"coreset_zero_index\": 3, cat eff_log_bin | grep -i -e freq -e scs -e band 00001261908350886288 info add_coreset_zero: 0xff BWP{id=0, start_crb=0, num_rb=24, Scs::KHZ_30, CyclicPrefix::NORMAL} 00001261908350897548 info add_coreset_zero: 0x00 BWP{id=0, start_crb=0, num_rb=106, Scs::KHZ_30, CyclicPrefix::NORMAL} 00001261908387312201 info dlChannelBandwidth: 106 00001261908387312831 info ulChannelBandwidth_present: 0 00001261908387313431 info band: 78 00001261908387313991 info absoluteFrequencyPointA: 648840 00001261908387314601 info ulCenterFrequency_present: 0 00001261908387320821 info ssbConfig.absoluteFrequencySsb: 649152 00001261908387328381 info prachConfig.msg1_FrequencyStart: 0","title":"The frequency in the RRU"},{"location":"benetel650-install/#set-rru-mac-address-in-du-server","text":"","title":"Set RRU mac address in DU server"},{"location":"benetel650-install/#throubleshoot","text":"","title":"Throubleshoot"},{"location":"benetel650-install/#gps","text":"See if GPS is locked root@benetelru:~# syncmon DPLL0 State (SyncE/Ethernet clock): LOCKED DPLL1 State (FPGA clocks): FREERUN DPLL2 State (FPGA clocks): FREERUN DPLL3 State (RF/PTP clock): LOCKED CLK0 SyncE LIVE: OK CLK0 SyncE STICKY: LOS + No Activity CLK2 10MHz LIVE: LOS + No Activity CLK2 10MHz STICKY: LOS + No Activity CLK5 GPS LIVE: OK CLK5 GPS STICKY: LOS and Frequency Offset CLK6 EXT 1PPS LIVE: LOS and Frequency Offset CLK6 EXT 1PPS STICKY: LOS and Frequency Offset","title":"GPS"},{"location":"benetel650-install/#to-be-noted","text":"some important registers root@benetelru:~# reportRuStatus [INFO] Sync status is: Register 0xc0367, Value : 0x1 ------------------------------- [INFO] RU Status information is: Register 0xc0306, Value : 0x470800 ------------------------------- [INFO] Fill level of Reception Window is: Register 0xc0308, Value : 0x6c12 ------------------------------- [INFO] Sample Count is: Register 0xc0311, Value : 0x56f49 ------------------------------- ============================================================ RU Status Register description: ============================================================ [31:19] not used [18] set to 1 if handshake is successful [17] set to 1 when settling time (fronthaul) has completed [16] set to 1 if symbolndex=0 was captured [15] set to 1 if payload format is invalid [14] set to 1 if symbol index error has been detected [13:12] not used [11] set to 1 if DU MAC address is correct [10:2] not used [1] Reception Window Buffer is empty [0] Reception Window Buffer is full ------------------------------------------------------------ =========================================================== [NOTE] Max buffer depth is 53424 (112 symbols, 2 antennas) =========================================================== trace traffic between RRU and L1. Also the mac can be read from this trace. Packet lengths are 3874. Remember we increased the MTU size to 9000. Without increasing the L1 would crash on the fragmented udp packets. $ tcpdump -i enp45s0f0 -c 5 port 44000 -en tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on enp45s0f0, link-type EN10MB (Ethernet), capture size 262144 bytes 19:22:47.096453 02:00:5e:01:01:01 > 6c:b3:11:08:a4:e0, ethertype IPv4 (0x0800), length 64: 10.10.0.2.44000 > 10.10.0.1.44000: UDP, length 20 19:22:47.106677 6c:b3:11:08:a4:e0 > 02:00:5e:01:01:01, ethertype IPv4 (0x0800), length 54: 10.10.0.1.44000 > 10.10.0.2.44000: UDP, length 12 19:23:14.596247 02:00:5e:01:01:01 > 6c:b3:11:08:a4:e0, ethertype IPv4 (0x0800), length 64: 10.10.0.2.44000 > 10.10.0.1.44000: UDP, length 12 19:23:14.596621 6c:b3:11:08:a4:e0 > 02:00:5e:01:01:01, ethertype IPv4 (0x0800), length 3874: 10.10.0.1.44000 > 10.10.0.2.44000: UDP, length 3832 19:23:14.596631 6c:b3:11:08:a4:e0 > 02:00:5e:01:01:01, ethertype IPv4 (0x0800), length 3874: 10.10.0.1.44000 > 10.10.0.2.44000: UDP, length 3832 5 packets captured Check if the L1 is listening $ while true ; do sleep 1 ; netstat -ano | grep 44000 ;echo $RANDOM; done udp 0 118272 10.10.0.1:44000 0.0.0.0:* off (0.00/0/0) 1427 udp 0 16896 10.10.0.1:44000 0.0.0.0:* off (0.00/0/0) 11962 udp 0 42240 10.10.0.1:44000 0.0.0.0:* off (0.00/0/0) 16780 udp 0 0 10.10.0.1:44000 0.0.0.0:* off (0.00/0/0) 502 Show the traffic between rru and l1 $ ifstat -i enp45s0f0 enp45s0f0 KB/s in KB/s out 71320.01 105959.7 71313.36 105930.1","title":"To be noted"},{"location":"benetel650-install/#troubleshooting-fiber-port-not-showing-up","text":"https://www.serveradminz.com/blog/unsupported-sfp-linux/ fiber port should always be like this enp1s0f1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 9000 enp1s0f1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 9000 enp1s0f1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 9000 enp1s0f1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 9000 enp1s0f1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 9000 There should exist an arp entry like this $ arp -an | grep 10.10 ? (10.10.0.2) at 02:00:5e:01:01:01 [ether] PERM on enp1s0f1","title":"Troubleshooting Fiber Port not showing up"},{"location":"benetel650-install/#starting-rru-benetel-650","text":"Perform these steps to get a running active cell. 1) Start L1 2) Start DU At this point following sequence is DU CU | F1SetupRequest---> | | <---F1SetupResponse | | | | <---GNBCUConfigurationUpdate | | | The L1 starts listening on ip:port 10.10.0.1:44000 3) type ssh root@10.10.0.100 handshake After less than 30 seconds communication between rru and du starts. around 100 Mbytes/second DU CU | GNBCUConfigurationUpdateAck---> | | | 5) type ssh root@10.10.0.100 handshake again to stop the traffic. ( If it does not stop use ssh_rru \"registercontrol -w c0310 -x 0 but be carefull )","title":"Starting RRU Benetel 650"},{"location":"benetel650-install/#fiber-port-not-coming-up","text":"It has been seen on a rare occasion that the B650 is not pingable although the interface is UP. pinging the server has this result $ ping 10.10.0.100 PING 10.10.0.100 (10.10.0.100) 56(84) bytes of data. From 10.10.0.1 icmp_seq=1 Destination Host Unreachable From 10.10.0.1 icmp_seq=2 Destination Host Unreachable From 10.10.0.1 icmp_seq=5 Destination Host Unreachable $ ip -br a enp43s0f0 UP 10.10.0.1/24 fe80::21b:21ff:fec1:38a8/64 in that case check the arp table $ arp -an ? (10.10.0.100) at <incomplete> on enp43s0f0 powercycling the B650 a few times fixes this issue. Also I connect and reconnect the fiber SFP+ at the server side.","title":"Fiber port not coming up."},{"location":"benetel650-install/PTPInstall/","text":"Prerequisites \u00b6 The RU needs at least version 2V0.6.0 oot@benetelru:~# cat /etc/benetel-rootfs-version RAN550-2V0.6.0 Configuration \u00b6 Dawing \u00b6 created in https://asciiflow.com/ CU IP : 10.10.0.1 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 RU \u2502 \u2502 IP : 10.10.0.100 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 eth0\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524eno1 \u2502 \u2502 \u2502 Fiber \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Server side \u00b6 Install ptp server \u00b6 sudo apt update sudo apt install linuxptp configuration file \u00b6 cat << EOF > benetel550_ptp.cfg [global] domainNumber 24 twoStepFlag 1 assume_two_step 1 #masterOnly 1 announceReceiptTimeout 3 logAnnounceInterval 1 logSyncInterval -4 logMinDelayReqInterval -4 network_transport L2 hybrid_e2e 1 EOF the interface to the RU \u00b6 the interface to the should have an ip starting with 10.10. ITF_RU=$(ip -br a | grep 10.10.0 | awk '{print $1}') echo $ITF_RU run the server \u00b6 sudo ptp4l -2 -E -f benetel550_ptp.cfg -i $ITF_RU -m -2 means that it is ethernet multicast packets that are being sent. At this moment multicast messages will be sent on the interface eno1 example of a tcpdump on the server ad@duserver:~$ sudo tcpdump -i eno1 -en [sudo] password for ad: tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eno1, link-type EN10MB (Ethernet), capture size 262144 bytes 13:03:30.226015 ec:f4:bb:e3:b4:68 > 01:1b:19:00:00:00, ethertype Unknown (0x88f7), length 58: 0x0000: 0002 002c 1800 0200 0000 0000 0000 0000 ...,............ 0x0010: 0000 0000 ecf4 bbff fee3 b468 0001 74cc ...........h..t. 0x0020: 00fc 0000 0000 0000 0000 0000 ............ RU side \u00b6 Commands to configure PTP \u00b6 Below the command to execute to use PTP to sync root@benetelru:~# setSyncModePtp.sh 57 settings written to SMU RU Set to use PTP on next reboot. Reboot for this to take effect. trace ethernet packets \u00b6 tracing ethernet packets on the RU oot@benetelru:~# tcpdump -i eth0 ether proto 0x88f7 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 12:31:00.865296 ec:f4:bb:e3:b4:68 (oui Unknown) > 01:1b:19:00:00:00 (oui Unknown), ethertype Unknown (0x88f7), length 78: 0x0000: 0b02 0040 1800 0008 0000 0000 0000 0000 ...@............ 0x0010: 0000 0000 ecf4 bbff fee3 b468 0001 0000 ...........h.... 0x0020: 0501 0000 0000 0000 0000 0000 0025 0080 .............%.. 0x0030: f8fe ffff 80ec f4bb fffe e3b4 6800 00a0 ............h... restart sync or reboot \u00b6 without rebooting we can restart the sync by this command systemctl restart ru_sync when rebooting the unit the previous command is not necessary logs \u00b6 tail -f /var/log/pcm4l this is a succesful sync. It first shows the drift and ends with Time and Freq locked to Yes. E::SyncAnalysis: 2020-02-11 19:34:02 373837860 ns [3, Tracker#0] (3240) offset: -355.0 ns delay: 262.0 ns RE::SyncAnalysis: 2020-02-11 19:34:02 437792660 ns [3, Tracker#0] (3240) offset: -370.5 ns delay: 262.5 ns RE::SyncAnalysis: 2020-02-11 19:34:02 497767860 ns [3, Tracker#0] (3240) offset: -389.5 ns delay: 266.5 ns RE::SyncAnalysis: 2020-02-11 19:34:02 561789160 ns [3, Tracker#0] (3240) offset: -403.0 ns delay: 265.0 ns : RE::SyncAnalysis: 2020-02-11 19:34:06 001791300 ns [3, Tracker#0] (3240) offset: 0.0 ns delay: 269.0 ns RE::SyncAnalysis: 2020-02-11 19:34:07 377794460 ns [3, Tracker#0] (3240) offset: 0.0 ns delay: 269.0 ns : RE::Debug: 2020-02-11 19:34:07 695880440 ns [2, Supervisor] Enter time locked state RE::Debug: 2020-02-11 19:34:07 696391520 ns [2, Supervisor] Tracker#0: RE::Debug: 2020-02-11 19:34:07 701360020 ns [2, Supervisor] Master port ID: ec:f4:bb:ff:fe:e3:b4:68.1 RE::Debug: 2020-02-11 19:34:07 701947880 ns [2, Supervisor] Current reference master: Yes RE::Debug: 2020-02-11 19:34:07 702461120 ns [2, Supervisor] Freq locked: Yes RE::Debug: 2020-02-11 19:34:07 702870420 ns [2, Supervisor] Time locked: Yes reboot the RU \u00b6 rebooting the RU will make the RU sync The indication of Sync Completed in this file indicates sync is succesful root@benetelru:~# cat /tmp/logs/radio_status [INFO] Platform: RAN550_B [INFO] Radio bringup begin [INFO] Initialize TDD Pattern [INFO] Load EEPROM Data [INFO] Tx1 Attenuation set to 05580 mdB [INFO] Tx3 Attenuation set to 07660 mdB [INFO] Operating Frequency set to 3601.920 MHz [INFO] Waiting for Sync [INFO] Sync completed [INFO] Kick off Synchronization of Linux system time to PTP time [INFO] Start Radio Configuration : : [INFO] Radio bringup complete 15:54:47 up 4 min, load average: 0.08, 0.23, 0.11 another file indicates the success aswell. root@benetelru:~# cat /tmp/logs/radio_sync_status Configuring CP60 for PTP Sync Mode PTP Settings configured Directory /usr/lib/benetel OK Directory /usr/lib/benetel/splane OK Directory /usr/lib/benetel/splane/flags OK todsync started PTP4L started PCM4L started Syncmon started Waiting for initial PTP sync... PTP locked, enabling holdover functionality. Source References \u00b6 Software User Guide for RANx50-02 CAT-B O-RUs [June 2022] https://drive.google.com/drive/u/0/folders/17uyl3vpKXAO_9IMyiEKNQw0VpvLJvVxe","title":"PTPInstall"},{"location":"benetel650-install/PTPInstall/#prerequisites","text":"The RU needs at least version 2V0.6.0 oot@benetelru:~# cat /etc/benetel-rootfs-version RAN550-2V0.6.0","title":"Prerequisites"},{"location":"benetel650-install/PTPInstall/#configuration","text":"","title":"Configuration"},{"location":"benetel650-install/PTPInstall/#dawing","text":"created in https://asciiflow.com/ CU IP : 10.10.0.1 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 RU \u2502 \u2502 IP : 10.10.0.100 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 eth0\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524eno1 \u2502 \u2502 \u2502 Fiber \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Dawing"},{"location":"benetel650-install/PTPInstall/#server-side","text":"","title":"Server side"},{"location":"benetel650-install/PTPInstall/#install-ptp-server","text":"sudo apt update sudo apt install linuxptp","title":"Install ptp server"},{"location":"benetel650-install/PTPInstall/#configuration-file","text":"cat << EOF > benetel550_ptp.cfg [global] domainNumber 24 twoStepFlag 1 assume_two_step 1 #masterOnly 1 announceReceiptTimeout 3 logAnnounceInterval 1 logSyncInterval -4 logMinDelayReqInterval -4 network_transport L2 hybrid_e2e 1 EOF","title":"configuration file"},{"location":"benetel650-install/PTPInstall/#the-interface-to-the-ru","text":"the interface to the should have an ip starting with 10.10. ITF_RU=$(ip -br a | grep 10.10.0 | awk '{print $1}') echo $ITF_RU","title":"the interface to the RU"},{"location":"benetel650-install/PTPInstall/#run-the-server","text":"sudo ptp4l -2 -E -f benetel550_ptp.cfg -i $ITF_RU -m -2 means that it is ethernet multicast packets that are being sent. At this moment multicast messages will be sent on the interface eno1 example of a tcpdump on the server ad@duserver:~$ sudo tcpdump -i eno1 -en [sudo] password for ad: tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eno1, link-type EN10MB (Ethernet), capture size 262144 bytes 13:03:30.226015 ec:f4:bb:e3:b4:68 > 01:1b:19:00:00:00, ethertype Unknown (0x88f7), length 58: 0x0000: 0002 002c 1800 0200 0000 0000 0000 0000 ...,............ 0x0010: 0000 0000 ecf4 bbff fee3 b468 0001 74cc ...........h..t. 0x0020: 00fc 0000 0000 0000 0000 0000 ............","title":"run the server"},{"location":"benetel650-install/PTPInstall/#ru-side","text":"","title":"RU side"},{"location":"benetel650-install/PTPInstall/#commands-to-configure-ptp","text":"Below the command to execute to use PTP to sync root@benetelru:~# setSyncModePtp.sh 57 settings written to SMU RU Set to use PTP on next reboot. Reboot for this to take effect.","title":"Commands to configure PTP"},{"location":"benetel650-install/PTPInstall/#trace-ethernet-packets","text":"tracing ethernet packets on the RU oot@benetelru:~# tcpdump -i eth0 ether proto 0x88f7 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 12:31:00.865296 ec:f4:bb:e3:b4:68 (oui Unknown) > 01:1b:19:00:00:00 (oui Unknown), ethertype Unknown (0x88f7), length 78: 0x0000: 0b02 0040 1800 0008 0000 0000 0000 0000 ...@............ 0x0010: 0000 0000 ecf4 bbff fee3 b468 0001 0000 ...........h.... 0x0020: 0501 0000 0000 0000 0000 0000 0025 0080 .............%.. 0x0030: f8fe ffff 80ec f4bb fffe e3b4 6800 00a0 ............h...","title":"trace ethernet packets"},{"location":"benetel650-install/PTPInstall/#restart-sync-or-reboot","text":"without rebooting we can restart the sync by this command systemctl restart ru_sync when rebooting the unit the previous command is not necessary","title":"restart sync or reboot"},{"location":"benetel650-install/PTPInstall/#logs","text":"tail -f /var/log/pcm4l this is a succesful sync. It first shows the drift and ends with Time and Freq locked to Yes. E::SyncAnalysis: 2020-02-11 19:34:02 373837860 ns [3, Tracker#0] (3240) offset: -355.0 ns delay: 262.0 ns RE::SyncAnalysis: 2020-02-11 19:34:02 437792660 ns [3, Tracker#0] (3240) offset: -370.5 ns delay: 262.5 ns RE::SyncAnalysis: 2020-02-11 19:34:02 497767860 ns [3, Tracker#0] (3240) offset: -389.5 ns delay: 266.5 ns RE::SyncAnalysis: 2020-02-11 19:34:02 561789160 ns [3, Tracker#0] (3240) offset: -403.0 ns delay: 265.0 ns : RE::SyncAnalysis: 2020-02-11 19:34:06 001791300 ns [3, Tracker#0] (3240) offset: 0.0 ns delay: 269.0 ns RE::SyncAnalysis: 2020-02-11 19:34:07 377794460 ns [3, Tracker#0] (3240) offset: 0.0 ns delay: 269.0 ns : RE::Debug: 2020-02-11 19:34:07 695880440 ns [2, Supervisor] Enter time locked state RE::Debug: 2020-02-11 19:34:07 696391520 ns [2, Supervisor] Tracker#0: RE::Debug: 2020-02-11 19:34:07 701360020 ns [2, Supervisor] Master port ID: ec:f4:bb:ff:fe:e3:b4:68.1 RE::Debug: 2020-02-11 19:34:07 701947880 ns [2, Supervisor] Current reference master: Yes RE::Debug: 2020-02-11 19:34:07 702461120 ns [2, Supervisor] Freq locked: Yes RE::Debug: 2020-02-11 19:34:07 702870420 ns [2, Supervisor] Time locked: Yes","title":"logs"},{"location":"benetel650-install/PTPInstall/#reboot-the-ru","text":"rebooting the RU will make the RU sync The indication of Sync Completed in this file indicates sync is succesful root@benetelru:~# cat /tmp/logs/radio_status [INFO] Platform: RAN550_B [INFO] Radio bringup begin [INFO] Initialize TDD Pattern [INFO] Load EEPROM Data [INFO] Tx1 Attenuation set to 05580 mdB [INFO] Tx3 Attenuation set to 07660 mdB [INFO] Operating Frequency set to 3601.920 MHz [INFO] Waiting for Sync [INFO] Sync completed [INFO] Kick off Synchronization of Linux system time to PTP time [INFO] Start Radio Configuration : : [INFO] Radio bringup complete 15:54:47 up 4 min, load average: 0.08, 0.23, 0.11 another file indicates the success aswell. root@benetelru:~# cat /tmp/logs/radio_sync_status Configuring CP60 for PTP Sync Mode PTP Settings configured Directory /usr/lib/benetel OK Directory /usr/lib/benetel/splane OK Directory /usr/lib/benetel/splane/flags OK todsync started PTP4L started PCM4L started Syncmon started Waiting for initial PTP sync... PTP locked, enabling holdover functionality.","title":"reboot the RU"},{"location":"benetel650-install/PTPInstall/#source-references","text":"Software User Guide for RANx50-02 CAT-B O-RUs [June 2022] https://drive.google.com/drive/u/0/folders/17uyl3vpKXAO_9IMyiEKNQw0VpvLJvVxe","title":"Source References"},{"location":"core-install/","text":"Core Installation \u00b6 In order to have end-to-end connectivity a core network is required. If no core network is available, Open5GS can be installed in a virtual machine on the installation machine. If you already have a core you can skip this chapter. Create a Network Bridge \u00b6 In order to be able to assign a public IP address to the Virtual Machine, the network connection of the installation machine has to be bridged. This allows to add the virtual machine to the created bridge. To create a bridge, replace the network configuration of the installation machine which is assumed to be located in /etc/netplan/00-installer-config.yaml with the configuration below. You might have to edit this configuration to match your set-up. network: ethernets: $SERVER_INT : dhcp4: false : : bridges: br0: interfaces: [ $SERVER_INT ] addresses: - $SERVER_IP /24 gateway4: $GATEWAY_IP nameservers: addresses: [ 8 .8.8.8 ] version: 2 Next run the following command to test and apply the new configuration: sudo netplan try Make sure to use br0 as the $NODE_INT from now on: export NODE_INT = br0 Create a Virtual Machine \u00b6 Next you can install the virtual machine that hosts the core. The installation process is outside the scope of this document. Still we share the commandline you can use Below a command line that creates a VM with the correct settings. IMPORTANT ! the $CORE_SET_CU can only be a comma seperated list. sudo virt-install --name \" $OPEN5GS_VM_NAME \" --memory 16768 --vcpus \"sockets=1,cores= $CORE_AMOUNT_CU ,cpuset= $CORE_SET_CU \" --os-type linux --os-variant rhel7.0 --accelerate --disk \"/var/lib/libvirt/images/o5gsCORE-ubuntu-20.04.4-live-server-amd64.img,device=disk,size=100,sparse=yes,cache=none,format=qcow2,bus=virtio\" --network \"source=br0,type=bridge\" --vnc --noautoconsole --cdrom \"./ubuntu-20.04.4-live-server-amd64.iso\" --console pty,target_type = virtio some notes about this command * --noautoconsole : if you ommit this, a graphical console window will popup. This works only when the remote server can export its graphical UI to your local graphical environment like an X-windows * --console pty,target_type=virtio will make sure you can use virsh console $CU_VM_NAME Continue with virt-manager console Choose all default except for * Fill in the hostname $OPEN5GS_HOSTNAME * Fill in the static ip of $CORE_IP * select [ x ] openSsh VM installation takes 5 minutes Make sure to create a bridged network for the virtual machine and assign a fixed IP address ( $CORE_IP ) in the same subnet as $NODE_IP to it. Copy the .var file over to the core VM. scp $HOME /.vars $USER @ $CORE_IP :.vars ssh $USER @ $CORE_IP \"echo . .vars >> .profile\" ssh into the CORE VM. ssh $USER @ $CORE_IP Install Open5GS \u00b6 Please refer to the Open5GS website for information on how to install and configure the Open5GS core network on the virtual machine. NOTE : don't forget the ip forwarding section. If forgotten the UE connects with an exclemation mark in the triangle and has no internet connectivity. Configure Open5GS \u00b6 The default configuration of Open5GS can mostly be used as-is. There are a couple of modifications that have to be made to its configuration: Edit /etc/open5gs/amf.yaml and set the NGAP listen address to the public address of the virtual machine: amf : ngap : - addr : $CORE_IP Change the plmn_id: everywhere in the amf.yaml file to the one of your setup. Edit /etc/open5gs/upf.yaml and set the GTP-U listen address to the public address of the virtual machine: upf : gtpu : - addr : $CORE_IP Restart the AMF and UPF: sudo systemctl restart open5gs-amfd sudo systemctl restart open5gs-upfd open5gs GUI \u00b6 To be able to reach the GUI from any IP address add these lines [Service] Environment=HOSTNAME=0.0.0.0 Environment=PORT=3000 to the file /etc/systemd/system/multi-user.target.wants/open5gs-webui.service under de [Service] section. and restart the service systemctl daemon-reload systemctl restart open5gs-webui.service now you will find that this service is listening on port 3000 on any ip netstat -ano | grep 3000 #tcp 0 0 0.0.0.0:3000 0.0.0.0:* LISTEN off (0.00/0/0) Provision a UE \u00b6 This can be done through command line open5gs-dbctl add 001010000006309 00112233445566778899aabbccddeeff 84d4c9c08b4f482861e3a9c6c35bc4d8 internet or webgui use static ip address which you can map to the imsi. This makes debugging traffic much easier when using multiple UE's. eg: imsi 235880000009834 gets ip address 10.0.0.34 Verify if open5gs is functional \u00b6 open5gs listens on ngap interface netstat -ano | grep 38412 # sctp 10.55.7.104:38412 LISTEN 5g configuration grep -e 10 .55.7 -e m [ cn ] c /etc/open5gs/* | egrep -v : \\s *# #/etc/open5gs/amf.yaml: - addr: 10.55.7.104 #/etc/open5gs/amf.yaml: mcc: 001 #/etc/open5gs/amf.yaml: mnc: 01 #/etc/open5gs/amf.yaml: mcc: 001 #/etc/open5gs/amf.yaml: mnc: 01 #/etc/open5gs/amf.yaml: mcc: 001 #/etc/open5gs/amf.yaml: mnc: 01 #/etc/open5gs/upf.yaml: - addr: 10.55.7.104 Some scripts one often uses \u00b6 restart \u00b6 at > restartcore.sh << EOF sudo systemctl restart open5gs-mmed sudo systemctl restart open5gs-sgwcd sudo systemctl restart open5gs-smfd sudo systemctl restart open5gs-amfd sudo systemctl restart open5gs-sgwud sudo systemctl restart open5gs-upfd sudo systemctl restart open5gs-hssd sudo systemctl restart open5gs-pcrfd sudo systemctl restart open5gs-nrfd sudo systemctl restart open5gs-ausfd sudo systemctl restart open5gs-udmd sudo systemctl restart open5gs-pcfd sudo systemctl restart open5gs-udrd sudo systemctl restart open5gs-webui EOF version \u00b6 cat > versioncore.sh << EOF set -x open5gs-mmed -v open5gs-sgwcd -v open5gs-smfd -v open5gs-amfd -v open5gs-sgwud -v open5gs-upfd -v open5gs-hssd -v open5gs-pcrfd -v open5gs-nrfd -v open5gs-ausfd -v open5gs-udmd -v open5gs-pcfd -v open5gs-udrd -v open5gs-webui -v set +x EOF","title":"Core Installation"},{"location":"core-install/#core-installation","text":"In order to have end-to-end connectivity a core network is required. If no core network is available, Open5GS can be installed in a virtual machine on the installation machine. If you already have a core you can skip this chapter.","title":"Core Installation"},{"location":"core-install/#create-a-network-bridge","text":"In order to be able to assign a public IP address to the Virtual Machine, the network connection of the installation machine has to be bridged. This allows to add the virtual machine to the created bridge. To create a bridge, replace the network configuration of the installation machine which is assumed to be located in /etc/netplan/00-installer-config.yaml with the configuration below. You might have to edit this configuration to match your set-up. network: ethernets: $SERVER_INT : dhcp4: false : : bridges: br0: interfaces: [ $SERVER_INT ] addresses: - $SERVER_IP /24 gateway4: $GATEWAY_IP nameservers: addresses: [ 8 .8.8.8 ] version: 2 Next run the following command to test and apply the new configuration: sudo netplan try Make sure to use br0 as the $NODE_INT from now on: export NODE_INT = br0","title":"Create a Network Bridge"},{"location":"core-install/#create-a-virtual-machine","text":"Next you can install the virtual machine that hosts the core. The installation process is outside the scope of this document. Still we share the commandline you can use Below a command line that creates a VM with the correct settings. IMPORTANT ! the $CORE_SET_CU can only be a comma seperated list. sudo virt-install --name \" $OPEN5GS_VM_NAME \" --memory 16768 --vcpus \"sockets=1,cores= $CORE_AMOUNT_CU ,cpuset= $CORE_SET_CU \" --os-type linux --os-variant rhel7.0 --accelerate --disk \"/var/lib/libvirt/images/o5gsCORE-ubuntu-20.04.4-live-server-amd64.img,device=disk,size=100,sparse=yes,cache=none,format=qcow2,bus=virtio\" --network \"source=br0,type=bridge\" --vnc --noautoconsole --cdrom \"./ubuntu-20.04.4-live-server-amd64.iso\" --console pty,target_type = virtio some notes about this command * --noautoconsole : if you ommit this, a graphical console window will popup. This works only when the remote server can export its graphical UI to your local graphical environment like an X-windows * --console pty,target_type=virtio will make sure you can use virsh console $CU_VM_NAME Continue with virt-manager console Choose all default except for * Fill in the hostname $OPEN5GS_HOSTNAME * Fill in the static ip of $CORE_IP * select [ x ] openSsh VM installation takes 5 minutes Make sure to create a bridged network for the virtual machine and assign a fixed IP address ( $CORE_IP ) in the same subnet as $NODE_IP to it. Copy the .var file over to the core VM. scp $HOME /.vars $USER @ $CORE_IP :.vars ssh $USER @ $CORE_IP \"echo . .vars >> .profile\" ssh into the CORE VM. ssh $USER @ $CORE_IP","title":"Create a Virtual Machine"},{"location":"core-install/#install-open5gs","text":"Please refer to the Open5GS website for information on how to install and configure the Open5GS core network on the virtual machine. NOTE : don't forget the ip forwarding section. If forgotten the UE connects with an exclemation mark in the triangle and has no internet connectivity.","title":"Install Open5GS"},{"location":"core-install/#configure-open5gs","text":"The default configuration of Open5GS can mostly be used as-is. There are a couple of modifications that have to be made to its configuration: Edit /etc/open5gs/amf.yaml and set the NGAP listen address to the public address of the virtual machine: amf : ngap : - addr : $CORE_IP Change the plmn_id: everywhere in the amf.yaml file to the one of your setup. Edit /etc/open5gs/upf.yaml and set the GTP-U listen address to the public address of the virtual machine: upf : gtpu : - addr : $CORE_IP Restart the AMF and UPF: sudo systemctl restart open5gs-amfd sudo systemctl restart open5gs-upfd","title":"Configure Open5GS"},{"location":"core-install/#open5gs-gui","text":"To be able to reach the GUI from any IP address add these lines [Service] Environment=HOSTNAME=0.0.0.0 Environment=PORT=3000 to the file /etc/systemd/system/multi-user.target.wants/open5gs-webui.service under de [Service] section. and restart the service systemctl daemon-reload systemctl restart open5gs-webui.service now you will find that this service is listening on port 3000 on any ip netstat -ano | grep 3000 #tcp 0 0 0.0.0.0:3000 0.0.0.0:* LISTEN off (0.00/0/0)","title":"open5gs GUI"},{"location":"core-install/#provision-a-ue","text":"This can be done through command line open5gs-dbctl add 001010000006309 00112233445566778899aabbccddeeff 84d4c9c08b4f482861e3a9c6c35bc4d8 internet or webgui use static ip address which you can map to the imsi. This makes debugging traffic much easier when using multiple UE's. eg: imsi 235880000009834 gets ip address 10.0.0.34","title":"Provision a UE"},{"location":"core-install/#verify-if-open5gs-is-functional","text":"open5gs listens on ngap interface netstat -ano | grep 38412 # sctp 10.55.7.104:38412 LISTEN 5g configuration grep -e 10 .55.7 -e m [ cn ] c /etc/open5gs/* | egrep -v : \\s *# #/etc/open5gs/amf.yaml: - addr: 10.55.7.104 #/etc/open5gs/amf.yaml: mcc: 001 #/etc/open5gs/amf.yaml: mnc: 01 #/etc/open5gs/amf.yaml: mcc: 001 #/etc/open5gs/amf.yaml: mnc: 01 #/etc/open5gs/amf.yaml: mcc: 001 #/etc/open5gs/amf.yaml: mnc: 01 #/etc/open5gs/upf.yaml: - addr: 10.55.7.104","title":"Verify if open5gs is functional"},{"location":"core-install/#some-scripts-one-often-uses","text":"","title":"Some scripts one often uses"},{"location":"core-install/#restart","text":"at > restartcore.sh << EOF sudo systemctl restart open5gs-mmed sudo systemctl restart open5gs-sgwcd sudo systemctl restart open5gs-smfd sudo systemctl restart open5gs-amfd sudo systemctl restart open5gs-sgwud sudo systemctl restart open5gs-upfd sudo systemctl restart open5gs-hssd sudo systemctl restart open5gs-pcrfd sudo systemctl restart open5gs-nrfd sudo systemctl restart open5gs-ausfd sudo systemctl restart open5gs-udmd sudo systemctl restart open5gs-pcfd sudo systemctl restart open5gs-udrd sudo systemctl restart open5gs-webui EOF","title":"restart"},{"location":"core-install/#version","text":"cat > versioncore.sh << EOF set -x open5gs-mmed -v open5gs-sgwcd -v open5gs-smfd -v open5gs-amfd -v open5gs-sgwud -v open5gs-upfd -v open5gs-hssd -v open5gs-pcrfd -v open5gs-nrfd -v open5gs-ausfd -v open5gs-udmd -v open5gs-pcfd -v open5gs-udrd -v open5gs-webui -v set +x EOF","title":"version"},{"location":"drax-install/","text":"RIC & CU Installation \u00b6 Introduction \u00b6 This document contains only the minimal set of information to achieve a default installation of dRAX, with some assumptions made such as software and hardware prerequisites as well as Network Configuration. The first section gives a detailed overview on hardware, software and other requirements that are considered as default prerequisites to install and operate a dRAX installation. Customers who require a more custom-designed deployment should contact Accelleran's Customer Support team to get a tailored solution. The second section describes all of the steps needed to deploy and run a new software version of Accelleran's dRAX for the first time, using provided Helm charts. This section is split into multiple subsections, including one for the installation of the dRAX base, one for the 4G components, and one for the 5G components. For a first time installation, it is important to verify of course the SW and HW prerequisites presented in this document before proceeding further. The third section covers configuration of dRAX, including details on both RAN as well as xApp configuration. We advise customers who wish to know more about dRAX's architecture to request the full dRAX Architecture User Guide. Software and Hardware Prerequisites \u00b6 The assumption made in this User Guide is that the typical Customer who doesn't want a full turn key dRAX kit is familiar with Server installations, VMs and VNFs Also, as mentioned in the Overview section of this document, it is assumed that the Customer has already created a VM with a $NODE_IP address in the same subnet of the Server ( $SERVER_IP ) and a linux bridge br0 . Software Requirements have been installed in previous chapter. \u00b6 Linux Ubuntu Server 20.04 LTS Docker (recommended version 19.03, check the latest compatible version with Kubernetes) Permanently disabled swap Kubernetes 1.13 or later till 1.20 (1.21 is currently unsupported) Helm, version 3 Other Requirements \u00b6 dRAX License, as provided by Accelleran's Customer Support team The Customer Network allows access to internet services a DockerHub account, and have shared the username with the Accelleran team to provide access to the needed images EPC/5GC must be routable without NAT from dRAX (and E1000 DUs in case of 4G) From Accelleran you will need access to the Dockerhub repository please create your account with user, password and email from dockerub 4G Specific requirements: \u00b6 A DHCP server must be available on the subnet where the E1000 DUs will be installed E1000 DUs must be in the same subnet as Kubernetes' advertise address (otherwise refer to Appendix: E1000 on separate subnet ) Limitations : \u00b6 When using a graphical interface, make sure it will not go to sleep or to standby. Installation \u00b6 Introduction \u00b6 This section explains how to install dRAX for the very first time in its default configuration. Assuming that the Customer has already verified all the prerequisites described in the previous Section 4. If you already have dRAX and are only updating it, please refer to the section on updating an existing installation . dRAX consists of multiple components: RIC and Dashboard (required) 4G components based on Accelleran's E1000 DU and 4G CU 5G components based on Accelleran's 5G SA CU You should decide at this point which of these components you intend to install during this process as it will impact many of the steps. Plan your deployment \u00b6 We recommend storing all files created during this installation process inside of a dedicated folder, e.g. dRAX-yyyymmdd , so that they are clearly available for when you next update the installation. These files could also be committed to version control, or backed up to the cloud. Plan parameters \u00b6 Please determine the following parameters for your setup - these will be used during the installation process. Description Parameter Kubernetes advertise IP address $NODE_IP The interface where Kubernetes is advertising $NODE_INT Prepare License and Certificate \u00b6 In order to run Accelleran's dRAX software, a License file is required - please contact Accelleran's customer support to request the appropriate license. This license file will be named license.crt and will be used in a later step. 4G Only : If you intend to deploy the 4G aspects of dRAX (together with Accelleran's E1000 4G DUs), you will also need to prepare a certificate to ensure secure communication between the various components. Please refer to the Appendix on creating certificates . This will also need to be validated and signed by Accelleran's customer support team, so please do this in advance of attempting the installation. Namespaces \u00b6 The definition of namespaces is optional and should be avoided if there is no specific need to define them in order to separate the pods and their visibility, as it brings in a certain complexity in the installation, the creation of secrets, keys, and the execution of kubernetes commands that is worth being considered upfront. At the preference of the customer, additional Kubernetes namespaces may be used for the various components which will be installed during this process. Kubernetes namespaces should be all lowercase letters and can include the \"-\" sign. As mentioned, extra steps or flags must be used with most of the commands that follow. The following table describes the different \u201cblocks\u201d of components, and for each, a distinct namespace that may be used, as well as the default namespace where these components will be installed. Description Parameter Default Namespace Core dRAX components $NS_DRAX default dRAX 4G CUs $NS_4G_CU $NS_DRAX dRAX 5G CUs $NS_5G_CU default The Default Namespace column sometimes contains another Namespace placeholder, e.g. the NS_4G_CU default is $NS_DRAX - this means that the default behaviour is to run the CUs in the $NS_DRAX namespace, but it can be overridden. If neither $NS_DRAX nor $NS_4G_CU is specified, the CU will run in the \"default\" namespace. Install dRAX for the first time \u00b6 When you are not dealing with a new installation you can skip this chapter and move to chapter \"Updating existing installation\" install helm \u00b6 if helm is not yet installed install it this way curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh Add Accelleran Helm Chart Repo \u00b6 Use the helm command: helm repo add acc-helm https://accelleran.github.io/helm-charts/ Update Helm Charts \u00b6 To update to our latest version, we need to update the Helm charts: helm repo update Create namespace(s) for dRAX (optional) \u00b6 If you choose to use dedicated namespaces for dRAX, please create them before the installation process. export NS_DRAX=$NS_DRAX kubectl create namespace $NS_DRAX This needs to be repeated for each namespace that you wish to use for dRAX, either for the RIC, 4G or 5G components, as per the table in the Namespaces section . Warning If you choose to use specific namespaces, special care must be used throughout the remaining steps when executing the kubectl commands. For each one, it is important to specify the appropriate namespace using the -n option, example: kubectl get pods -n $NS_DRAX Configure DockerHub credentials in Kubernetes \u00b6 If you have previously obtained (from the Customer Support) access to Accelleran Dockerhub repository, you can now proceed to create a secret named accelleran-secret with your DockerHub credentials, specifically using the kubectl command (do not forget the -n <namespace> option if you selected different namespaces previously): kubectl create secret docker-registry accelleran-secret --docker-server = docker.io --docker-username = $DOCKER_USER --docker-password = $DOCKER_PASS --docker-email = $DOCKER_EMAIL This needs to be repeated for each namespace that you created previously, specifying each namespace one time using the -n flag. Configure License in Kubernetes \u00b6 Create a secret named accelleran-license using the previously provided License file. The name of this secret is critical - this name is used in our Helm charts to access the License file. Please refer to the previous section on the License file if you don't yet have one. kubectl create secret generic accelleran-license --from-file = license.crt Note: if you need for any reason to use a license file with a different (ex. myfile) name the command is a bit more cumbersome: kubectl create secret generic accelleran-license --from-file = license.crt = myfile This needs to be repeated for each namespace that you created previously, specifying each namespace one time using the -n flag. Install dRAX RIC and Dashboard \u00b6 Prepare RIC values configuration file \u00b6 We first have to prepare the Helm values configuration file for the dRAX RIC and Dashboard Helm chart. To do so, we first retrieve the default values file from the Helm chart repository and save it to a file named ric-values.yaml . We do this with the following command: curl https://raw.githubusercontent.com/accelleran/helm-charts/ ${ RIC_VERSION } /ric/simple-values/simple-values.yaml > ric-values.yaml Next, edit the newly created ric-values.yaml file. Find the following fields and edit them according to your setup. We use parameters from the Plan your deployment section, such as * $NODE_IP , to show what should be filled in In the example below we disabled 4G assuming we don't install the 4G component. global : kubeIp : $NODE_IP enable4G : false # Enable the components that you intend to install # Note that these must also be supported by the License you have Enabling 5G components \u00b6 If you plan to install the 5G components (and you have the license to support this), you need to make a few other adjustments to the ric-values.yaml file: Let the $E1_CU_IP and $F1_CU_IP be the last in the range of ip addresses in the file below. Of which the $F1_CU_IP is the last one in the range and is the odd number in the LSB of the ipv4. eg: RANGE=10.10.10.110-10.10.10.121 , E1=10.10.10.120, F1=10.10.10.121 global: enable5G: true acc-5g-infrastructure: metallb: configInline: address-pools: - name: default protocol: layer2 # IP pool used for E1, F1 and GTP interfaces when exposed outside of Kubernetes addresses: - $LOADBALANCER_IP_RANGE NOTE : The IP pool which is selected here will be used by MetalLB , which we use to expose the E1, F1, and GTP interfaces to the external O-RAN components, such as the DU, and the 5GCore. In other words, the CUCP E1, CUCP F1 and the CUUP GTP IP addresses will be taken from the specifed pool: $ kubectl get services #NAME TYPE CLUSTER-IP EXTERNAL-IP PORT> S) AGE #acc-5g-cu-cp-cucp-1-sctp-e1 LoadBalancer 10.107.230.196 192.168.88.170 38462:31859/SCTP 3h35m #acc-5g-cu-cp-cucp-1-sctp-f1 LoadBalancer 10.99.246.255 192.168.88.171 38472:30306/SCTP 3h35m #acc-5g-cu-up-cuup-1-cu-up-gtp-0 LoadBalancer 10.104.129.111 192.168.88.160 2152:30176/UDP 3h34m #acc-5g-cu-up-cuup-1-cu-up-gtp-1 LoadBalancer 10.110.90.45 192.168.88.161 2152:30816/UDP 3h34m NOTE : MetalLB works by handling ARP requests for these addresses, so the external components need to be in the same L2 subnet in order to access these interfaces. To avoid difficulties, it's recommended that this IP pool is unique in the wider network and in the same subnet of your Kubernetes Node Enabling 4G components \u00b6 4G Only : when you don't need 4G you can skip and move on to chapter Install the dRAX RIC and Dashboard where the RIC is actually being installed. If you are not planning any 4G deployment you can skip this section and proceed to the Install the dRAX RIC and Dashboard section 4G : Prepare keys and certificates for the dRAX Provisioner \u00b6 The working assumption is that keys and certificates for the dRAX Provisioner have been created by the Accelleran Support Team, however, for a more detailed guide, please check the Appendix: dRAX Provisioner - Keys and Certificates Generation of this document. 4G : Create configMaps for the dRAX Provisioner \u00b6 We now need to store the previously created keys and certificates as configMaps in Kubernetes, so that they can be used by the dRAX Provisioner: kubectl create configmap -n $NS_DRAX_4G prov-server-key --from-file = server.key kubectl create configmap -n $NS_DRAX_4G prov-server-crt --from-file = server.crt kubectl create configmap -n $NS_DRAX_4G prov-client-crt --from-file = client.crt kubectl create configmap -n $NS_DRAX_4G prov-client-key --from-file = client.key kubectl create configmap -n $NS_DRAX_4G prov-ca-crt --from-file = ca.crt Warning The names of these configmaps are critical - these names are referenced specifically in other parts of Accelleran's software. 4G : Prepare the values configuration file \u00b6 If you plan to install the 4G components (and you have the license to support this), you need to make a few other adjustments in the ric-values.yaml file we first need to enable the 4G components: global : enable4G : true Find and update the following fields with the names of the Namespaces which you've chosen to use: 4g-radio-controller : config : # The namespace where the 4G CU pods will be installed l3Namespace : \"$NS_4G_CU\" Finally, if you are using the Provisioner, you need to configure the provisioner-dhcp component. This component is using the DHCP protocol, and hence needs to know the default interface of the machine where dRAX is installed. This interface will be used to reach the cells, hence make sure the cells are reachable through the interface specified here. The configuration is located here: provisioner-dhcp : configuration : Interface : eno1 Here, change eno1 to the intended interface on your machine. 4G : Pre-provisioning the list of E1000 DUs \u00b6 If you already have access to the Accelleran E1000 DUs that you wish to use with this dRAX installation, we can pre-provision the information regarding these during installation. This can also be done later, or if new E1000 DUs are added. Each Accelleran E1000 has a Model, a Hardware Version, and a Serial Number - this information is displayed on the label attached to the unit, and is required in order to pre-provision the DUs. A unique identifier is constructed from this information in the following format: Model-HardwareVersion-SerialNumber . This identifier is then listed, along with a unique name, for each E1000. This name could be as simple as du-1 - all that matters is that it is unique in this dRAX installation. Edit the drax-4g-values.yaml file, adding a new line for each E1000 that you would like to pre-provision: configurator : provisioner : # Pre-provision the E1000 4G DUs, create a list of identifier: name as shown below cells : E1011-GC01-ACC000000000001 : du-1 E1011-GC01-ACC000000000002 : du-2 (In this example, the E1000 specific Model is E1011, the Hardware Version is GC01, and the Serial Numbers were 0001, and 0002. Update this according to the values of your E1000s.) Note If your dRAX installation and Accelleran E1000s will not be on the same subnet, after completing the previous step, please also follow Appendix: dRAX and Accelleran E1000s on different subnets . 4G : Update E1000 DUs \u00b6 The Accelleran E1000 DUs need to be updated to match the new version of dRAX. The following steps will guide you through this update process. As a prerequisite, the E1000s must be powered on, and you must be able to connect to them via SSH. If you do not have an SSH key to access the E1000s, contact Accelleran's support team. 4G : Download the E1000 update files \u00b6 There is a server included with the dRAX installation that hosts the E1000 update files. Depending on the E1000 type (FDD or TDD), you can grab those files using the following command: curl http:// $NODE_IP :30603/fdd --output fdd-update.tar.gz curl http:// $NODE_IP :30603/tdd --output tdd-update.tar.gz Note Please replace the $NODE_IP with the advertised address of your Kubernetes 4G : Update software of E1000 \u00b6 Copy the TDD or FDD image to the E1000 in /tmp/. For example: scp -i ~/guest.key tdd-update.tar.gz guest@<ip_of_e1000>:/tmp/update.tar.gz SSH into the E1000: ssh -i guest.key guest@<ip_of_e1000> Now execute: do_update.sh 4G : Verify the update of E1000 on the unit and the alignment with dRAX version \u00b6 To validate that the newly updated software matches with the installed version of dRAX, we can run the following steps: SSH into the E1000: ssh -i guest.key guest@<ip_of_e1000> Note down the Git commit of the newly installed software: strings /mnt/app/acc.tar | grep Git Now on the dRAX server, we need to retrieve the Git commit of the 4g-radio-controller to compare. Find the correct pod name using this command: kubectl get pods | grep 4g-radio-controller With the full pod name, run the following command (replace xxx with the correct identifier from the previous command): kubectl exec -it drax-4g-4g-radio-controller-xxxx -- cat /data/oranC | strings | grep Git The two commits must match, if not please verify the installation and contact Accelleran for support. Install the dRAX RIC and Dashboard \u00b6 Install the RIC and Dashboard with Helm (if installing without dedicated namespaces, leave off the -n option): helm install ric acc-helm/ric --version $RIC_VERSION --values ric-values.yaml -n $NS_DRAX Info The installation may take up to 5 minutes, it is essential that you wait till the installation is completed and all the pods are in RUNNING or COMPLETE mode, please do NOT interrupt the installation by trying to regain control of the command line To check if the installation was successful first use Helm: helm list #NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION #ric default 1 2022-08-30 12:23:24.894432912 +0000 UTC deployed ric-5.0.0 5.0.0 Than view the pods that have been created. watch kubectl get pod You should see something like this. You can ignore the status of Jaeger in this release. It is not used at the moment. NAME READY STATUS RESTARTS AGE ric-acc-fiveg-pmcounters-6d47899ccc-k2w66 1/1 Running 0 56m ric-acc-kafka-955b96786-lvkns 2/2 Running 2 56m ric-acc-kminion-57648f8c49-g89cj 1/1 Running 1 56m ric-acc-service-monitor-8766845b8-fv9md 1/1 Running 1 56m ric-acc-service-orchestrator-869996756d-kfdfp 1/1 Running 1 56m ric-cassandra-0 1/1 Running 1 56m ric-cassandra-1 1/1 Running 5 54m ric-dash-front-back-end-85db9b456c-r2l6v 1/1 Running 1 56m ric-fluent-bit-loki-jpzfc 1/1 Running 1 56m ric-grafana-7488865b58-nwqvx 1/1 Running 2 56m ric-influxdb-0 1/1 Running 1 56m ric-jaeger-agent-qn6xv 1/1 Running 1 56m ric-kube-eagle-776bf55547-55f5m 1/1 Running 1 56m ric-loki-0 1/1 Running 1 56m ric-metallb-controller-7dc7845dbc-zlmvv 1/1 Running 1 56m ric-metallb-speaker-vsvln 1/1 Running 1 56m ric-metrics-server-b4dd76cbc-hwf6d 1/1 Running 1 56m ric-nats-5g-0 3/3 Running 3 55m ric-nkafka-5g-76b6558c5f-zs4np 1/1 Running 1 56m ric-prometheus-alertmanager-7d78866cc6-svxc5 2/2 Running 2 56m ric-prometheus-kube-state-metrics-585d88b6bb-6kx5l 1/1 Running 1 56m ric-prometheus-node-exporter-pxh6w 1/1 Running 1 56m ric-prometheus-pushgateway-55b97997bf-xb2m2 1/1 Running 1 56m ric-prometheus-server-846c4bf867-ff4s5 2/2 Running 2 56m ric-redis-5g-6f9fbdbcf-j447s 1/1 Running 1 56m ric-vector-84c8b58dbc-cdtmb 1/1 Running 0 56m ric-vectorfiveg-6b8bf8fb4c-79vl7 1/1 Running 0 56m ric-zookeeper-0 1/1 Running 1 56m Install dRAX 5G Components \u00b6 Accelleran's 5G Components are managed and installed via the Dashboard. From the dRAX Dashboard sidebar, select New deployment and then click 5G CU deployment : You will reach the Deploy a new CU component page. Here, you have the ability to deploy either a CU-CP or a CU-UP component. Therefore, you first have to pick one from the drop-down menu: 5G CU-CP Installation \u00b6 When installing the 5G CU-CP component, there are a number of configuration parameters that should be filled in the Deploy a new CU component form once the CU-CP is chosen from the drop-down menu. The form with the deployment parameters is shown below: NOTE : fill in the E1 and F1 address manually according to what's set in the Preperation section in the start of this installation document. for F1 it will be the ip address we will also configure the DU with. Required Parameters \u00b6 The deployment parameters are split into required and optional ones. It is important to pay attention to certain constraints on two of the parameters in order to obtain the desired installation: The Instance ID must consist of no more than 16 lower case alphanumeric characters or '-', start with an alphabetic character, and end with an alphanumeric character (e.g. 'my-name', or 'abc-123', but not 123-cucp) The maximum number of UE that can be admitted depends also on how many ds-ctrl components get created (by default one per UE) so because occasionally at attach the UE may need two of such components, as a rule of thumb the desired maximum number of UEs must be doubled: if you intend to have at most 2 UEs, set the maximum number of UEs to 4 The required parameters are: Required Parameter Description Instance ID The instance ID of the CU-CP component - this must be unique across all CU-CP and CU-UPs Number of supported AMFs The maximum number of AMFs which can be connected to at any time Number of supported CU-UPs The maximum number of CU-UPs which can be connected to at any time Number of supported DUs The maximum number of DUs which can be connected to at any time Number of supported RUs The maximum number of RUs which can be supported at any time Number of supported UEs The maximum number of UEs which can be supported at any time Once the deployment parameters are set, click the submit button to deploy the 5G CU-CP. Optional Parameters \u00b6 The optional parameters are auto-discovered and auto-filled by dRAX. As such they do not need to be changed. However, depending on the use case, you may want to edit them. In this case, you first have to toggle the Set optional parameters to ON . The optional parameters are: Optional Parameter Description NATS URL/Port Connection details towards NATS. When installing the RIC and Dashboard component, if you set the enable5g option to true, a NATS server was deployed, which will be auto-discovered. Redis URL/Port Connection details towards Redis. Similar to NATS, if you set the enable5g option to true, a Redis server was deployed, which will be auto-discovered. dRAX Node Selector name If you label your Kubernetes node with the label draxName , you can specify the value of that label here and force the CU component to be installed on a specific node in the cluster. Namespace The namespace where the CU component should be installed. E1 Service IP Part of the CU-CP is the E1 interface. The 5G component will be exposed outside of Kubernetes on a specific IP and the E1 port of 38462. This IP is given by MetalLB, which is part of the 5G infrastructure. If this field is set to auto, MetalLB will give out the first free IP, otherwise you can specify the exact IP to be used. NOTE: The IP must be from the MetalLB IP pool defined in Enabling 5G components . F1 Service IP Similar to E1, you can specify the IP to be used for the F1 interface. NOTE: Again it has to be from the MetalLB IP pool defined in Enabling 5G components . NETCONF Server Port The NETCONF server used for configuring the 5G CU-CP component is exposed on the host machine on a random port. You can override this and set a predefined port. NOTE: The exposed port has to be in the Kubernetes NodePort range. Version This is the version of the 5G CU component. By default, the latest stable version compatible with the dRAX version is installed. Other released versions can be specified, but compatibility is not guaranteed. 5G CU-UP Installation \u00b6 When deploying the 5G CU-UP component, there is only one required parameter in the Deploy a new CU component form. The form with the deployment parameters is shown below: Required Parameters \u00b6 The required deployment parameter is: Required Parameter Description Instance ID The instance ID of the CU-UP component. As before, the Instance ID must be unique, different from the relative CU-CP and must consist of at most 16 lower case alphanumeric characters or '-', start with an alphabetic character, and end with an alphanumeric character (e.g. 'my-name', or 'abc-123'). Optional Parameters \u00b6 Optional parameters are auto-discovered and auto-filled by dRAX. As such they do not need to be changed. However, depending on the use case, you may want to edit them. In this case, you first have to toggle the Set optional parameters to ON . The optional parameters are: Optional Parameter Description NATS URL/Port The details where the NATS is located. When installing the RIC and Dashboard component, if you set the enable5g option to true, the 5G infrastructure will be deployed, which includes the 5G NATS. This NATS is auto-discovered and auto-filled here Redis URL/Port Like NATS, a 5G REDIS is deployed and autofilled dRAX Node Selector name If you label your Kubernetes node with the label draxName , you can specify the value of that label here and force the CU component to be installed on a specific node in the cluster Namespace The namespace where the CU component will be installed NETCONF Server Port The NETCONF server used for configuring the 5G CU-UP component is exposed on the host machine on a random port. You can override this and set a predefined port. NOTE: The exposed port has to be in the Kubernetes NodePort range. Version This is the version of the 5G CU component. By default, the latest stable version compatible with the dRAX version is installed. Other versions can be specified, but compatibility is not guaranteed Now the installation of CU is done. To see the pods and services execute following steps. Here is what to expect. Optional : Install xApps \u00b6 For a basic installation you can skip this chapter. Compatible xApps can be managed and installed via the Dashboard. This can be achieved by clicking on New deployment in the sidebar, and then clicking xApp deployment: In the resulting form, xApps can be deployed either from a remote Helm repository or by uploading a local packaged Helm chart file. In the \"Metadata\" section of the form, the user inputs information regarding the xApp name, the organization and team who own the xApp, the version of the xApp Helm Chart and the namespace where the xApp will be deployed on. When deploying an xApp from a remote Helm repository, the user needs to specify the name of the remote repository, its URL and the Helm chart name. Optionally, the user can upload a values configuration file to override the default configuration present in the remote Helm Chart. When deploying an xApp using the second method, the user can upload a local packaged Helm chart (a .tgz file produced by the command \"helm package <chartName>\") which contains the dRAX compatible xApp and optionally an accompanying values configuration file. Upon clicking the \"Submit\" button, the xApp will be deployed on the user-defined namespace in Kubernetes following the naming convention \"organization-team-xappname-version\". dRAX Configuration \u00b6 dRAX configuration is split into multiple subsections which mirrors the microservice approach central to dRAX's design. Most of the configuration can be managed via the Dashboard. The Dashboard is accessible at http://$NODE_IP:31315 . xApp Configuration \u00b6 xApps can be configured via our Dashboard. From the sidebar, select the xApps Management section, and then click Overview : You will be presented with a list of installed xApps - you can click on the icon in the Details column to access further information on the xApp: From the following page, you will be presented with information on the behaviour of the xApp and the topics that the xApp consumes and produces - the exact information is dependent on the vendor of the xApp. Configuration of the xApp is now managed in the Configuration Parameters section - it may need to be expanded with the collapse/expand button at the top right of the section. You can also expand the Services, by clicking the Show button in the Services column. This will show all the services used and exposed by the xApp, including the port information. 4G E1000 Provisioning \u00b6 When you don't use 4G you can skip this and go to 5G Configuration The certificates and keys referenced in this section are those mentioned in the Prepare keys and certificates for the dRAX Provisioner section . These are required so that the onboarding of new E1000s is a secure process. Listing currently provisioned E1000s \u00b6 The current list of provisioned E1000s can be retrieved with the following command: curl --cacert ca.crt https:// $NODE_IP :31610/get/ Provisioning additional Accelleran E1000 DUs \u00b6 Each additional E1000 DU, which is to be used with this dRAX installation, needs to be provisioned. This is only needed for E1000 DUs which were not pre-provisioned during the installation process. Determine Unique Identifier \u00b6 Each Accelleran E1000 has a Model, a Hardware Version, and a Serial Number - this information is displayed on the label attached to the unit, and is required in order to pre-provision the DUs. A unique identifier is constructed from this information in the following format: Model-HardwareVersion-SerialNumber This identifier can also be determined automatically via SSH using the following command: echo \" $( eeprom_vars.sh -k ) - $( eeprom_vars.sh -v ) - $( eeprom_vars.sh -s ) \" Each E1000 also needs to be given a unique name. This name could be as simple as \"du-1\" - all that matters is that it is unique in this dRAX installation. Prepare configuration file \u00b6 To provision a new E1000, create a new file called cellconfig.yaml with the following contents: E1011-GC01-ACC000000000001 : redis : hostname : $NODE_IP port : 32000 loki : hostname : $NODE_IP port : 30302 instance : filter : du-1 Replace the unique identifier based on the specific E1000, replace $KUBE_IP with the correct IP for your installation, and replace du-1 with the chosen unique name for this DU. If you'd like to provision multiple E1000s at once, duplicate the above snippet for each additional E1000, updating the unique identifier and the name in each case. Make sure to match the indentation in each duplicated snippet - incorrect indentation will result in an error. It's recommended to keep these snippets all in the same file so that we can push the new configuration with a single command. Push new configuration \u00b6 Now run the following command to push this configuration to the Provisioner: curl --cacert ca.crt --cert client.crt --key client.key https:// $NODE_IP :31610/push/ --data-binary @cellconfig.yaml Changing the name of an E1000 \u00b6 The name of a specific E1000 can be updated if required in a slightly more straightforward manner. First determine the unique identifier - refer to the Determine Unique Identifier section above for the exact instructions. Use the following command, replacing $KUBE_IP with the correct IP for your installation, the unique identifier with that just determined, and replacing du-1 with the new name: curl --cacert ca.crt --cert admin.crt --key admin.key https://_ $NODE_IP :31610_/set/E0123-GC01-ACC0123456978901/instance/filter -d du-1 4G RAN Configuration \u00b6 Configuration of the 4G RAN is made simple, intuitive and efficient when using the dRAX Dashboard. Note: all of these options require the Accelleran E1000s to already have been provisioned as described in the E1000 Provisioning section above, or during the installation process. eNB Configuration via eNB list \u00b6 To access the configuration page for an eNB, first click on the RAN Configuration section, and then click on eNB Configuration. From the displayed list of eNBs, click on the Cog icon in the Edit column corresponding to the eNB you'd like to reconfigure. From the following screen, the configuration of this eNB can be adjusted. Once the configuration has been updated as desired, click on the Create button at the bottom left of the page: Notes: Make sure the Cell ID is a multiple of 256, you can submit Cell IDs that are not a multiple of 256, however this will result in a Macro eNB ID that looks different on the surface, There is no conflict or error check in manual mode, therefore for instance it is possible to configure two cells with the same ID, set an EARFCN that is out of band, and so on: it is assumed that the User is aware of what he/she tries to set up The reference signal power is calculated automatically from the output power, please adjust the output power in dBm which represent the maximum power per channel at the exit without antenna gain eNB Configuration via Dashboard \u00b6 An alternative way of configuring an individual eNB is to make use of the Dashboard initial page (click on Dashboard in the sidebar to return there). Click on the eNB in the Network Topology, and then choose Configure Cell on the Selected Node window at the right: this will take you to the eNB Configuration page and described in the previous section. 5G RAN Configuration \u00b6 If you have a dRAX License for 5G, have enabled 5G during the RIC and Dashboard installation in Enabling 5G components , and have deployed the CU components as instructed in Install dRAX 5G Components , you can now configure the 5G CU components. You can do so by navigating to RAN Configuration in the dRAX Dashboard sidebar and clicking the gNB Configuration : You will reach the 5G CU components configuration page: On this page there are two lists, one for CU-CPs and one for CU-UPs. You can click the icon under the Edit column of each CU component to edit its configuration. When you deploy the 5G CU component and click this button for the first time, you will be asked to set the initial configuration. Later on, you can click this button to edit the configuration. 5G CU-CP configuration \u00b6 The 5G CU-CP components have a number of parameters that you can set as can be seen below: PLMN ID: The PLMN ID to be used GNB ID: The GNB ID GNB CU-CP name: A friendly name of the 5G CU-CP component AMF NG interface IP Address: You can click on the (+) sign in the table to expand it like on the figure below. You can now Add Rows to add multiple AMF NG interface IP addresses, or delete them using the Delete Row field. Edit the NG Destination IP Address to be the AMF NG IP address of your setup. This IP is the $CORE_IP. Click the Submit button to send the configuration. 5G CU-UP configuration \u00b6 The 5G CU-UP has a number of configuration parameters as seen below: GNB CU-UP ID: The 3GPP ID of the CU-UP component. GNB CU-UP name: The 3GPP friendly name of the CU-UP component, E1 Links: You can Add Row or Delete Rows using the button. Here we add the E1 IP address of the CU-CP component that this CU-UP component will connect to. Enter the E1 IP under E1 Destination IP Address. This IP is the $E1_CU_IP . Supported PLMN Slices; Expand the table by clicking the (+) sign. You can now Add Rows or Delete Rows to add multiple PLMN IDs. For each PLMN ID, you can Add Rows to add slices or Delete Rows to delete slices. Each slice is defined by the Slice Type and Slice Differentiator. Install XDP \u00b6 This chapter will improve the CU performance. go to the CU VM ssh $USER @ $NODE_IP login to docker docker login -u $DOCKER_USER -p $DOCKER_PASS Copy/Paste the below and 2 scripts are generated * startXdpAfterBoot.sh * deploy_xdpupsappl.sh Now here below the script 2 scripts that get generated by copy/pasting the below in one go. export XDP_CU_VERSION = $( helm list | grep \"cu-up\" | awk '{print $NF}' ) mkdir -p $HOME /install_ $XDP_CU_VERSION cd !$ tee startXdpAfterBoot.sh <<EOF #!/bin/bash export XDP_INSTANCE_ID=$(kubectl get pod | grep \"e1-sctp\" | sed \"s/-e1-sctp.*//g\") export XDP_CU_VERSION=$(helm list | grep \"cu-up\" | awk '{print $NF}') export XDP_GTP_IP=$NODE_IP export XDP_GTP_ITF=$(ip -br a | grep $NODE_IP | xargs | cut -d ' ' -f 1) until [ \\$(docker ps | wc -l) -ge \"15\" ] do echo \"We are waiting for first 15 docker containers.\" echo \"Current amount of docker containers running: \"\\$(docker ps | wc -l) sleep 1 done if kubectl get pods | grep ups ; then echo \"UPS pods are still running. Deleting...\" kubectl get pods --no-headers=true | awk '/ups/{print \\$1}'| xargs kubectl delete pod else echo \"UPS Pods are not present. Doing nothing.\" fi if docker ps | grep xdpupsappl ; then echo \"XDP Already running. Doing nothing.\" else echo \"XDP not found. Starting...:\" $HOME/install_$XDP_CU_VERSION/deploy_xdpupsappl.sh -i \\$XDP_INSTANCE_ID -g \\$XDP_GTP_IP -G \\$XDP_GTP_ITF -t \\$XDP_CU_VERSION docker logs xdp_cu_up fi EOF mkdir -p $HOME /install_ $XDP_CU_VERSION cd !$ cat > deploy_xdpupsappl.sh <<EOF #!/bin/bash mtu=1460 node_ip=\"\\$(kubectl get node -o jsonpath='{.items[0].status.addresses[?(.type == \"InternalIP\")].address}')\" instance_id= while getopts 'i:G:g:n:n:t:' option; do case \"\\$option\" in i) instance_id=\"\\$OPTARG\" ;; G) gtp_iface=\"\\$OPTARG\" ;; g) gtp_ip=\"\\$OPTARG\" ;; m) mtu=\"\\$OPTARG\" ;; n) node_ip=\"\\$OPTARG\" ;; t) build_tag=\"\\$OPTARG\" ;; esac done if [ -z \"\\$instance_id\" ]; then echo \"Error: no instance ID (-i) specified\" >&2 exit 1 fi if [ -z \"\\$gtp_iface\" ]; then echo \"Error: no GTP interface (-G) specified\" >&2 exit 1 fi if [ -z \"\\$gtp_ip\" ]; then echo \"Error: no GTP IP (-g) specified\" >&2 exit 1 fi if [ -z \"\\$mtu\" ]; then echo \"Error: no MTU (-m) specified\" >&2 exit 1 fi if [ -z \"\\$node_ip\" ]; then echo \"Error: no node IP (-n) specified\" >&2 exit 1 fi if [ -z \"\\$build_tag\" ]; then echo \"Error: no build tag (-t) specified\" >&2 exit 1 fi config_dir=\"\\$(mktemp -d)\" cat >\"\\$config_dir/bootstrap\" <<NESTED_EOF redis.hostname: \\$ node_ip redis.port:32200 instance.filter: \\$ instance_id NESTED_EOF cat > \"\\$config_dir/zlog.conf\" <<NESTED_EOF [global] strict init = true buffer min = 64K buffer max = 64K rotate lock file = /tmp/zlog.lock [formats] printf_format = \"%d(%b %d %H:%M:%S).%ms %8.8H %m%n\" [rules] user.* >stdout ;printf_format NESTED_EOF docker run \\ --name xdp_cu_up \\ --detach \\ --rm \\ --privileged \\ --user 0 \\ --network host \\ --volume \"\\$config_dir:/home/accelleran/5G/config:ro\" \\ --env \"IFNAME=\\$gtp_iface\" \\ --env \"NATS_SERVICE_URL=nats://\\$node_ip:31100\" \\ --env \"MTU_SIZE=\\$mtu\" \\ --env __APPNAME = cuUp \\ --env __APPID = 1 \\ --env ZLOG_CONF_PATH = /home/accelleran/5G/config/zlog.conf \\ --env BOOTSTRAP_FILENAME = /home/accelleran/5G/config/bootstrap \\ --env XDP_OBJECT_FILE = /home/accelleran/5G/xdp_gtp_kernel.o \\ --env LD_LIBRARY_PATH = /home/accelleran/5G/lib \\ --env HOSTMODE = true \\ \"accelleran/xdpupsappl:\\$build_tag\" \\ /home/accelleran/5G/xdpUpsAppl.exe \\ --uplink \"\\$gtp_ip\" \\ --downlink \"\\$gtp_ip\" \\ --bind \"\\$gtp_ip\" \\ EOF chmod 777 * In the first script startXdpAfterBoot.sh verify the 4 variables on top of the script. They are autogenerated but you need to verify them manually. XDP_INSTANCE_ID= The prefix of the output of kubectl get pod | grep e1-sctp XDP_GTP_IP= The $NODE_IP mentioned in the preperation page XDP_GTP_ITF= The interface in the CU VM holding the $NODE_IP XDP_CU_VERSION= The version of the CU. Should be the same as the $CU_VERSION given in the preperation page. Startup the xdp /home/ $USER /install_ ${ CU_VERSION } /startXdpAfterBoot.sh >> /tmp/xdp_bootscript_response Make the run of the script boot persistent by putting it in crontab crontab -e Add this line into the crontab editor file. Change the $USER and install install_directory accordingly. Check the preperation page. @reboot /home/ $USER /install_ ${ CU_VERSION } /startXdpAfterBoot.sh >> /tmp/xdp_bootscript_response execute to verify crontab -l Verifying the dRAX installation \u00b6 Monitoring via the Kubernetes API \u00b6 As specified in the previous sections of this document, the installation of Accelleran dRAX consists of multiple components. Exactly which are installed depends on the choices made during the installation process. All Pods that have been installed should be running correctly at this point though. To verify this, we can use the following command: watch \"kubectl get pods -A | grep -e ric- -e drax-4g- -e acc-5g- -e l3-\" This is what to expect NAME READY STATUS RESTARTS AGE acc-5g-cu-cp-cucp-01-amf-controller-5cb5d654fd-p75n9 1/1 Running 0 7m15s acc-5g-cu-cp-cucp-01-cu-up-controller-75859656cd-t9shf 1/1 Running 0 7m14s acc-5g-cu-cp-cucp-01-ds-ctrl-0 1/1 Running 0 7m15s acc-5g-cu-cp-cucp-01-ds-ctrl-1 1/1 Running 0 6m39s acc-5g-cu-cp-cucp-01-ds-ctrl-10 1/1 Running 0 6m18s acc-5g-cu-cp-cucp-01-ds-ctrl-11 1/1 Running 0 6m16s acc-5g-cu-cp-cucp-01-ds-ctrl-12 1/1 Running 0 6m15s acc-5g-cu-cp-cucp-01-ds-ctrl-13 1/1 Running 0 6m14s acc-5g-cu-cp-cucp-01-ds-ctrl-14 1/1 Running 0 6m13s acc-5g-cu-cp-cucp-01-ds-ctrl-15 1/1 Running 0 6m11s acc-5g-cu-cp-cucp-01-ds-ctrl-2 1/1 Running 0 6m36s acc-5g-cu-cp-cucp-01-ds-ctrl-3 1/1 Running 0 6m34s acc-5g-cu-cp-cucp-01-ds-ctrl-4 1/1 Running 0 6m31s acc-5g-cu-cp-cucp-01-ds-ctrl-5 1/1 Running 0 6m29s acc-5g-cu-cp-cucp-01-ds-ctrl-6 1/1 Running 0 6m27s acc-5g-cu-cp-cucp-01-ds-ctrl-7 1/1 Running 0 6m25s acc-5g-cu-cp-cucp-01-ds-ctrl-8 1/1 Running 0 6m23s acc-5g-cu-cp-cucp-01-ds-ctrl-9 1/1 Running 0 6m20s acc-5g-cu-cp-cucp-01-du-controller-8477b5f5c8-69j26 1/1 Running 0 7m15s acc-5g-cu-cp-cucp-01-e1-cp-0 1/1 Running 0 7m15s acc-5g-cu-cp-cucp-01-f1-ap-0 1/1 Running 0 7m15s acc-5g-cu-cp-cucp-01-f1-ap-1 1/1 Running 0 6m48s acc-5g-cu-cp-cucp-01-f1-ap-2 1/1 Running 0 6m43s acc-5g-cu-cp-cucp-01-gnb-controller-7d666fdfdd-lps9c 1/1 Running 0 7m14s acc-5g-cu-cp-cucp-01-netconf-8974d4495-f5mln 1/1 Running 0 7m15s acc-5g-cu-cp-cucp-01-ng-ap-0 1/1 Running 0 7m15s acc-5g-cu-cp-cucp-01-pm-controller-7869f89778-hf228 1/1 Running 0 7m15s acc-5g-cu-cp-cucp-01-res-mgr-cd6c87484-2v8s4 1/1 Running 0 7m15s acc-5g-cu-cp-cucp-01-rr-ctrl-0 1/1 Running 0 7m15s acc-5g-cu-cp-cucp-01-rr-ctrl-1 1/1 Running 0 6m43s acc-5g-cu-cp-cucp-01-rr-ctrl-2 1/1 Running 0 6m41s acc-5g-cu-cp-cucp-01-sctp-f46df5cfb-4kzxh 1/1 Running 0 7m15s acc-5g-cu-up-cuup-01-cu-up-0 1/1 Running 0 6m54s acc-5g-cu-up-cuup-01-cu-up-1 1/1 Running 0 6m54s acc-5g-cu-up-cuup-01-e1-sctp-up-868897844f-xh4rx 1/1 Running 0 6m54s acc-5g-cu-up-cuup-01-netconf-6746749b49-kdqbq 1/1 Running 0 6m54s acc-5g-cu-up-cuup-01-pm-controller-up-57f874bbdb-ttg5k 1/1 Running 0 6m54s acc-5g-cu-up-cuup-01-res-mgr-up-589689966c-9txd8 1/1 Running 0 6m54s busybox 1/1 Running 2 160m ric-acc-fiveg-pmcounters-6d47899ccc-k2w66 1/1 Running 0 71m ric-acc-kafka-955b96786-lvkns 2/2 Running 2 71m ric-acc-kminion-57648f8c49-g89cj 1/1 Running 1 71m ric-acc-service-monitor-8766845b8-fv9md 1/1 Running 1 71m ric-acc-service-orchestrator-869996756d-kfdfp 1/1 Running 1 71m ric-cassandra-0 1/1 Running 1 71m ric-cassandra-1 1/1 Running 5 69m ric-dash-front-back-end-85db9b456c-r2l6v 1/1 Running 1 71m ric-fluent-bit-loki-jpzfc 1/1 Running 1 71m ric-grafana-7488865b58-nwqvx 1/1 Running 2 71m ric-influxdb-0 1/1 Running 1 71m ric-jaeger-agent-qn6xv 1/1 Running 1 71m ric-jaeger-collector-55597cfbbc-r9mdh 0/1 CrashLoopBackOff 19 71m ric-jaeger-query-774f759bb6-jz7jc 1/2 CrashLoopBackOff 19 71m ric-kube-eagle-776bf55547-55f5m 1/1 Running 1 71m ric-loki-0 1/1 Running 1 71m ric-metallb-controller-7dc7845dbc-zlmvv 1/1 Running 1 71m ric-metallb-speaker-vsvln 1/1 Running 1 71m ric-metrics-server-b4dd76cbc-hwf6d 1/1 Running 1 71m ric-nats-5g-0 3/3 Running 3 70m ric-nkafka-5g-76b6558c5f-zs4np 1/1 Running 1 71m ric-prometheus-alertmanager-7d78866cc6-svxc5 2/2 Running 2 71m ric-prometheus-kube-state-metrics-585d88b6bb-6kx5l 1/1 Running 1 71m ric-prometheus-node-exporter-pxh6w 1/1 Running 1 71m ric-prometheus-pushgateway-55b97997bf-xb2m2 1/1 Running 1 71m ric-prometheus-server-846c4bf867-ff4s5 2/2 Running 2 71m ric-redis-5g-6f9fbdbcf-j447s 1/1 Running 1 71m ric-vector-84c8b58dbc-cdtmb 1/1 Running 0 71m ric-vectorfiveg-6b8bf8fb4c-79vl7 1/1 Running 0 71m ric-zookeeper-0 1/1 Running 1 71m Another check you need to do is this one. kubectl get services you can see 4 External IP addresses. Those ip addresses are the ones of the range we filled in in the ric-values.yaml file. The 2 last in the range are of the E1 and F1 service. The first two are selected the handle the GTP traffic. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE acc-5g-cu-cp-cucp-01-sctp-e1 LoadBalancer 10.104.225.53 10.55.7.130 38462:32063/SCTP 6m10s acc-5g-cu-cp-cucp-01-sctp-f1 LoadBalancer 10.103.34.228 10.55.7.131 38472:31066/SCTP 6m10s acc-5g-cu-up-cuup-01-cu-up-gtp-0 LoadBalancer 10.96.213.103 10.55.7.120 2152:32081/UDP 5m49s acc-5g-cu-up-cuup-01-cu-up-gtp-1 LoadBalancer 10.99.208.214 10.55.7.121 2152:30575/UDP 5m49s acc-service-monitor NodePort 10.104.125.9 <none> 80:30500/TCP 70m acc-service-orchestrator NodePort 10.111.157.49 <none> 80:30502/TCP 70m kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 160m netconf-cucp-01 NodePort 10.110.18.130 <none> 830:32285/TCP 6m10s netconf-cuup-01 NodePort 10.103.120.206 <none> 830:31705/TCP 5m49s ric-acc-fiveg-pmcounters NodePort 10.98.3.182 <none> 8000:30515/TCP 70m ric-acc-kafka NodePort 10.98.24.152 <none> 9092:31090/TCP,9010:32537/TCP,5556:32155/TCP 70m ric-acc-kminion ClusterIP 10.107.221.30 <none> 8080/TCP 70m ric-cassandra ClusterIP None <none> 7000/TCP,7001/TCP,7199/TCP,9042/TCP,9160/TCP 70m ric-dash-front-back-end NodePort 10.106.72.78 <none> 5000:31315/TCP 70m ric-dash-front-back-end-websocket NodePort 10.102.245.64 <none> 5001:31316/TCP 70m ric-grafana NodePort 10.96.41.39 <none> 80:30300/TCP 70m ric-influxdb ClusterIP 10.108.225.110 <none> 8088/TCP 70m ric-influxdb-api NodePort 10.105.161.178 <none> 8086:30303/TCP 70m ric-jaeger-agent ClusterIP 10.103.0.234 <none> 5775/UDP,6831/UDP,6832/UDP,5778/TCP,14271/TCP 70m ric-jaeger-collector ClusterIP 10.100.187.234 <none> 14250/TCP,14268/TCP,14269/TCP 70m ric-jaeger-query NodePort 10.97.254.197 <none> 80:31445/TCP,16687:31025/TCP 70m ric-kube-eagle ClusterIP 10.102.90.103 <none> 8080/TCP 70m ric-loki NodePort 10.108.39.131 <none> 3100:30302/TCP 70m ric-loki-headless ClusterIP None <none> 3100/TCP 70m ric-metrics-server ClusterIP 10.105.180.254 <none> 443/TCP 70m ric-nats-5g NodePort 10.107.246.192 <none> 4222:31100/TCP,6222:32053/TCP,8222:30606/TCP,7777:30168/TCP,7422:30680/TCP,7522:31616/TCP 70m ric-prometheus-alertmanager ClusterIP 10.106.127.91 <none> 80/TCP 70m ric-prometheus-kube-state-metrics ClusterIP None <none> 80/TCP,81/TCP 70m ric-prometheus-node-exporter ClusterIP None <none> 9100/TCP 70m ric-prometheus-pushgateway ClusterIP 10.105.167.58 <none> 9091/TCP 70m ric-prometheus-server NodePort 10.97.205.182 <none> 80:30304/TCP 70m ric-redis-5g NodePort 10.96.155.105 <none> 6379:32200/TCP 70m ric-zookeeper NodePort 10.109.78.254 <none> 2181:30305/TCP 70m ric-zookeeper-headless ClusterIP None <none> 2181/TCP,3888/TCP,2888/TCP 70m The listed Pods should either all be Running and fully Ready (i.e. all expected instances are running - 1/1, 2/2, etc.), or Completed - it may take a few minutes to reach this state. The number of restarts for each pod should also stabilize and stop increasing. If something crashes or you need to restart a pod, you can use the scale command - for example: kubectl scale deployment $DEPLOYMENT_NAME --replicas = 0 kubectl scale deployment $DEPLOYMENT_NAME --replicas = 1 SCTP connections setup \u00b6 Check services and verify E1 and F1 ip address. kubectl get services Verify SCTP connection is setup. Expecting HB REQ and HB ACK tracing with this tcpdump commandline. sudo tcpdump -i any or 38462 The Grafana Accelleran dRAX System Dashboard \u00b6 The dRAX Grafana contains two system health dashboards, one for 4G and one for 5G. On these dashboards we can check if all the 4G or 5G components are running, if there are Kubernetes pods that are in an error state, we can check the RAM and CPU usage per pod, etc. In fact, they give a complete overview of the System in 5 sections, each containing detailed information to allow a graphic, intuitive approach to the System health conditions. Each section can be expanded by clicking on its name. Cluster Information contains: Number of nodes Allocatable RAM Running PODs Pending Containers Crashed Containers Pending or Crashed containers listed by Node, Namespace, Status, POD name and package RAM Information contains: Total RAM Usage Node RAM Usage POD RAM Usage Container RAM Usage RAM Usage History Node RAM Info listed by Node, Requested RAM Limit RAM Allocatable RAM RAM Reserved, RAM Usage POD RAM information listed by Node, Pod, Requested RAM, RAM Limit, Used RAM Container RAM information listed by Node, Pod, Requested RAM, RAM Limit, Used RAM CPU Information contains: Total CPU Usage Node CPU Usage POD CPU Usage Container CPU Usage CPU Usage History Node CPU Info listed by Node, Requested Core Limit Cores Allocatable Cores CPU Reserved,CPU Burstable, CPU Usage POD CPU information listed by Node, Pod, Requested Cores, Limit Cores, Used Cores Container CPU information listed by Node, Pod, Requested Cores, Limit Cores, Used Cores Network Information contains: TX Traffic RX Traffic Disk Space information contains: Disk Usage Disk Usage History Disk Usage per Node listed by Node, User Disk Space, Free Disk Space, Total Disk Space Persistent Disk Volumes listed by Node, Volume Name,Disk Space, Bound status 4G system health dashboard \u00b6 To access the dRAX Grafana, browse to http://$NODE_IP:30300 . From here you can browse the different pre-built Grafana dashboards that come with dRAX. One of them is the Accelleran dRAX System Dashboard : The 4G specific health dashboard, in addition to the 5 global sections explained above, also shows which components of 4G dRAX are running (Redis, NATS, 4GRC, etc.). 5G system health dashboard \u00b6 The 5G system health dashboard can also be found on dRAX Grafana on http://$NODE_IP:30300 . This time, pick the Accelleran dRAX 5G System Dashboard from the list of pre-built Grafana dashboards: The 5G specific health dashboard, in addition to the 5 global sections explained above, also shows which components of 5G dRAX are running (AMF Controller, CUUP, DS Ctrl, etc.). Appendix: How to enable/disable DHCP for the IP address of the E1000 4G DU \u00b6 The 4G DU units are separate hardware components and therefore get preconfigured at Accelleran with a standard SW image which of course will have default settings that may require changes. Typically in fact a network component will require IP address Netmask default gateway to be configured and the Customer will want to rework these settings before commissioning the component into the existing Network. The default settings are: Static IP address DHCP Off Provisioner Off Bootstrap file with Redis port 32000, dRAX IP 10.20.20.20 and a generic eNB Name indicating for instance the Customer name, Band, and a progressing number The rest of environment variables are visible once logged in to the E1000 using the fprintenv command. So for instance the variable you will be able to see are: ethact=octeth0 ethaddr=1C:49:7B:DE:35:F7 fdtcontroladdr=80000 gatewayip=10.188.6.1 gpsenable=1 ipaddr=10.20.20.222 loadaddr=0x20000000 mtdparts=mtdparts=octeon_nor0:0x220000(u-boot)ro,128k(u-boot-env)ro,128k(u-boot-env-bak)ro,128k(u-boot-env-gemtek)ro,0x1340000(init-app)ro,-(accelleran-app)ro namedalloc=namedalloc dsp-dump 0x400000 0x7f4D0000; namedalloc pf4cazac 0x13000 0x84000000; namedalloc cazac 0x630000 0x7f8D0000; namedalloc cpu-dsp-if 0x100000 0x7ff00000; namedalloc dsp-log-buf 0x4000000 0x80000000; namedalloc initrd 0x2B00000 0x30800000; netmask=255.255.255.0 numcores=4 octeon_failsafe_mode=0 octeon_ram_mode=0 serverip=10.188.6.137 stderr=serial stdin=serial stdout=serial swloadby=flash unprotect=protect off 0x17cc0000 0x1fc00000; ver=U-Boot 2017.05 (Sep 08 2017 - 16:27:53 +0000) xoservoenable=1 xoservolog=/var/log/xolog.txt dhcp=yes If the Customers wants to change IP address using the command line he can do the following (special attention must be put as an error in the input can bring the E1000 out of reach): fsetenv ipaddr <new_ip_address> In order to modify the netmask type: fsetenv netmask <new_net_mask> ( ex. 255 .255.255.0 ) NOTE: the User that wishes to perform such modifications must be aware of the consequences of that choice, for instance the necessity of shipping back the unit to Accelleran for refurbishment in case of misconfigurations. In case the E1000 is supposed to get a dynamic address from a DHCP server in the Customer network the related flag shall be enable: fsetenv dhcp yes Don't forget to reboot the E1000 once done with the settings. If for any reasons the Customer decides not to use the dynamic address assignment he can revert the choice by typing: fsetenv dhcp no IMPORTANT : In this case it is also essential to configure a static address and an IP Mask in harmony with the rest of the network: fsetenv ipaddr <new_ip_address> fsetenv netmask <new_net_mask> ( ex. 255 .255.255.0 ) After that , you can reboot the E1000 which will come back with ARP signals with the chosen static address. How to configure dRAX for a cell \u00b6 Introduction \u00b6 For the cell to be able to communicate with the dRAX Provisioner, it needs to use its own certificate. For this certificate to be valid, the time and date on the cell need to be synchronized. If the time and date are not correct, the certificates will not work. How to enable the dRAX Provisioner \u00b6 SSH into the E1000: ssh -i guest.key guest@<ip_of_e1000> Create a folder: mkdir /mnt/common/bootstrap_source Create and save an empty dhcp file in the folder created: touch /mnt/common/bootstrap_source/dhcp Appendix: dRAX and Accelleran E1000s on different subnets \u00b6 Normally, we recommend your Accelleran E1000s are located on the same subnet as your dRAX. However, if that is not the case, then you need to run a specific component of the dRAX Provisioner called the Provisioner-DHCP. This component should be running on any machine that is part of the subnet where the Accelleran E1000s are. We support running this component via Docker, so you must have Docker installed on that machine. Prepare conf file \u00b6 Create the configuration file named udhcpd.conf. The contents are: start 10.0.0.20 end 10.255.255.254 interface eth0 opt dns 10.0.0.1 10.0.0.2 option subnet 255.0.0.0 opt router 10.0.0.1 opt wins 10.0.0.2 option domain local option lease 86400 option provision https://$NODE_IP:31610 option staticroutes 10.20.20.0/24 10.22.10.52 Substitute the IP in the \"option provision \u2026\" line to where the provisioner is installed. Also change the interface to the main interface of the machine (eth0, eno1, br0, \u2026) that is used to reach the subnet where the dRAX is installed. NOTE: You should make sure from your networking aspect that the two subnets are reachable by one-another. If this is not the case, although we do not recommend this, you can create a static route on the E1000s towards the subnet where dRAX is installed. This can be done using the Provisioner-DHCP component. Find the line: option staticroutes 10.20.20.0/24 10.22.10.52 . This translates to \"create a static route to the 10.20.20.0/24 network (where the dRAX is) via gateway 10.22.10.52\". Replace the values with the correct ones for your network case. Create docker login details \u00b6 Login with docker command: sudo docker login Then use the username and password of your DockerHub account that you also used to create the kubernetes secret. Pull the image from DockerHub \u00b6 Check what is the latest version on DockerHub https://hub.docker.com/repository/docker/accelleran/provisioner-dhcp . Pull the image using the docker command, and substitute the <version> with the one found in the above step: sudo docker image pull accelleran/provisioner-dhcp:<version> Run as docker container \u00b6 Start the container with the docker run command. Make sure to give the full path to the configuration file (/home/ad/...). Also make sure you give the correct docker image name at the end of the command including the version: sudo docker run -d --name dhcp --net host --mount type = bind,source = /path/to/udhcpd.conf,target = /conf/udhcpd.conf accelleran/provisioner-dhcp:<version> To check if the service is running use sudo docker ps | grep dhcp Appendix: dRAX Provisioner - Keys and Certificates Generation \u00b6 In general, TLS certificates only allow you to connect to a server if the URL of the server matches one of the subjects in the certificate configuration. This section assumes the usage of openssl to handle TLS security due to its flexibility, even if it is both complex to use and easy to make mistakes. Customers can choose to use different options to generate the keys and certificates as long as of course the final output matches the content of this section. Create the certificates \u00b6 Create the server.key \u00b6 First thing is to create a key (if it doesn't exist yet): openssl genrsa -out server.key 4096 This command will create a RSA based server key with a key length of 4096 bits. Create a server certificate \u00b6 First, create the cert.conf . Create a file like the example below and save it as cert.conf : [ req ] default_bits = 2048 default_keyfile = server-key.pem distinguished_name = subject req_extensions = req_ext x509_extensions = req_ext string_mask = utf8only [ subject ] countryName = Country Name (2 letter code) countryName_default = BE stateOrProvinceName = State or Province Name (full name) stateOrProvinceName_default = Example state localityName = Locality Name (eg, city) localityName_default = Example city organizationName = Organization Name (eg, company) organizationName_default = Example company commonName = Common Name (e.g. server FQDN or YOUR name) commonName_default = Example Company emailAddress = Email Address emailAddress_default = test@example.com [ req_ext ] subjectKeyIdentifier = hash basicConstraints = CA:FALSE keyUsage = digitalSignature, keyEncipherment subjectAltName = @alternate_names nsComment = \"OpenSSL Generated Certificate\" [ alternate_names ] DNS.1 = localhost IP.1 = 10.0.0.1 IP.2 = 10.20.20.20 Fill in the details, like the country, company name, etc. IMPORTANT: Edit the last line of the file. IP.2 should be equal to IP where the provisioner will be running. This is the $NODE_IP from the planning phase. The default is set to 10.20.20.20. To create the server certificate, use the following command: openssl req -new -key server.key -config cert.conf -out server.csr -batch Command explanation: openssl req -new : create a new certificate -key server.key : use server.key as the private half of the certificate -config cert.conf : use the configuration as a template -out server.csr : generate a csr -batch : don't ask about the configuration on the terminal Create a self-signed client certificate \u00b6 To create the client certificate, use the following command: openssl req -newkey rsa:4096 -nodes -keyout client.key -sha384 -x509 -days 3650 -out client.crt -subj /C = XX/ST = YY/O = RootCA This command will create a client.key and client.crt from scratch to use for TLS-based authentication, in details the options are: openssl req : create a certificate -newkey rsa:4096 : create a new client key -nodes : do not encrypt the newly create client key with a passphrase (other options are -aes) -keyout client.key : write the key to client.key -x509 : sign this certificate immediately -sha384 : use sha384 for signing the certificate` -days 3650 : this certificate is valid for ten years -subj /C=XX/ST=YY/O=RootCA : use some default configuration -out client.crt : write the certificate to client.crt Sign the server certificate using the root certificate authority key \u00b6 The server certificate needs to be signed by Accelleran. To do so, please contact the Accelleran Customers Support Team and send us the following files you created previously: server.csr cert.conf You will receive from Accelleran the following files: signed server.crt ca.crt Verify the certificates work \u00b6 The following commands should be used and return both OK: openssl verify -CAfile client.crt client.crt openssl verify -CAfile ca.crt server.crt Appendix: Configure 4G Radio Controller \u00b6 In order for the dRAX 4G components to function properly, we need to configure the 4G Radio Controller. This can be done from the Dashboard, which is accessible at http://$NODE_IP:31315 . From the sidebar, select the xApps Management section, and then click Overview : From the dRAX Core section, find the 4G-Radio-Controller entry, and click on the corresponding cog icon in the Configure column, as shown in the picture below: You will be presented with a configuration page - the following changes should be made, making sure to replace $NODE_IP with the value from your installation: The parameters are: Automatic Handover: If set to true, the 4G default handover algorithm is activated, based on the A3 event. If set to false, the A3 event from the UE is ignored by dRAX and the handover will not be triggered. Publish Measurement Data: Publish UE Data: Measurement Type: Orchestrator URL: This should be the $KUBE_IP:6443, so the Kubernetes advertise address and using the secure port 6443 Appendix: License Error Codes \u00b6 Sometimes you might run into issues when trying to launch dRAX due to a licensing error. A list of possible error codes is provided below: ID Tag Explanation E001 ENotInEnv Environment variable not set E002 EInvalidUTF8 The content of the environment varable is not valid UTF8 E003 ECannotOpen Cannot open license file, was it added as a secret with the right name? To verify whether it's loaded correctly, run: bash kubectl get secret accelleran-license -o'jsonpath={.data.license\\\\.crt}' which should give you a base64 encoded dump. E004 ELicenseExpired Your license is expired! You'll likely need a new license from Accelleran E005 EDecryption An error occurred during decryption E006 EVerification An error occurred during verification E007 EMissingPermission You do not have the permissions to execute the software. You'll likely need a more permissive license from Accelleran. E008 ESOError Inner function returned an error E009 ERunFn Cannot find the correct function in the library E010 ELoadLibrary Cannot load the .so file E011 ETryWait An error occurred while waiting for the subprocess to return E012 ESpawn Could not spawn subprocess E013 EWriteDecrypted Cannot write to file descriptor E014 EMemFd Cannot open memory file descriptor E015 ECypher Cannot create cypher Appendix : Remove existing deployments \u00b6 In order to continue with the remaining steps, we remove the existing deployments of our charts. Note that this will not remove the data, so any configured components should remain once the installation is completed. It may be that the previous versions used different names for the Helm deployments, so to check the correct names we can use the helm list command: helm list #NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION #acc-5g-cu-cp default 1 2022-01-21 15:16:35.893230618 +0000 UTC deployed acc-5g-cu-cp-3.0.0 release-2.3-duvel-8b8d7f05 #acc-5g-cu-up default 1 2022-01-21 15:16:44.931753616 +0000 UTC deployed acc-5g-cu-up-3.0.0 release-2.3-duvel-8b8d7f05 #acc-helm-ric default 1 2022-01-09 17:20:52.860528687 +0000 UTC deployed ric-4.0.0 4.0.0 In the above example, the installations are called acc-5g-cu-cp , acc-5g-cu-up and ric , so the required commands would be: helm uninstall acc-5g-cu-cp helm uninstall acc-5g-cu-up helm uninstall ric Please wait until all the pods and resources of the previous dRAX installation are deleted. You can view them by: watch kubectl get pods -A You can now continue with the remaining steps.","title":"RIC & CU Installation"},{"location":"drax-install/#ric-cu-installation","text":"","title":"RIC &amp; CU Installation"},{"location":"drax-install/#introduction","text":"This document contains only the minimal set of information to achieve a default installation of dRAX, with some assumptions made such as software and hardware prerequisites as well as Network Configuration. The first section gives a detailed overview on hardware, software and other requirements that are considered as default prerequisites to install and operate a dRAX installation. Customers who require a more custom-designed deployment should contact Accelleran's Customer Support team to get a tailored solution. The second section describes all of the steps needed to deploy and run a new software version of Accelleran's dRAX for the first time, using provided Helm charts. This section is split into multiple subsections, including one for the installation of the dRAX base, one for the 4G components, and one for the 5G components. For a first time installation, it is important to verify of course the SW and HW prerequisites presented in this document before proceeding further. The third section covers configuration of dRAX, including details on both RAN as well as xApp configuration. We advise customers who wish to know more about dRAX's architecture to request the full dRAX Architecture User Guide.","title":"Introduction"},{"location":"drax-install/#software-and-hardware-prerequisites","text":"The assumption made in this User Guide is that the typical Customer who doesn't want a full turn key dRAX kit is familiar with Server installations, VMs and VNFs Also, as mentioned in the Overview section of this document, it is assumed that the Customer has already created a VM with a $NODE_IP address in the same subnet of the Server ( $SERVER_IP ) and a linux bridge br0 .","title":"Software and Hardware Prerequisites"},{"location":"drax-install/#software-requirements-have-been-installed-in-previous-chapter","text":"Linux Ubuntu Server 20.04 LTS Docker (recommended version 19.03, check the latest compatible version with Kubernetes) Permanently disabled swap Kubernetes 1.13 or later till 1.20 (1.21 is currently unsupported) Helm, version 3","title":"Software Requirements have been installed in previous chapter."},{"location":"drax-install/#other-requirements","text":"dRAX License, as provided by Accelleran's Customer Support team The Customer Network allows access to internet services a DockerHub account, and have shared the username with the Accelleran team to provide access to the needed images EPC/5GC must be routable without NAT from dRAX (and E1000 DUs in case of 4G) From Accelleran you will need access to the Dockerhub repository please create your account with user, password and email from dockerub","title":"Other Requirements"},{"location":"drax-install/#4g-specific-requirements","text":"A DHCP server must be available on the subnet where the E1000 DUs will be installed E1000 DUs must be in the same subnet as Kubernetes' advertise address (otherwise refer to Appendix: E1000 on separate subnet )","title":"4G Specific requirements:"},{"location":"drax-install/#limitations","text":"When using a graphical interface, make sure it will not go to sleep or to standby.","title":"Limitations :"},{"location":"drax-install/#installation","text":"","title":"Installation"},{"location":"drax-install/#introduction_1","text":"This section explains how to install dRAX for the very first time in its default configuration. Assuming that the Customer has already verified all the prerequisites described in the previous Section 4. If you already have dRAX and are only updating it, please refer to the section on updating an existing installation . dRAX consists of multiple components: RIC and Dashboard (required) 4G components based on Accelleran's E1000 DU and 4G CU 5G components based on Accelleran's 5G SA CU You should decide at this point which of these components you intend to install during this process as it will impact many of the steps.","title":"Introduction"},{"location":"drax-install/#plan-your-deployment","text":"We recommend storing all files created during this installation process inside of a dedicated folder, e.g. dRAX-yyyymmdd , so that they are clearly available for when you next update the installation. These files could also be committed to version control, or backed up to the cloud.","title":"Plan your deployment"},{"location":"drax-install/#plan-parameters","text":"Please determine the following parameters for your setup - these will be used during the installation process. Description Parameter Kubernetes advertise IP address $NODE_IP The interface where Kubernetes is advertising $NODE_INT","title":"Plan parameters"},{"location":"drax-install/#prepare-license-and-certificate","text":"In order to run Accelleran's dRAX software, a License file is required - please contact Accelleran's customer support to request the appropriate license. This license file will be named license.crt and will be used in a later step. 4G Only : If you intend to deploy the 4G aspects of dRAX (together with Accelleran's E1000 4G DUs), you will also need to prepare a certificate to ensure secure communication between the various components. Please refer to the Appendix on creating certificates . This will also need to be validated and signed by Accelleran's customer support team, so please do this in advance of attempting the installation.","title":"Prepare License and Certificate"},{"location":"drax-install/#namespaces","text":"The definition of namespaces is optional and should be avoided if there is no specific need to define them in order to separate the pods and their visibility, as it brings in a certain complexity in the installation, the creation of secrets, keys, and the execution of kubernetes commands that is worth being considered upfront. At the preference of the customer, additional Kubernetes namespaces may be used for the various components which will be installed during this process. Kubernetes namespaces should be all lowercase letters and can include the \"-\" sign. As mentioned, extra steps or flags must be used with most of the commands that follow. The following table describes the different \u201cblocks\u201d of components, and for each, a distinct namespace that may be used, as well as the default namespace where these components will be installed. Description Parameter Default Namespace Core dRAX components $NS_DRAX default dRAX 4G CUs $NS_4G_CU $NS_DRAX dRAX 5G CUs $NS_5G_CU default The Default Namespace column sometimes contains another Namespace placeholder, e.g. the NS_4G_CU default is $NS_DRAX - this means that the default behaviour is to run the CUs in the $NS_DRAX namespace, but it can be overridden. If neither $NS_DRAX nor $NS_4G_CU is specified, the CU will run in the \"default\" namespace.","title":"Namespaces"},{"location":"drax-install/#install-drax-for-the-first-time","text":"When you are not dealing with a new installation you can skip this chapter and move to chapter \"Updating existing installation\"","title":"Install dRAX for the first time"},{"location":"drax-install/#install-helm","text":"if helm is not yet installed install it this way curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh","title":"install helm"},{"location":"drax-install/#add-accelleran-helm-chart-repo","text":"Use the helm command: helm repo add acc-helm https://accelleran.github.io/helm-charts/","title":"Add Accelleran Helm Chart Repo"},{"location":"drax-install/#update-helm-charts","text":"To update to our latest version, we need to update the Helm charts: helm repo update","title":"Update Helm Charts"},{"location":"drax-install/#create-namespaces-for-drax-optional","text":"If you choose to use dedicated namespaces for dRAX, please create them before the installation process. export NS_DRAX=$NS_DRAX kubectl create namespace $NS_DRAX This needs to be repeated for each namespace that you wish to use for dRAX, either for the RIC, 4G or 5G components, as per the table in the Namespaces section . Warning If you choose to use specific namespaces, special care must be used throughout the remaining steps when executing the kubectl commands. For each one, it is important to specify the appropriate namespace using the -n option, example: kubectl get pods -n $NS_DRAX","title":"Create namespace(s) for dRAX (optional)"},{"location":"drax-install/#configure-dockerhub-credentials-in-kubernetes","text":"If you have previously obtained (from the Customer Support) access to Accelleran Dockerhub repository, you can now proceed to create a secret named accelleran-secret with your DockerHub credentials, specifically using the kubectl command (do not forget the -n <namespace> option if you selected different namespaces previously): kubectl create secret docker-registry accelleran-secret --docker-server = docker.io --docker-username = $DOCKER_USER --docker-password = $DOCKER_PASS --docker-email = $DOCKER_EMAIL This needs to be repeated for each namespace that you created previously, specifying each namespace one time using the -n flag.","title":"Configure DockerHub credentials in Kubernetes"},{"location":"drax-install/#configure-license-in-kubernetes","text":"Create a secret named accelleran-license using the previously provided License file. The name of this secret is critical - this name is used in our Helm charts to access the License file. Please refer to the previous section on the License file if you don't yet have one. kubectl create secret generic accelleran-license --from-file = license.crt Note: if you need for any reason to use a license file with a different (ex. myfile) name the command is a bit more cumbersome: kubectl create secret generic accelleran-license --from-file = license.crt = myfile This needs to be repeated for each namespace that you created previously, specifying each namespace one time using the -n flag.","title":"Configure License in Kubernetes"},{"location":"drax-install/#install-drax-ric-and-dashboard","text":"","title":"Install dRAX RIC and Dashboard"},{"location":"drax-install/#prepare-ric-values-configuration-file","text":"We first have to prepare the Helm values configuration file for the dRAX RIC and Dashboard Helm chart. To do so, we first retrieve the default values file from the Helm chart repository and save it to a file named ric-values.yaml . We do this with the following command: curl https://raw.githubusercontent.com/accelleran/helm-charts/ ${ RIC_VERSION } /ric/simple-values/simple-values.yaml > ric-values.yaml Next, edit the newly created ric-values.yaml file. Find the following fields and edit them according to your setup. We use parameters from the Plan your deployment section, such as * $NODE_IP , to show what should be filled in In the example below we disabled 4G assuming we don't install the 4G component. global : kubeIp : $NODE_IP enable4G : false # Enable the components that you intend to install # Note that these must also be supported by the License you have","title":"Prepare RIC values configuration file"},{"location":"drax-install/#enabling-5g-components","text":"If you plan to install the 5G components (and you have the license to support this), you need to make a few other adjustments to the ric-values.yaml file: Let the $E1_CU_IP and $F1_CU_IP be the last in the range of ip addresses in the file below. Of which the $F1_CU_IP is the last one in the range and is the odd number in the LSB of the ipv4. eg: RANGE=10.10.10.110-10.10.10.121 , E1=10.10.10.120, F1=10.10.10.121 global: enable5G: true acc-5g-infrastructure: metallb: configInline: address-pools: - name: default protocol: layer2 # IP pool used for E1, F1 and GTP interfaces when exposed outside of Kubernetes addresses: - $LOADBALANCER_IP_RANGE NOTE : The IP pool which is selected here will be used by MetalLB , which we use to expose the E1, F1, and GTP interfaces to the external O-RAN components, such as the DU, and the 5GCore. In other words, the CUCP E1, CUCP F1 and the CUUP GTP IP addresses will be taken from the specifed pool: $ kubectl get services #NAME TYPE CLUSTER-IP EXTERNAL-IP PORT> S) AGE #acc-5g-cu-cp-cucp-1-sctp-e1 LoadBalancer 10.107.230.196 192.168.88.170 38462:31859/SCTP 3h35m #acc-5g-cu-cp-cucp-1-sctp-f1 LoadBalancer 10.99.246.255 192.168.88.171 38472:30306/SCTP 3h35m #acc-5g-cu-up-cuup-1-cu-up-gtp-0 LoadBalancer 10.104.129.111 192.168.88.160 2152:30176/UDP 3h34m #acc-5g-cu-up-cuup-1-cu-up-gtp-1 LoadBalancer 10.110.90.45 192.168.88.161 2152:30816/UDP 3h34m NOTE : MetalLB works by handling ARP requests for these addresses, so the external components need to be in the same L2 subnet in order to access these interfaces. To avoid difficulties, it's recommended that this IP pool is unique in the wider network and in the same subnet of your Kubernetes Node","title":"Enabling 5G components"},{"location":"drax-install/#enabling-4g-components","text":"4G Only : when you don't need 4G you can skip and move on to chapter Install the dRAX RIC and Dashboard where the RIC is actually being installed. If you are not planning any 4G deployment you can skip this section and proceed to the Install the dRAX RIC and Dashboard section","title":"Enabling 4G components"},{"location":"drax-install/#4g-prepare-keys-and-certificates-for-the-drax-provisioner","text":"The working assumption is that keys and certificates for the dRAX Provisioner have been created by the Accelleran Support Team, however, for a more detailed guide, please check the Appendix: dRAX Provisioner - Keys and Certificates Generation of this document.","title":"4G : Prepare keys and certificates for the dRAX Provisioner"},{"location":"drax-install/#4g-create-configmaps-for-the-drax-provisioner","text":"We now need to store the previously created keys and certificates as configMaps in Kubernetes, so that they can be used by the dRAX Provisioner: kubectl create configmap -n $NS_DRAX_4G prov-server-key --from-file = server.key kubectl create configmap -n $NS_DRAX_4G prov-server-crt --from-file = server.crt kubectl create configmap -n $NS_DRAX_4G prov-client-crt --from-file = client.crt kubectl create configmap -n $NS_DRAX_4G prov-client-key --from-file = client.key kubectl create configmap -n $NS_DRAX_4G prov-ca-crt --from-file = ca.crt Warning The names of these configmaps are critical - these names are referenced specifically in other parts of Accelleran's software.","title":"4G : Create configMaps for the dRAX Provisioner"},{"location":"drax-install/#4g-prepare-the-values-configuration-file","text":"If you plan to install the 4G components (and you have the license to support this), you need to make a few other adjustments in the ric-values.yaml file we first need to enable the 4G components: global : enable4G : true Find and update the following fields with the names of the Namespaces which you've chosen to use: 4g-radio-controller : config : # The namespace where the 4G CU pods will be installed l3Namespace : \"$NS_4G_CU\" Finally, if you are using the Provisioner, you need to configure the provisioner-dhcp component. This component is using the DHCP protocol, and hence needs to know the default interface of the machine where dRAX is installed. This interface will be used to reach the cells, hence make sure the cells are reachable through the interface specified here. The configuration is located here: provisioner-dhcp : configuration : Interface : eno1 Here, change eno1 to the intended interface on your machine.","title":"4G : Prepare the values configuration file"},{"location":"drax-install/#4g-pre-provisioning-the-list-of-e1000-dus","text":"If you already have access to the Accelleran E1000 DUs that you wish to use with this dRAX installation, we can pre-provision the information regarding these during installation. This can also be done later, or if new E1000 DUs are added. Each Accelleran E1000 has a Model, a Hardware Version, and a Serial Number - this information is displayed on the label attached to the unit, and is required in order to pre-provision the DUs. A unique identifier is constructed from this information in the following format: Model-HardwareVersion-SerialNumber . This identifier is then listed, along with a unique name, for each E1000. This name could be as simple as du-1 - all that matters is that it is unique in this dRAX installation. Edit the drax-4g-values.yaml file, adding a new line for each E1000 that you would like to pre-provision: configurator : provisioner : # Pre-provision the E1000 4G DUs, create a list of identifier: name as shown below cells : E1011-GC01-ACC000000000001 : du-1 E1011-GC01-ACC000000000002 : du-2 (In this example, the E1000 specific Model is E1011, the Hardware Version is GC01, and the Serial Numbers were 0001, and 0002. Update this according to the values of your E1000s.) Note If your dRAX installation and Accelleran E1000s will not be on the same subnet, after completing the previous step, please also follow Appendix: dRAX and Accelleran E1000s on different subnets .","title":"4G : Pre-provisioning the list of E1000 DUs"},{"location":"drax-install/#4g-update-e1000-dus","text":"The Accelleran E1000 DUs need to be updated to match the new version of dRAX. The following steps will guide you through this update process. As a prerequisite, the E1000s must be powered on, and you must be able to connect to them via SSH. If you do not have an SSH key to access the E1000s, contact Accelleran's support team.","title":"4G : Update E1000 DUs"},{"location":"drax-install/#4g-download-the-e1000-update-files","text":"There is a server included with the dRAX installation that hosts the E1000 update files. Depending on the E1000 type (FDD or TDD), you can grab those files using the following command: curl http:// $NODE_IP :30603/fdd --output fdd-update.tar.gz curl http:// $NODE_IP :30603/tdd --output tdd-update.tar.gz Note Please replace the $NODE_IP with the advertised address of your Kubernetes","title":"4G : Download the E1000 update files"},{"location":"drax-install/#4g-update-software-of-e1000","text":"Copy the TDD or FDD image to the E1000 in /tmp/. For example: scp -i ~/guest.key tdd-update.tar.gz guest@<ip_of_e1000>:/tmp/update.tar.gz SSH into the E1000: ssh -i guest.key guest@<ip_of_e1000> Now execute: do_update.sh","title":"4G : Update software of E1000"},{"location":"drax-install/#4g-verify-the-update-of-e1000-on-the-unit-and-the-alignment-with-drax-version","text":"To validate that the newly updated software matches with the installed version of dRAX, we can run the following steps: SSH into the E1000: ssh -i guest.key guest@<ip_of_e1000> Note down the Git commit of the newly installed software: strings /mnt/app/acc.tar | grep Git Now on the dRAX server, we need to retrieve the Git commit of the 4g-radio-controller to compare. Find the correct pod name using this command: kubectl get pods | grep 4g-radio-controller With the full pod name, run the following command (replace xxx with the correct identifier from the previous command): kubectl exec -it drax-4g-4g-radio-controller-xxxx -- cat /data/oranC | strings | grep Git The two commits must match, if not please verify the installation and contact Accelleran for support.","title":"4G : Verify the update of E1000 on the unit and the alignment with dRAX version"},{"location":"drax-install/#install-the-drax-ric-and-dashboard","text":"Install the RIC and Dashboard with Helm (if installing without dedicated namespaces, leave off the -n option): helm install ric acc-helm/ric --version $RIC_VERSION --values ric-values.yaml -n $NS_DRAX Info The installation may take up to 5 minutes, it is essential that you wait till the installation is completed and all the pods are in RUNNING or COMPLETE mode, please do NOT interrupt the installation by trying to regain control of the command line To check if the installation was successful first use Helm: helm list #NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION #ric default 1 2022-08-30 12:23:24.894432912 +0000 UTC deployed ric-5.0.0 5.0.0 Than view the pods that have been created. watch kubectl get pod You should see something like this. You can ignore the status of Jaeger in this release. It is not used at the moment. NAME READY STATUS RESTARTS AGE ric-acc-fiveg-pmcounters-6d47899ccc-k2w66 1/1 Running 0 56m ric-acc-kafka-955b96786-lvkns 2/2 Running 2 56m ric-acc-kminion-57648f8c49-g89cj 1/1 Running 1 56m ric-acc-service-monitor-8766845b8-fv9md 1/1 Running 1 56m ric-acc-service-orchestrator-869996756d-kfdfp 1/1 Running 1 56m ric-cassandra-0 1/1 Running 1 56m ric-cassandra-1 1/1 Running 5 54m ric-dash-front-back-end-85db9b456c-r2l6v 1/1 Running 1 56m ric-fluent-bit-loki-jpzfc 1/1 Running 1 56m ric-grafana-7488865b58-nwqvx 1/1 Running 2 56m ric-influxdb-0 1/1 Running 1 56m ric-jaeger-agent-qn6xv 1/1 Running 1 56m ric-kube-eagle-776bf55547-55f5m 1/1 Running 1 56m ric-loki-0 1/1 Running 1 56m ric-metallb-controller-7dc7845dbc-zlmvv 1/1 Running 1 56m ric-metallb-speaker-vsvln 1/1 Running 1 56m ric-metrics-server-b4dd76cbc-hwf6d 1/1 Running 1 56m ric-nats-5g-0 3/3 Running 3 55m ric-nkafka-5g-76b6558c5f-zs4np 1/1 Running 1 56m ric-prometheus-alertmanager-7d78866cc6-svxc5 2/2 Running 2 56m ric-prometheus-kube-state-metrics-585d88b6bb-6kx5l 1/1 Running 1 56m ric-prometheus-node-exporter-pxh6w 1/1 Running 1 56m ric-prometheus-pushgateway-55b97997bf-xb2m2 1/1 Running 1 56m ric-prometheus-server-846c4bf867-ff4s5 2/2 Running 2 56m ric-redis-5g-6f9fbdbcf-j447s 1/1 Running 1 56m ric-vector-84c8b58dbc-cdtmb 1/1 Running 0 56m ric-vectorfiveg-6b8bf8fb4c-79vl7 1/1 Running 0 56m ric-zookeeper-0 1/1 Running 1 56m","title":"Install the dRAX RIC and Dashboard"},{"location":"drax-install/#install-drax-5g-components","text":"Accelleran's 5G Components are managed and installed via the Dashboard. From the dRAX Dashboard sidebar, select New deployment and then click 5G CU deployment : You will reach the Deploy a new CU component page. Here, you have the ability to deploy either a CU-CP or a CU-UP component. Therefore, you first have to pick one from the drop-down menu:","title":"Install dRAX 5G Components"},{"location":"drax-install/#5g-cu-cp-installation","text":"When installing the 5G CU-CP component, there are a number of configuration parameters that should be filled in the Deploy a new CU component form once the CU-CP is chosen from the drop-down menu. The form with the deployment parameters is shown below: NOTE : fill in the E1 and F1 address manually according to what's set in the Preperation section in the start of this installation document. for F1 it will be the ip address we will also configure the DU with.","title":"5G CU-CP Installation"},{"location":"drax-install/#required-parameters","text":"The deployment parameters are split into required and optional ones. It is important to pay attention to certain constraints on two of the parameters in order to obtain the desired installation: The Instance ID must consist of no more than 16 lower case alphanumeric characters or '-', start with an alphabetic character, and end with an alphanumeric character (e.g. 'my-name', or 'abc-123', but not 123-cucp) The maximum number of UE that can be admitted depends also on how many ds-ctrl components get created (by default one per UE) so because occasionally at attach the UE may need two of such components, as a rule of thumb the desired maximum number of UEs must be doubled: if you intend to have at most 2 UEs, set the maximum number of UEs to 4 The required parameters are: Required Parameter Description Instance ID The instance ID of the CU-CP component - this must be unique across all CU-CP and CU-UPs Number of supported AMFs The maximum number of AMFs which can be connected to at any time Number of supported CU-UPs The maximum number of CU-UPs which can be connected to at any time Number of supported DUs The maximum number of DUs which can be connected to at any time Number of supported RUs The maximum number of RUs which can be supported at any time Number of supported UEs The maximum number of UEs which can be supported at any time Once the deployment parameters are set, click the submit button to deploy the 5G CU-CP.","title":"Required Parameters"},{"location":"drax-install/#optional-parameters","text":"The optional parameters are auto-discovered and auto-filled by dRAX. As such they do not need to be changed. However, depending on the use case, you may want to edit them. In this case, you first have to toggle the Set optional parameters to ON . The optional parameters are: Optional Parameter Description NATS URL/Port Connection details towards NATS. When installing the RIC and Dashboard component, if you set the enable5g option to true, a NATS server was deployed, which will be auto-discovered. Redis URL/Port Connection details towards Redis. Similar to NATS, if you set the enable5g option to true, a Redis server was deployed, which will be auto-discovered. dRAX Node Selector name If you label your Kubernetes node with the label draxName , you can specify the value of that label here and force the CU component to be installed on a specific node in the cluster. Namespace The namespace where the CU component should be installed. E1 Service IP Part of the CU-CP is the E1 interface. The 5G component will be exposed outside of Kubernetes on a specific IP and the E1 port of 38462. This IP is given by MetalLB, which is part of the 5G infrastructure. If this field is set to auto, MetalLB will give out the first free IP, otherwise you can specify the exact IP to be used. NOTE: The IP must be from the MetalLB IP pool defined in Enabling 5G components . F1 Service IP Similar to E1, you can specify the IP to be used for the F1 interface. NOTE: Again it has to be from the MetalLB IP pool defined in Enabling 5G components . NETCONF Server Port The NETCONF server used for configuring the 5G CU-CP component is exposed on the host machine on a random port. You can override this and set a predefined port. NOTE: The exposed port has to be in the Kubernetes NodePort range. Version This is the version of the 5G CU component. By default, the latest stable version compatible with the dRAX version is installed. Other released versions can be specified, but compatibility is not guaranteed.","title":"Optional Parameters"},{"location":"drax-install/#5g-cu-up-installation","text":"When deploying the 5G CU-UP component, there is only one required parameter in the Deploy a new CU component form. The form with the deployment parameters is shown below:","title":"5G CU-UP Installation"},{"location":"drax-install/#required-parameters_1","text":"The required deployment parameter is: Required Parameter Description Instance ID The instance ID of the CU-UP component. As before, the Instance ID must be unique, different from the relative CU-CP and must consist of at most 16 lower case alphanumeric characters or '-', start with an alphabetic character, and end with an alphanumeric character (e.g. 'my-name', or 'abc-123').","title":"Required Parameters"},{"location":"drax-install/#optional-parameters_1","text":"Optional parameters are auto-discovered and auto-filled by dRAX. As such they do not need to be changed. However, depending on the use case, you may want to edit them. In this case, you first have to toggle the Set optional parameters to ON . The optional parameters are: Optional Parameter Description NATS URL/Port The details where the NATS is located. When installing the RIC and Dashboard component, if you set the enable5g option to true, the 5G infrastructure will be deployed, which includes the 5G NATS. This NATS is auto-discovered and auto-filled here Redis URL/Port Like NATS, a 5G REDIS is deployed and autofilled dRAX Node Selector name If you label your Kubernetes node with the label draxName , you can specify the value of that label here and force the CU component to be installed on a specific node in the cluster Namespace The namespace where the CU component will be installed NETCONF Server Port The NETCONF server used for configuring the 5G CU-UP component is exposed on the host machine on a random port. You can override this and set a predefined port. NOTE: The exposed port has to be in the Kubernetes NodePort range. Version This is the version of the 5G CU component. By default, the latest stable version compatible with the dRAX version is installed. Other versions can be specified, but compatibility is not guaranteed Now the installation of CU is done. To see the pods and services execute following steps. Here is what to expect.","title":"Optional Parameters"},{"location":"drax-install/#optional-install-xapps","text":"For a basic installation you can skip this chapter. Compatible xApps can be managed and installed via the Dashboard. This can be achieved by clicking on New deployment in the sidebar, and then clicking xApp deployment: In the resulting form, xApps can be deployed either from a remote Helm repository or by uploading a local packaged Helm chart file. In the \"Metadata\" section of the form, the user inputs information regarding the xApp name, the organization and team who own the xApp, the version of the xApp Helm Chart and the namespace where the xApp will be deployed on. When deploying an xApp from a remote Helm repository, the user needs to specify the name of the remote repository, its URL and the Helm chart name. Optionally, the user can upload a values configuration file to override the default configuration present in the remote Helm Chart. When deploying an xApp using the second method, the user can upload a local packaged Helm chart (a .tgz file produced by the command \"helm package <chartName>\") which contains the dRAX compatible xApp and optionally an accompanying values configuration file. Upon clicking the \"Submit\" button, the xApp will be deployed on the user-defined namespace in Kubernetes following the naming convention \"organization-team-xappname-version\".","title":"Optional : Install xApps"},{"location":"drax-install/#drax-configuration","text":"dRAX configuration is split into multiple subsections which mirrors the microservice approach central to dRAX's design. Most of the configuration can be managed via the Dashboard. The Dashboard is accessible at http://$NODE_IP:31315 .","title":"dRAX Configuration"},{"location":"drax-install/#xapp-configuration","text":"xApps can be configured via our Dashboard. From the sidebar, select the xApps Management section, and then click Overview : You will be presented with a list of installed xApps - you can click on the icon in the Details column to access further information on the xApp: From the following page, you will be presented with information on the behaviour of the xApp and the topics that the xApp consumes and produces - the exact information is dependent on the vendor of the xApp. Configuration of the xApp is now managed in the Configuration Parameters section - it may need to be expanded with the collapse/expand button at the top right of the section. You can also expand the Services, by clicking the Show button in the Services column. This will show all the services used and exposed by the xApp, including the port information.","title":"xApp Configuration"},{"location":"drax-install/#4g-e1000-provisioning","text":"When you don't use 4G you can skip this and go to 5G Configuration The certificates and keys referenced in this section are those mentioned in the Prepare keys and certificates for the dRAX Provisioner section . These are required so that the onboarding of new E1000s is a secure process.","title":"4G E1000 Provisioning"},{"location":"drax-install/#listing-currently-provisioned-e1000s","text":"The current list of provisioned E1000s can be retrieved with the following command: curl --cacert ca.crt https:// $NODE_IP :31610/get/","title":"Listing currently provisioned E1000s"},{"location":"drax-install/#provisioning-additional-accelleran-e1000-dus","text":"Each additional E1000 DU, which is to be used with this dRAX installation, needs to be provisioned. This is only needed for E1000 DUs which were not pre-provisioned during the installation process.","title":"Provisioning additional Accelleran E1000 DUs"},{"location":"drax-install/#determine-unique-identifier","text":"Each Accelleran E1000 has a Model, a Hardware Version, and a Serial Number - this information is displayed on the label attached to the unit, and is required in order to pre-provision the DUs. A unique identifier is constructed from this information in the following format: Model-HardwareVersion-SerialNumber This identifier can also be determined automatically via SSH using the following command: echo \" $( eeprom_vars.sh -k ) - $( eeprom_vars.sh -v ) - $( eeprom_vars.sh -s ) \" Each E1000 also needs to be given a unique name. This name could be as simple as \"du-1\" - all that matters is that it is unique in this dRAX installation.","title":"Determine Unique Identifier"},{"location":"drax-install/#prepare-configuration-file","text":"To provision a new E1000, create a new file called cellconfig.yaml with the following contents: E1011-GC01-ACC000000000001 : redis : hostname : $NODE_IP port : 32000 loki : hostname : $NODE_IP port : 30302 instance : filter : du-1 Replace the unique identifier based on the specific E1000, replace $KUBE_IP with the correct IP for your installation, and replace du-1 with the chosen unique name for this DU. If you'd like to provision multiple E1000s at once, duplicate the above snippet for each additional E1000, updating the unique identifier and the name in each case. Make sure to match the indentation in each duplicated snippet - incorrect indentation will result in an error. It's recommended to keep these snippets all in the same file so that we can push the new configuration with a single command.","title":"Prepare configuration file"},{"location":"drax-install/#push-new-configuration","text":"Now run the following command to push this configuration to the Provisioner: curl --cacert ca.crt --cert client.crt --key client.key https:// $NODE_IP :31610/push/ --data-binary @cellconfig.yaml","title":"Push new configuration"},{"location":"drax-install/#changing-the-name-of-an-e1000","text":"The name of a specific E1000 can be updated if required in a slightly more straightforward manner. First determine the unique identifier - refer to the Determine Unique Identifier section above for the exact instructions. Use the following command, replacing $KUBE_IP with the correct IP for your installation, the unique identifier with that just determined, and replacing du-1 with the new name: curl --cacert ca.crt --cert admin.crt --key admin.key https://_ $NODE_IP :31610_/set/E0123-GC01-ACC0123456978901/instance/filter -d du-1","title":"Changing the name of an E1000"},{"location":"drax-install/#4g-ran-configuration","text":"Configuration of the 4G RAN is made simple, intuitive and efficient when using the dRAX Dashboard. Note: all of these options require the Accelleran E1000s to already have been provisioned as described in the E1000 Provisioning section above, or during the installation process.","title":"4G RAN Configuration"},{"location":"drax-install/#enb-configuration-via-enb-list","text":"To access the configuration page for an eNB, first click on the RAN Configuration section, and then click on eNB Configuration. From the displayed list of eNBs, click on the Cog icon in the Edit column corresponding to the eNB you'd like to reconfigure. From the following screen, the configuration of this eNB can be adjusted. Once the configuration has been updated as desired, click on the Create button at the bottom left of the page: Notes: Make sure the Cell ID is a multiple of 256, you can submit Cell IDs that are not a multiple of 256, however this will result in a Macro eNB ID that looks different on the surface, There is no conflict or error check in manual mode, therefore for instance it is possible to configure two cells with the same ID, set an EARFCN that is out of band, and so on: it is assumed that the User is aware of what he/she tries to set up The reference signal power is calculated automatically from the output power, please adjust the output power in dBm which represent the maximum power per channel at the exit without antenna gain","title":"eNB Configuration via eNB list"},{"location":"drax-install/#enb-configuration-via-dashboard","text":"An alternative way of configuring an individual eNB is to make use of the Dashboard initial page (click on Dashboard in the sidebar to return there). Click on the eNB in the Network Topology, and then choose Configure Cell on the Selected Node window at the right: this will take you to the eNB Configuration page and described in the previous section.","title":"eNB Configuration via Dashboard"},{"location":"drax-install/#5g-ran-configuration","text":"If you have a dRAX License for 5G, have enabled 5G during the RIC and Dashboard installation in Enabling 5G components , and have deployed the CU components as instructed in Install dRAX 5G Components , you can now configure the 5G CU components. You can do so by navigating to RAN Configuration in the dRAX Dashboard sidebar and clicking the gNB Configuration : You will reach the 5G CU components configuration page: On this page there are two lists, one for CU-CPs and one for CU-UPs. You can click the icon under the Edit column of each CU component to edit its configuration. When you deploy the 5G CU component and click this button for the first time, you will be asked to set the initial configuration. Later on, you can click this button to edit the configuration.","title":"5G RAN Configuration"},{"location":"drax-install/#5g-cu-cp-configuration","text":"The 5G CU-CP components have a number of parameters that you can set as can be seen below: PLMN ID: The PLMN ID to be used GNB ID: The GNB ID GNB CU-CP name: A friendly name of the 5G CU-CP component AMF NG interface IP Address: You can click on the (+) sign in the table to expand it like on the figure below. You can now Add Rows to add multiple AMF NG interface IP addresses, or delete them using the Delete Row field. Edit the NG Destination IP Address to be the AMF NG IP address of your setup. This IP is the $CORE_IP. Click the Submit button to send the configuration.","title":"5G CU-CP configuration"},{"location":"drax-install/#5g-cu-up-configuration","text":"The 5G CU-UP has a number of configuration parameters as seen below: GNB CU-UP ID: The 3GPP ID of the CU-UP component. GNB CU-UP name: The 3GPP friendly name of the CU-UP component, E1 Links: You can Add Row or Delete Rows using the button. Here we add the E1 IP address of the CU-CP component that this CU-UP component will connect to. Enter the E1 IP under E1 Destination IP Address. This IP is the $E1_CU_IP . Supported PLMN Slices; Expand the table by clicking the (+) sign. You can now Add Rows or Delete Rows to add multiple PLMN IDs. For each PLMN ID, you can Add Rows to add slices or Delete Rows to delete slices. Each slice is defined by the Slice Type and Slice Differentiator.","title":"5G CU-UP configuration"},{"location":"drax-install/#install-xdp","text":"This chapter will improve the CU performance. go to the CU VM ssh $USER @ $NODE_IP login to docker docker login -u $DOCKER_USER -p $DOCKER_PASS Copy/Paste the below and 2 scripts are generated * startXdpAfterBoot.sh * deploy_xdpupsappl.sh Now here below the script 2 scripts that get generated by copy/pasting the below in one go. export XDP_CU_VERSION = $( helm list | grep \"cu-up\" | awk '{print $NF}' ) mkdir -p $HOME /install_ $XDP_CU_VERSION cd !$ tee startXdpAfterBoot.sh <<EOF #!/bin/bash export XDP_INSTANCE_ID=$(kubectl get pod | grep \"e1-sctp\" | sed \"s/-e1-sctp.*//g\") export XDP_CU_VERSION=$(helm list | grep \"cu-up\" | awk '{print $NF}') export XDP_GTP_IP=$NODE_IP export XDP_GTP_ITF=$(ip -br a | grep $NODE_IP | xargs | cut -d ' ' -f 1) until [ \\$(docker ps | wc -l) -ge \"15\" ] do echo \"We are waiting for first 15 docker containers.\" echo \"Current amount of docker containers running: \"\\$(docker ps | wc -l) sleep 1 done if kubectl get pods | grep ups ; then echo \"UPS pods are still running. Deleting...\" kubectl get pods --no-headers=true | awk '/ups/{print \\$1}'| xargs kubectl delete pod else echo \"UPS Pods are not present. Doing nothing.\" fi if docker ps | grep xdpupsappl ; then echo \"XDP Already running. Doing nothing.\" else echo \"XDP not found. Starting...:\" $HOME/install_$XDP_CU_VERSION/deploy_xdpupsappl.sh -i \\$XDP_INSTANCE_ID -g \\$XDP_GTP_IP -G \\$XDP_GTP_ITF -t \\$XDP_CU_VERSION docker logs xdp_cu_up fi EOF mkdir -p $HOME /install_ $XDP_CU_VERSION cd !$ cat > deploy_xdpupsappl.sh <<EOF #!/bin/bash mtu=1460 node_ip=\"\\$(kubectl get node -o jsonpath='{.items[0].status.addresses[?(.type == \"InternalIP\")].address}')\" instance_id= while getopts 'i:G:g:n:n:t:' option; do case \"\\$option\" in i) instance_id=\"\\$OPTARG\" ;; G) gtp_iface=\"\\$OPTARG\" ;; g) gtp_ip=\"\\$OPTARG\" ;; m) mtu=\"\\$OPTARG\" ;; n) node_ip=\"\\$OPTARG\" ;; t) build_tag=\"\\$OPTARG\" ;; esac done if [ -z \"\\$instance_id\" ]; then echo \"Error: no instance ID (-i) specified\" >&2 exit 1 fi if [ -z \"\\$gtp_iface\" ]; then echo \"Error: no GTP interface (-G) specified\" >&2 exit 1 fi if [ -z \"\\$gtp_ip\" ]; then echo \"Error: no GTP IP (-g) specified\" >&2 exit 1 fi if [ -z \"\\$mtu\" ]; then echo \"Error: no MTU (-m) specified\" >&2 exit 1 fi if [ -z \"\\$node_ip\" ]; then echo \"Error: no node IP (-n) specified\" >&2 exit 1 fi if [ -z \"\\$build_tag\" ]; then echo \"Error: no build tag (-t) specified\" >&2 exit 1 fi config_dir=\"\\$(mktemp -d)\" cat >\"\\$config_dir/bootstrap\" <<NESTED_EOF redis.hostname: \\$ node_ip redis.port:32200 instance.filter: \\$ instance_id NESTED_EOF cat > \"\\$config_dir/zlog.conf\" <<NESTED_EOF [global] strict init = true buffer min = 64K buffer max = 64K rotate lock file = /tmp/zlog.lock [formats] printf_format = \"%d(%b %d %H:%M:%S).%ms %8.8H %m%n\" [rules] user.* >stdout ;printf_format NESTED_EOF docker run \\ --name xdp_cu_up \\ --detach \\ --rm \\ --privileged \\ --user 0 \\ --network host \\ --volume \"\\$config_dir:/home/accelleran/5G/config:ro\" \\ --env \"IFNAME=\\$gtp_iface\" \\ --env \"NATS_SERVICE_URL=nats://\\$node_ip:31100\" \\ --env \"MTU_SIZE=\\$mtu\" \\ --env __APPNAME = cuUp \\ --env __APPID = 1 \\ --env ZLOG_CONF_PATH = /home/accelleran/5G/config/zlog.conf \\ --env BOOTSTRAP_FILENAME = /home/accelleran/5G/config/bootstrap \\ --env XDP_OBJECT_FILE = /home/accelleran/5G/xdp_gtp_kernel.o \\ --env LD_LIBRARY_PATH = /home/accelleran/5G/lib \\ --env HOSTMODE = true \\ \"accelleran/xdpupsappl:\\$build_tag\" \\ /home/accelleran/5G/xdpUpsAppl.exe \\ --uplink \"\\$gtp_ip\" \\ --downlink \"\\$gtp_ip\" \\ --bind \"\\$gtp_ip\" \\ EOF chmod 777 * In the first script startXdpAfterBoot.sh verify the 4 variables on top of the script. They are autogenerated but you need to verify them manually. XDP_INSTANCE_ID= The prefix of the output of kubectl get pod | grep e1-sctp XDP_GTP_IP= The $NODE_IP mentioned in the preperation page XDP_GTP_ITF= The interface in the CU VM holding the $NODE_IP XDP_CU_VERSION= The version of the CU. Should be the same as the $CU_VERSION given in the preperation page. Startup the xdp /home/ $USER /install_ ${ CU_VERSION } /startXdpAfterBoot.sh >> /tmp/xdp_bootscript_response Make the run of the script boot persistent by putting it in crontab crontab -e Add this line into the crontab editor file. Change the $USER and install install_directory accordingly. Check the preperation page. @reboot /home/ $USER /install_ ${ CU_VERSION } /startXdpAfterBoot.sh >> /tmp/xdp_bootscript_response execute to verify crontab -l","title":"Install XDP"},{"location":"drax-install/#verifying-the-drax-installation","text":"","title":"Verifying the dRAX installation"},{"location":"drax-install/#monitoring-via-the-kubernetes-api","text":"As specified in the previous sections of this document, the installation of Accelleran dRAX consists of multiple components. Exactly which are installed depends on the choices made during the installation process. All Pods that have been installed should be running correctly at this point though. To verify this, we can use the following command: watch \"kubectl get pods -A | grep -e ric- -e drax-4g- -e acc-5g- -e l3-\" This is what to expect NAME READY STATUS RESTARTS AGE acc-5g-cu-cp-cucp-01-amf-controller-5cb5d654fd-p75n9 1/1 Running 0 7m15s acc-5g-cu-cp-cucp-01-cu-up-controller-75859656cd-t9shf 1/1 Running 0 7m14s acc-5g-cu-cp-cucp-01-ds-ctrl-0 1/1 Running 0 7m15s acc-5g-cu-cp-cucp-01-ds-ctrl-1 1/1 Running 0 6m39s acc-5g-cu-cp-cucp-01-ds-ctrl-10 1/1 Running 0 6m18s acc-5g-cu-cp-cucp-01-ds-ctrl-11 1/1 Running 0 6m16s acc-5g-cu-cp-cucp-01-ds-ctrl-12 1/1 Running 0 6m15s acc-5g-cu-cp-cucp-01-ds-ctrl-13 1/1 Running 0 6m14s acc-5g-cu-cp-cucp-01-ds-ctrl-14 1/1 Running 0 6m13s acc-5g-cu-cp-cucp-01-ds-ctrl-15 1/1 Running 0 6m11s acc-5g-cu-cp-cucp-01-ds-ctrl-2 1/1 Running 0 6m36s acc-5g-cu-cp-cucp-01-ds-ctrl-3 1/1 Running 0 6m34s acc-5g-cu-cp-cucp-01-ds-ctrl-4 1/1 Running 0 6m31s acc-5g-cu-cp-cucp-01-ds-ctrl-5 1/1 Running 0 6m29s acc-5g-cu-cp-cucp-01-ds-ctrl-6 1/1 Running 0 6m27s acc-5g-cu-cp-cucp-01-ds-ctrl-7 1/1 Running 0 6m25s acc-5g-cu-cp-cucp-01-ds-ctrl-8 1/1 Running 0 6m23s acc-5g-cu-cp-cucp-01-ds-ctrl-9 1/1 Running 0 6m20s acc-5g-cu-cp-cucp-01-du-controller-8477b5f5c8-69j26 1/1 Running 0 7m15s acc-5g-cu-cp-cucp-01-e1-cp-0 1/1 Running 0 7m15s acc-5g-cu-cp-cucp-01-f1-ap-0 1/1 Running 0 7m15s acc-5g-cu-cp-cucp-01-f1-ap-1 1/1 Running 0 6m48s acc-5g-cu-cp-cucp-01-f1-ap-2 1/1 Running 0 6m43s acc-5g-cu-cp-cucp-01-gnb-controller-7d666fdfdd-lps9c 1/1 Running 0 7m14s acc-5g-cu-cp-cucp-01-netconf-8974d4495-f5mln 1/1 Running 0 7m15s acc-5g-cu-cp-cucp-01-ng-ap-0 1/1 Running 0 7m15s acc-5g-cu-cp-cucp-01-pm-controller-7869f89778-hf228 1/1 Running 0 7m15s acc-5g-cu-cp-cucp-01-res-mgr-cd6c87484-2v8s4 1/1 Running 0 7m15s acc-5g-cu-cp-cucp-01-rr-ctrl-0 1/1 Running 0 7m15s acc-5g-cu-cp-cucp-01-rr-ctrl-1 1/1 Running 0 6m43s acc-5g-cu-cp-cucp-01-rr-ctrl-2 1/1 Running 0 6m41s acc-5g-cu-cp-cucp-01-sctp-f46df5cfb-4kzxh 1/1 Running 0 7m15s acc-5g-cu-up-cuup-01-cu-up-0 1/1 Running 0 6m54s acc-5g-cu-up-cuup-01-cu-up-1 1/1 Running 0 6m54s acc-5g-cu-up-cuup-01-e1-sctp-up-868897844f-xh4rx 1/1 Running 0 6m54s acc-5g-cu-up-cuup-01-netconf-6746749b49-kdqbq 1/1 Running 0 6m54s acc-5g-cu-up-cuup-01-pm-controller-up-57f874bbdb-ttg5k 1/1 Running 0 6m54s acc-5g-cu-up-cuup-01-res-mgr-up-589689966c-9txd8 1/1 Running 0 6m54s busybox 1/1 Running 2 160m ric-acc-fiveg-pmcounters-6d47899ccc-k2w66 1/1 Running 0 71m ric-acc-kafka-955b96786-lvkns 2/2 Running 2 71m ric-acc-kminion-57648f8c49-g89cj 1/1 Running 1 71m ric-acc-service-monitor-8766845b8-fv9md 1/1 Running 1 71m ric-acc-service-orchestrator-869996756d-kfdfp 1/1 Running 1 71m ric-cassandra-0 1/1 Running 1 71m ric-cassandra-1 1/1 Running 5 69m ric-dash-front-back-end-85db9b456c-r2l6v 1/1 Running 1 71m ric-fluent-bit-loki-jpzfc 1/1 Running 1 71m ric-grafana-7488865b58-nwqvx 1/1 Running 2 71m ric-influxdb-0 1/1 Running 1 71m ric-jaeger-agent-qn6xv 1/1 Running 1 71m ric-jaeger-collector-55597cfbbc-r9mdh 0/1 CrashLoopBackOff 19 71m ric-jaeger-query-774f759bb6-jz7jc 1/2 CrashLoopBackOff 19 71m ric-kube-eagle-776bf55547-55f5m 1/1 Running 1 71m ric-loki-0 1/1 Running 1 71m ric-metallb-controller-7dc7845dbc-zlmvv 1/1 Running 1 71m ric-metallb-speaker-vsvln 1/1 Running 1 71m ric-metrics-server-b4dd76cbc-hwf6d 1/1 Running 1 71m ric-nats-5g-0 3/3 Running 3 70m ric-nkafka-5g-76b6558c5f-zs4np 1/1 Running 1 71m ric-prometheus-alertmanager-7d78866cc6-svxc5 2/2 Running 2 71m ric-prometheus-kube-state-metrics-585d88b6bb-6kx5l 1/1 Running 1 71m ric-prometheus-node-exporter-pxh6w 1/1 Running 1 71m ric-prometheus-pushgateway-55b97997bf-xb2m2 1/1 Running 1 71m ric-prometheus-server-846c4bf867-ff4s5 2/2 Running 2 71m ric-redis-5g-6f9fbdbcf-j447s 1/1 Running 1 71m ric-vector-84c8b58dbc-cdtmb 1/1 Running 0 71m ric-vectorfiveg-6b8bf8fb4c-79vl7 1/1 Running 0 71m ric-zookeeper-0 1/1 Running 1 71m Another check you need to do is this one. kubectl get services you can see 4 External IP addresses. Those ip addresses are the ones of the range we filled in in the ric-values.yaml file. The 2 last in the range are of the E1 and F1 service. The first two are selected the handle the GTP traffic. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE acc-5g-cu-cp-cucp-01-sctp-e1 LoadBalancer 10.104.225.53 10.55.7.130 38462:32063/SCTP 6m10s acc-5g-cu-cp-cucp-01-sctp-f1 LoadBalancer 10.103.34.228 10.55.7.131 38472:31066/SCTP 6m10s acc-5g-cu-up-cuup-01-cu-up-gtp-0 LoadBalancer 10.96.213.103 10.55.7.120 2152:32081/UDP 5m49s acc-5g-cu-up-cuup-01-cu-up-gtp-1 LoadBalancer 10.99.208.214 10.55.7.121 2152:30575/UDP 5m49s acc-service-monitor NodePort 10.104.125.9 <none> 80:30500/TCP 70m acc-service-orchestrator NodePort 10.111.157.49 <none> 80:30502/TCP 70m kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 160m netconf-cucp-01 NodePort 10.110.18.130 <none> 830:32285/TCP 6m10s netconf-cuup-01 NodePort 10.103.120.206 <none> 830:31705/TCP 5m49s ric-acc-fiveg-pmcounters NodePort 10.98.3.182 <none> 8000:30515/TCP 70m ric-acc-kafka NodePort 10.98.24.152 <none> 9092:31090/TCP,9010:32537/TCP,5556:32155/TCP 70m ric-acc-kminion ClusterIP 10.107.221.30 <none> 8080/TCP 70m ric-cassandra ClusterIP None <none> 7000/TCP,7001/TCP,7199/TCP,9042/TCP,9160/TCP 70m ric-dash-front-back-end NodePort 10.106.72.78 <none> 5000:31315/TCP 70m ric-dash-front-back-end-websocket NodePort 10.102.245.64 <none> 5001:31316/TCP 70m ric-grafana NodePort 10.96.41.39 <none> 80:30300/TCP 70m ric-influxdb ClusterIP 10.108.225.110 <none> 8088/TCP 70m ric-influxdb-api NodePort 10.105.161.178 <none> 8086:30303/TCP 70m ric-jaeger-agent ClusterIP 10.103.0.234 <none> 5775/UDP,6831/UDP,6832/UDP,5778/TCP,14271/TCP 70m ric-jaeger-collector ClusterIP 10.100.187.234 <none> 14250/TCP,14268/TCP,14269/TCP 70m ric-jaeger-query NodePort 10.97.254.197 <none> 80:31445/TCP,16687:31025/TCP 70m ric-kube-eagle ClusterIP 10.102.90.103 <none> 8080/TCP 70m ric-loki NodePort 10.108.39.131 <none> 3100:30302/TCP 70m ric-loki-headless ClusterIP None <none> 3100/TCP 70m ric-metrics-server ClusterIP 10.105.180.254 <none> 443/TCP 70m ric-nats-5g NodePort 10.107.246.192 <none> 4222:31100/TCP,6222:32053/TCP,8222:30606/TCP,7777:30168/TCP,7422:30680/TCP,7522:31616/TCP 70m ric-prometheus-alertmanager ClusterIP 10.106.127.91 <none> 80/TCP 70m ric-prometheus-kube-state-metrics ClusterIP None <none> 80/TCP,81/TCP 70m ric-prometheus-node-exporter ClusterIP None <none> 9100/TCP 70m ric-prometheus-pushgateway ClusterIP 10.105.167.58 <none> 9091/TCP 70m ric-prometheus-server NodePort 10.97.205.182 <none> 80:30304/TCP 70m ric-redis-5g NodePort 10.96.155.105 <none> 6379:32200/TCP 70m ric-zookeeper NodePort 10.109.78.254 <none> 2181:30305/TCP 70m ric-zookeeper-headless ClusterIP None <none> 2181/TCP,3888/TCP,2888/TCP 70m The listed Pods should either all be Running and fully Ready (i.e. all expected instances are running - 1/1, 2/2, etc.), or Completed - it may take a few minutes to reach this state. The number of restarts for each pod should also stabilize and stop increasing. If something crashes or you need to restart a pod, you can use the scale command - for example: kubectl scale deployment $DEPLOYMENT_NAME --replicas = 0 kubectl scale deployment $DEPLOYMENT_NAME --replicas = 1","title":"Monitoring via the Kubernetes API"},{"location":"drax-install/#sctp-connections-setup","text":"Check services and verify E1 and F1 ip address. kubectl get services Verify SCTP connection is setup. Expecting HB REQ and HB ACK tracing with this tcpdump commandline. sudo tcpdump -i any or 38462","title":"SCTP connections setup"},{"location":"drax-install/#the-grafana-accelleran-drax-system-dashboard","text":"The dRAX Grafana contains two system health dashboards, one for 4G and one for 5G. On these dashboards we can check if all the 4G or 5G components are running, if there are Kubernetes pods that are in an error state, we can check the RAM and CPU usage per pod, etc. In fact, they give a complete overview of the System in 5 sections, each containing detailed information to allow a graphic, intuitive approach to the System health conditions. Each section can be expanded by clicking on its name. Cluster Information contains: Number of nodes Allocatable RAM Running PODs Pending Containers Crashed Containers Pending or Crashed containers listed by Node, Namespace, Status, POD name and package RAM Information contains: Total RAM Usage Node RAM Usage POD RAM Usage Container RAM Usage RAM Usage History Node RAM Info listed by Node, Requested RAM Limit RAM Allocatable RAM RAM Reserved, RAM Usage POD RAM information listed by Node, Pod, Requested RAM, RAM Limit, Used RAM Container RAM information listed by Node, Pod, Requested RAM, RAM Limit, Used RAM CPU Information contains: Total CPU Usage Node CPU Usage POD CPU Usage Container CPU Usage CPU Usage History Node CPU Info listed by Node, Requested Core Limit Cores Allocatable Cores CPU Reserved,CPU Burstable, CPU Usage POD CPU information listed by Node, Pod, Requested Cores, Limit Cores, Used Cores Container CPU information listed by Node, Pod, Requested Cores, Limit Cores, Used Cores Network Information contains: TX Traffic RX Traffic Disk Space information contains: Disk Usage Disk Usage History Disk Usage per Node listed by Node, User Disk Space, Free Disk Space, Total Disk Space Persistent Disk Volumes listed by Node, Volume Name,Disk Space, Bound status","title":"The Grafana Accelleran dRAX System Dashboard"},{"location":"drax-install/#4g-system-health-dashboard","text":"To access the dRAX Grafana, browse to http://$NODE_IP:30300 . From here you can browse the different pre-built Grafana dashboards that come with dRAX. One of them is the Accelleran dRAX System Dashboard : The 4G specific health dashboard, in addition to the 5 global sections explained above, also shows which components of 4G dRAX are running (Redis, NATS, 4GRC, etc.).","title":"4G system health dashboard"},{"location":"drax-install/#5g-system-health-dashboard","text":"The 5G system health dashboard can also be found on dRAX Grafana on http://$NODE_IP:30300 . This time, pick the Accelleran dRAX 5G System Dashboard from the list of pre-built Grafana dashboards: The 5G specific health dashboard, in addition to the 5 global sections explained above, also shows which components of 5G dRAX are running (AMF Controller, CUUP, DS Ctrl, etc.).","title":"5G system health dashboard"},{"location":"drax-install/#appendix-how-to-enabledisable-dhcp-for-the-ip-address-of-the-e1000-4g-du","text":"The 4G DU units are separate hardware components and therefore get preconfigured at Accelleran with a standard SW image which of course will have default settings that may require changes. Typically in fact a network component will require IP address Netmask default gateway to be configured and the Customer will want to rework these settings before commissioning the component into the existing Network. The default settings are: Static IP address DHCP Off Provisioner Off Bootstrap file with Redis port 32000, dRAX IP 10.20.20.20 and a generic eNB Name indicating for instance the Customer name, Band, and a progressing number The rest of environment variables are visible once logged in to the E1000 using the fprintenv command. So for instance the variable you will be able to see are: ethact=octeth0 ethaddr=1C:49:7B:DE:35:F7 fdtcontroladdr=80000 gatewayip=10.188.6.1 gpsenable=1 ipaddr=10.20.20.222 loadaddr=0x20000000 mtdparts=mtdparts=octeon_nor0:0x220000(u-boot)ro,128k(u-boot-env)ro,128k(u-boot-env-bak)ro,128k(u-boot-env-gemtek)ro,0x1340000(init-app)ro,-(accelleran-app)ro namedalloc=namedalloc dsp-dump 0x400000 0x7f4D0000; namedalloc pf4cazac 0x13000 0x84000000; namedalloc cazac 0x630000 0x7f8D0000; namedalloc cpu-dsp-if 0x100000 0x7ff00000; namedalloc dsp-log-buf 0x4000000 0x80000000; namedalloc initrd 0x2B00000 0x30800000; netmask=255.255.255.0 numcores=4 octeon_failsafe_mode=0 octeon_ram_mode=0 serverip=10.188.6.137 stderr=serial stdin=serial stdout=serial swloadby=flash unprotect=protect off 0x17cc0000 0x1fc00000; ver=U-Boot 2017.05 (Sep 08 2017 - 16:27:53 +0000) xoservoenable=1 xoservolog=/var/log/xolog.txt dhcp=yes If the Customers wants to change IP address using the command line he can do the following (special attention must be put as an error in the input can bring the E1000 out of reach): fsetenv ipaddr <new_ip_address> In order to modify the netmask type: fsetenv netmask <new_net_mask> ( ex. 255 .255.255.0 ) NOTE: the User that wishes to perform such modifications must be aware of the consequences of that choice, for instance the necessity of shipping back the unit to Accelleran for refurbishment in case of misconfigurations. In case the E1000 is supposed to get a dynamic address from a DHCP server in the Customer network the related flag shall be enable: fsetenv dhcp yes Don't forget to reboot the E1000 once done with the settings. If for any reasons the Customer decides not to use the dynamic address assignment he can revert the choice by typing: fsetenv dhcp no IMPORTANT : In this case it is also essential to configure a static address and an IP Mask in harmony with the rest of the network: fsetenv ipaddr <new_ip_address> fsetenv netmask <new_net_mask> ( ex. 255 .255.255.0 ) After that , you can reboot the E1000 which will come back with ARP signals with the chosen static address.","title":"Appendix: How to enable/disable DHCP for the IP address of the E1000 4G DU"},{"location":"drax-install/#how-to-configure-drax-for-a-cell","text":"","title":"How to configure dRAX for a cell"},{"location":"drax-install/#introduction_2","text":"For the cell to be able to communicate with the dRAX Provisioner, it needs to use its own certificate. For this certificate to be valid, the time and date on the cell need to be synchronized. If the time and date are not correct, the certificates will not work.","title":"Introduction"},{"location":"drax-install/#how-to-enable-the-drax-provisioner","text":"SSH into the E1000: ssh -i guest.key guest@<ip_of_e1000> Create a folder: mkdir /mnt/common/bootstrap_source Create and save an empty dhcp file in the folder created: touch /mnt/common/bootstrap_source/dhcp","title":"How to enable the dRAX Provisioner"},{"location":"drax-install/#appendix-drax-and-accelleran-e1000s-on-different-subnets","text":"Normally, we recommend your Accelleran E1000s are located on the same subnet as your dRAX. However, if that is not the case, then you need to run a specific component of the dRAX Provisioner called the Provisioner-DHCP. This component should be running on any machine that is part of the subnet where the Accelleran E1000s are. We support running this component via Docker, so you must have Docker installed on that machine.","title":"Appendix: dRAX and Accelleran E1000s on different subnets"},{"location":"drax-install/#prepare-conf-file","text":"Create the configuration file named udhcpd.conf. The contents are: start 10.0.0.20 end 10.255.255.254 interface eth0 opt dns 10.0.0.1 10.0.0.2 option subnet 255.0.0.0 opt router 10.0.0.1 opt wins 10.0.0.2 option domain local option lease 86400 option provision https://$NODE_IP:31610 option staticroutes 10.20.20.0/24 10.22.10.52 Substitute the IP in the \"option provision \u2026\" line to where the provisioner is installed. Also change the interface to the main interface of the machine (eth0, eno1, br0, \u2026) that is used to reach the subnet where the dRAX is installed. NOTE: You should make sure from your networking aspect that the two subnets are reachable by one-another. If this is not the case, although we do not recommend this, you can create a static route on the E1000s towards the subnet where dRAX is installed. This can be done using the Provisioner-DHCP component. Find the line: option staticroutes 10.20.20.0/24 10.22.10.52 . This translates to \"create a static route to the 10.20.20.0/24 network (where the dRAX is) via gateway 10.22.10.52\". Replace the values with the correct ones for your network case.","title":"Prepare conf file"},{"location":"drax-install/#create-docker-login-details","text":"Login with docker command: sudo docker login Then use the username and password of your DockerHub account that you also used to create the kubernetes secret.","title":"Create docker login details"},{"location":"drax-install/#pull-the-image-from-dockerhub","text":"Check what is the latest version on DockerHub https://hub.docker.com/repository/docker/accelleran/provisioner-dhcp . Pull the image using the docker command, and substitute the <version> with the one found in the above step: sudo docker image pull accelleran/provisioner-dhcp:<version>","title":"Pull the image from DockerHub"},{"location":"drax-install/#run-as-docker-container","text":"Start the container with the docker run command. Make sure to give the full path to the configuration file (/home/ad/...). Also make sure you give the correct docker image name at the end of the command including the version: sudo docker run -d --name dhcp --net host --mount type = bind,source = /path/to/udhcpd.conf,target = /conf/udhcpd.conf accelleran/provisioner-dhcp:<version> To check if the service is running use sudo docker ps | grep dhcp","title":"Run as docker container"},{"location":"drax-install/#appendix-drax-provisioner-keys-and-certificates-generation","text":"In general, TLS certificates only allow you to connect to a server if the URL of the server matches one of the subjects in the certificate configuration. This section assumes the usage of openssl to handle TLS security due to its flexibility, even if it is both complex to use and easy to make mistakes. Customers can choose to use different options to generate the keys and certificates as long as of course the final output matches the content of this section.","title":"Appendix: dRAX Provisioner - Keys and Certificates Generation"},{"location":"drax-install/#create-the-certificates","text":"","title":"Create the certificates"},{"location":"drax-install/#create-the-serverkey","text":"First thing is to create a key (if it doesn't exist yet): openssl genrsa -out server.key 4096 This command will create a RSA based server key with a key length of 4096 bits.","title":"Create the server.key"},{"location":"drax-install/#create-a-server-certificate","text":"First, create the cert.conf . Create a file like the example below and save it as cert.conf : [ req ] default_bits = 2048 default_keyfile = server-key.pem distinguished_name = subject req_extensions = req_ext x509_extensions = req_ext string_mask = utf8only [ subject ] countryName = Country Name (2 letter code) countryName_default = BE stateOrProvinceName = State or Province Name (full name) stateOrProvinceName_default = Example state localityName = Locality Name (eg, city) localityName_default = Example city organizationName = Organization Name (eg, company) organizationName_default = Example company commonName = Common Name (e.g. server FQDN or YOUR name) commonName_default = Example Company emailAddress = Email Address emailAddress_default = test@example.com [ req_ext ] subjectKeyIdentifier = hash basicConstraints = CA:FALSE keyUsage = digitalSignature, keyEncipherment subjectAltName = @alternate_names nsComment = \"OpenSSL Generated Certificate\" [ alternate_names ] DNS.1 = localhost IP.1 = 10.0.0.1 IP.2 = 10.20.20.20 Fill in the details, like the country, company name, etc. IMPORTANT: Edit the last line of the file. IP.2 should be equal to IP where the provisioner will be running. This is the $NODE_IP from the planning phase. The default is set to 10.20.20.20. To create the server certificate, use the following command: openssl req -new -key server.key -config cert.conf -out server.csr -batch Command explanation: openssl req -new : create a new certificate -key server.key : use server.key as the private half of the certificate -config cert.conf : use the configuration as a template -out server.csr : generate a csr -batch : don't ask about the configuration on the terminal","title":"Create a server certificate"},{"location":"drax-install/#create-a-self-signed-client-certificate","text":"To create the client certificate, use the following command: openssl req -newkey rsa:4096 -nodes -keyout client.key -sha384 -x509 -days 3650 -out client.crt -subj /C = XX/ST = YY/O = RootCA This command will create a client.key and client.crt from scratch to use for TLS-based authentication, in details the options are: openssl req : create a certificate -newkey rsa:4096 : create a new client key -nodes : do not encrypt the newly create client key with a passphrase (other options are -aes) -keyout client.key : write the key to client.key -x509 : sign this certificate immediately -sha384 : use sha384 for signing the certificate` -days 3650 : this certificate is valid for ten years -subj /C=XX/ST=YY/O=RootCA : use some default configuration -out client.crt : write the certificate to client.crt","title":"Create a self-signed client certificate"},{"location":"drax-install/#sign-the-server-certificate-using-the-root-certificate-authority-key","text":"The server certificate needs to be signed by Accelleran. To do so, please contact the Accelleran Customers Support Team and send us the following files you created previously: server.csr cert.conf You will receive from Accelleran the following files: signed server.crt ca.crt","title":"Sign the server certificate using the root certificate authority key"},{"location":"drax-install/#verify-the-certificates-work","text":"The following commands should be used and return both OK: openssl verify -CAfile client.crt client.crt openssl verify -CAfile ca.crt server.crt","title":"Verify the certificates work"},{"location":"drax-install/#appendix-configure-4g-radio-controller","text":"In order for the dRAX 4G components to function properly, we need to configure the 4G Radio Controller. This can be done from the Dashboard, which is accessible at http://$NODE_IP:31315 . From the sidebar, select the xApps Management section, and then click Overview : From the dRAX Core section, find the 4G-Radio-Controller entry, and click on the corresponding cog icon in the Configure column, as shown in the picture below: You will be presented with a configuration page - the following changes should be made, making sure to replace $NODE_IP with the value from your installation: The parameters are: Automatic Handover: If set to true, the 4G default handover algorithm is activated, based on the A3 event. If set to false, the A3 event from the UE is ignored by dRAX and the handover will not be triggered. Publish Measurement Data: Publish UE Data: Measurement Type: Orchestrator URL: This should be the $KUBE_IP:6443, so the Kubernetes advertise address and using the secure port 6443","title":"Appendix: Configure 4G Radio Controller"},{"location":"drax-install/#appendix-license-error-codes","text":"Sometimes you might run into issues when trying to launch dRAX due to a licensing error. A list of possible error codes is provided below: ID Tag Explanation E001 ENotInEnv Environment variable not set E002 EInvalidUTF8 The content of the environment varable is not valid UTF8 E003 ECannotOpen Cannot open license file, was it added as a secret with the right name? To verify whether it's loaded correctly, run: bash kubectl get secret accelleran-license -o'jsonpath={.data.license\\\\.crt}' which should give you a base64 encoded dump. E004 ELicenseExpired Your license is expired! You'll likely need a new license from Accelleran E005 EDecryption An error occurred during decryption E006 EVerification An error occurred during verification E007 EMissingPermission You do not have the permissions to execute the software. You'll likely need a more permissive license from Accelleran. E008 ESOError Inner function returned an error E009 ERunFn Cannot find the correct function in the library E010 ELoadLibrary Cannot load the .so file E011 ETryWait An error occurred while waiting for the subprocess to return E012 ESpawn Could not spawn subprocess E013 EWriteDecrypted Cannot write to file descriptor E014 EMemFd Cannot open memory file descriptor E015 ECypher Cannot create cypher","title":"Appendix: License Error Codes"},{"location":"drax-install/#appendix-remove-existing-deployments","text":"In order to continue with the remaining steps, we remove the existing deployments of our charts. Note that this will not remove the data, so any configured components should remain once the installation is completed. It may be that the previous versions used different names for the Helm deployments, so to check the correct names we can use the helm list command: helm list #NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION #acc-5g-cu-cp default 1 2022-01-21 15:16:35.893230618 +0000 UTC deployed acc-5g-cu-cp-3.0.0 release-2.3-duvel-8b8d7f05 #acc-5g-cu-up default 1 2022-01-21 15:16:44.931753616 +0000 UTC deployed acc-5g-cu-up-3.0.0 release-2.3-duvel-8b8d7f05 #acc-helm-ric default 1 2022-01-09 17:20:52.860528687 +0000 UTC deployed ric-4.0.0 4.0.0 In the above example, the installations are called acc-5g-cu-cp , acc-5g-cu-up and ric , so the required commands would be: helm uninstall acc-5g-cu-cp helm uninstall acc-5g-cu-up helm uninstall ric Please wait until all the pods and resources of the previous dRAX installation are deleted. You can view them by: watch kubectl get pods -A You can now continue with the remaining steps.","title":"Appendix : Remove existing deployments"},{"location":"du-install/","text":"DU Installation \u00b6 Introduction \u00b6 The DU will be installed in several Docker containers that run on metal on the host machine. As mentioned in the introduction, a separate Virtual Machine will host the RIC and the CU and their relative pods will be handled by Kubernetes inside that VM. Here we focus on the steps to get DU and L1 up and running. Variables needed for this install \u00b6 Before proceeding you may want to crosscheck and modify some paramters that caracterise each deployment and depends on the desired provisioning of the components. The parameters that should be considered for this purpose and can be safely modified are: 5G variables \u00b6 plmn_identity [ eg 235 88 ] nr_cell_identity [ eg 1 any number ] nr_pci [ eg 1 not any number. Ask Accelleran to do the PCI planning ] 5gs_tac [ eg 1 ] Frequency variables \u00b6 center_frequency_band [ eg 3751.680 ] point_a_arfcn [ 648840 consistent with center freq, scs 30khz ] band [ 77 consistent with center frequency ] licenses and files needed (see intro) \u00b6 accelleran-du-phluido-%Y-%m-%d-release.zip phluido_docker_xxxx.tar effnet-license-activation-%Y-%m-%d.zip 32 digit phluido license key, [ ex 2B2A-962F-783F-40B9-7064-2DE3-3906-9D2E ] For any other modification it is advisable to make contact with the Accelleran service desk as of course, if in principle every paramter in the confuguration file is up for modification, it is certainly not recommendable to proceed in that declaration as each front end may or may not work as a consequence and the analysis and recovery from error scenario will be less than intuitive Docker installation on the Server \u00b6 Make sure Docker and docker-compose have been installed and that docker can be run without superuser privileges, this is a prerequisite. DO NOT install Kubernetes where DU and L1 will run Add the Docker APT repository: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt update Install the required packages: sudo apt install docker-ce sudo apt install docker-ce-cli sudo apt install containerd.io sudo apt install docker-compose Add your user to the docker group to be able to run docker commands without sudo access. You might have to reboot or log out and in again for this change to take effect. sudo usermod -aG docker $USER To check if your installation is working you can try to run a test image in a container: sudo docker run hello-world Restart Docker and enable on boot: sudo systemctl enable docker sudo systemctl daemon-reload sudo systemctl restart docker Obtain the Effnet and Phluido licenses \u00b6 Preparation steps \u00b6 In this phase we will need to act in parallel for the DU and the L1/RRU licenses, which depend on our partner company so it is essential to give priority and possibly anticipate these two steps as there is no specific effort involved from the user/customer perspective and they may require longer than one working day before we can proceed further. Phluido : Install a Low Latency Kernel \u00b6 The PHY layer has very stringent latency requirements, therefore we install a low latency kernel: sudo apt install linux-image-lowlatency make sure this line is present in the /etc/default/grub file. It forces the GRUB to start a lowlatency kernel. $ cat /etc/default/grub | grep FLAV GRUB_FLAVOUR_ORDER=lowlatency Create a sysctl configuration file to configure the low latency kernel: sudo tee /etc/sysctl.d/10-phluido.conf <<EOF # Improves scheduling responsiveness for Phluido L1 kernel.sched_min_granularity_ns = 100000 kernel.sched_wakeup_granularity_ns = 20000 kernel.sched_latency_ns = 500000 kernel.sched_rt_runtime_us = -1 # Message queue fs.mqueue.msg_max = 64 EOF Remove the generic kernel to avoid the low latency kernel to be replaced by a generic kernel when updates are performed: sudo apt remove linux-image-generic sudo apt autoremove In order to avoid possible system performance degradation, CPU scaling must be disabled: sudo apt install cpufrequtils echo 'GOVERNOR=\"performance\"' | sudo tee /etc/default/cpufrequtils sudo systemctl disable ondemand Restart the machine to make the changes take effect: sudo reboot Phluido License: Run the sysTest utility from Phluido \u00b6 to go to the directory where the Phluido sysTest utility is : $ ./sysTest Running system test... 01234567890123456789012345678901 System test completed, output written to file \"sysTest.bin\". ( The test takes around 90 seconds) This will run a test of the system that will allow to determine if the server is properly configured and capable of running the demanding L1/RRU components Once it is finsihed it produces a file sysTest.bin in the same directory Send this file to Accelleran, to obtain the Phluido license key. Send this .bin file to phluido to receive a proper license. Effnet License: Install and check your Yubikey \u00b6 For the license activation file we indicate the generic format yyyy_mm_dd as the file name may vary from case to case, your Accelleran point of contact will make sure you receive the correct license activation archive file which will have a certain timestamp on it, example effnet-license-activation-2021-12-16.zip if you don't have yet the effnet license activation bundle, in order to obatin one you must comunicate to Accelleran the serial number of the Yubikey you intend to use so to be enabled for usage. You can obtain this information by using the following command on your server where the Yubikey has been installed physically to a USB port: To check if the server can see the key do (in this example Device004 is your key): lsusb ~$ lsusb Bus 002 Device 002 : ID 8087 :8002 Intel Corp. Bus 002 Device 001 : ID 1d6b:0002 Linux Foundation 2 .0 root hub Bus 001 Device 002 : ID 8087 :800a Intel Corp. Bus 001 Device 001 : ID 1d6b:0002 Linux Foundation 2 .0 root hub Bus 004 Device 003 : ID 2500 :0020 Ettus Research LLC USRP B200 Bus 004 Device 001 : ID 1d6b:0003 Linux Foundation 3 .0 root hub **Bus 003 Device 004 : ID 1050 :0407 Yubico.com Yubikey 4 OTP+U2F+CCID** Bus 003 Device 029 : ID 2a70:9024 OnePlus AC2003 Bus 003 Device 006 : ID 413c:a001 Dell Computer Corp. Hub Bus 003 Device 016 : ID 20ce:0023 Minicircuits Mini-Circuits Bus 003 Device 001 : ID 1d6b:0002 Linux Foundation 2 .0 root hub Then you can find the serial number (yubikey-manager needed, install if it's not already): sudo apt install yubikey-manager ykman list --serials #13134288 Effnet License: Create a PCSCD Docker Image \u00b6 The DU software needs access to a YubiKey that contains its license. The license in the YubiKey is shared by the PCSCD daemon, which itself can run in a Docker container to satisfy OS dependencies. Plug the YubiKey in a USB port of the machine. Then, create a Dockerfile named Dockerfile.pcscd for this Docker image: mkdir -p pcscd tee pcscd/Dockerfile.pcscd <<EOF FROM ubuntu:20.04 RUN \\ set -xe && \\ apt-get update && \\ DEBIAN_FRONTEND=\"noninteractive\" apt-get install -y \\ pcscd # Cleanup RUN \\ set -xe && \\ apt-get clean && \\ rm -rf \\ /var/lib/apt/lists/* \\ /var/tmp/* \\ /tmp/* ENTRYPOINT [\"/usr/sbin/pcscd\", \"--foreground\"] EOF The Docker image can now be built and started with: docker build --rm -t pcscd_yubikey - <pcscd/Dockerfile.pcscd docker run --restart always -id --privileged --name pcscd_yubikey_c -v /run/pcscd:/run/pcscd pcscd_yubikey You can verify it is running correct with the command: docker container ls --filter name = pcscd_yubikey_c If every step was performed correctly this command should produce output similar to: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES df4f41eb70c9 pcscd_yubikey \"/usr/sbin/pcscd --f\u2026\" 1 minute ago Up 1 minute pcscd_yubikey_c Effnet License: activate the yubikey \u00b6 In order to activate the license dongles unzip the received license activation bundle effnet-license-activation-yyyy-mm-dd.zip (as mentioned the date may differ on each case so let's use the generic format) and then you need to load the included Docker image into your docker-daemon, i.e. bunzip2 --stdout license-activation-yyyy-mm-dd.tar.bz2 | docker load Then run the image mapping the folder containing the pcscd daemon socket into the container: docker run -it -v /var/run/pcscd:/var/run/pcscd effnet/license-activation-yyyy-mm-dd If you get warnings similar to: WARNING: No dongle with serial-number 13134288 found It means that a dongle for the bundled license was not found, i.e. in this case the dongle with the serial number 13134288 has not been activated, or the licens bundle file you have received is not the correct one, contact Accelleran in such case Successful activation of a license-dongle should produce an output similar to: Loading certificate to Yubico YubiKey CCID 00 00 ( serial: 13134288 ) Which means that a license for the dongle with serial-number 13134288 was loaded to the dongle (i.e., it was bundled in the license-activation image). Install the Phluido L1 ( docker ) \u00b6 docker image load -i phluido_docker_ $L1_VERSION .tar Install Effnet DU ( docker ) \u00b6 unzip accelleran-du-phluido- $DU_VERSION .zip bzcat accelleran-du-phluido/accelleran-du-phluido- $DU_VERSION /gnb_du_main_phluido- $DU_VERSION .tar.bz2 | docker image load docker compose file ( with CPU PINNING ) \u00b6 To achieve maximum stability and performance it is necessary to optimise the CPU load and this can be done by distributing the available CPUs among the components and assign different priorities to the most demanding processes. We split therefore the CPUs in two groups, one group of CPUs for the VM where the RIC/CU run and one group of CPUs for the containers that run L1 and DU. The CPU pinning allows for ceertain components to run only on certain CPUs, however it doesn't impede other processes to use the same CPUs, so the full optimisation of the CPU usage and the exclusive allocation of the CPUs are beyond the scope of this document, here we illustrate one possible split as an example. First thing to find out is what resources are available on our system: These have been found during the preperation fase and shared over DU and CU. In this chapter the DU core are assigned. In this specific example, there are two banks of 4 cores, each capable of hyperthreading, so in total we can count on 16 CPUs, let's then set 8 CPUs aside to run the VM for kubernetes and CU, and the other 8 CPUs to run L1/L2 so that they never compete for the same CPU. The assumption is that the rest of the processes on the system (very light load) is equally spread over all cores. If a large number of cores is available, probably the core with a higher number will be mostly free and can be then dedicated to L1 and DU, as mentioned there is no specific rule. For the sake of argument let's assign the even cores to the L1 and DU equally, so the Docker compose looks as follows: mkdir -p ~/install- $DU_VERSION / cd !$ cat > docker-compose.yaml << EOF version: \"2\" services: phluido_l1: image: phluido_l1:$L1_VERSION container_name: phluido_l1 tty: true privileged: true ipc: shareable shm_size: 2gb command: /config.cfg volumes: - \"$PWD/l1-config-0.cfg:/config.cfg:ro\" - \"$PWD/logs-0/l1:/workdir\" - \"/etc/machine-id:/etc/machine-id:ro\" working_dir: \"/workdir\" network_mode: host cpuset: \"$CORE_SET_DU\" du: image: gnb_du_main_phluido:$DU_VERSION volumes: - \"$PWD/du-config-0.json:/config.json:ro\" - \"$PWD/logs-0/du:/workdir\" - /run/pcscd/pcscd.comm:/run/pcscd/pcscd.comm ipc: container:phluido_l1 tty: true privileged: true depends_on: - phluido_l1 entrypoint: [\"/bin/sh\", \"-c\", \"sleep 2 && exec /gnb_du_main_phluido /config.json\"] working_dir: \"/workdir\" extra_hosts: - \"cu:$F1_CU_IP\" - \"du:$SERVER_IP\" network_mode: host cpuset: \"$CORE_SET_DU\" EOF Notes: 1) the version has to be changed to '2' as version 3 does not support cpuset option 2) the ip address for the cu is the one of the f1 external ip interface of the relative cucp service 3) the DU ip address is the one of the server where the DU runs NOTE : verify the cpu pinning of the VM's are different then those we used in above compose file. We then need to take care of the CPUs that the VMs hosted on this server are intended to use. Earlier in the installation the CPU pinning has been done during creation. : virsh edit $CU_VM_NAME virsh edit $OPEN5GS_VM_NAME Other ways of creating a VM may not produce a configuration file in xml format, making things more difficult. We also recommend to identify the xml configuration file by searching the directory: /etc/libvirt/qemu/ But we definitely discourage the direct editing of such file as it will reset to default at the first reboot Once done, you can check the content of the xml configuration file, that in this case will show we decided to assign the odd CPUs to the VM: ubuntu@bbu3:~$ sudo cat /etc/libvirt/qemu/Ubuntu123.xml <!-- WARNING: THIS IS AN AUTO-GENERATED FILE. CHANGES TO IT ARE LIKELY TO BE OVERWRITTEN AND LOST. Changes to this xml configuration should be made using: virsh edit Ubuntu123 or other application using the libvirt API. --> <domain type = 'kvm' > <name>Ubuntu123</name> <uuid>f18a2a01-7b67-4f00-ad11-5920ec2b6f16</uuid> <metadata> <libosinfo:libosinfo xmlns:libosinfo = \"http://libosinfo.org/xmlns/libvirt/domain/1.0\" > <libosinfo:os id = \"http://ubuntu.com/ubuntu/20.04\" /> </libosinfo:libosinfo> </metadata> <memory unit = 'KiB' >33554432</memory> <currentMemory unit = 'KiB' >33554432</currentMemory> <vcpu placement = 'static' cpuset = '1,3,5,7,9,11,13,15' >8</vcpu> <os> < type arch = 'x86_64' machine = 'pc-q35-4.2' >hvm</type> <bootmenu enable = 'yes' /> </os> <features> set softirq priorities to realtime \u00b6 In a normal setup, the softirq processes will run at priority 20, equal to all user processes. Here they need to run at -2, which corresponds to real time priority. They are scheduled on all cores but will get strict priority over any other user processes. To adapt the priority of the ksoft, you can use spcific commands: to set to realtime priority 1 (lowest prio, but still \"run to completion\" before other default processes are executed): ps -A | grep ksoftirq | awk '{print $1}' | xargs -L1 sudo chrt -p 1 NOTE: to revert the priority to \"other policy\": ps -A | grep ksoftirq | awk '{print $1}' | xargs -L1 sudo chrt --other -p 0 finally to check all the priorities set: ps -A | grep ksoftirq | awk '{print $1}' | xargs -L1 chrt -p Use htop to verify the priorities of the softirq processes. The only thing remaining is now prioritise the softirq processes . One can use htop and work out the options to show priority and CPU ID * Press F2 for ```Setup```, navigate to ```Columns```, add ```PRIORITY``` * Press F2 for ```Setup```, navigate to ```Display Options```, unselect ```Hide kernel threads``` FOR B210 RU ONLY Install Phluido RRU ( docker ) \u00b6 Load the Phluido RRU Docker image (this step does not have to be taken when using Benetel RUs): docker build -f accelleran-du-phluido/accelleran-du-phluido-2022-01-31/phluido/docker/Dockerfile.rru -t phluido_rru:v0.8.1 Phluido5GL1/Phluido5GL1_v0.8.1 Prepare and bring on air the USRP B210 Radio \u00b6 This section is exclusively applicable to the user/customer that intends to use the Ettus USRP B210 Radio End with our Accellleran 5G end to end solution, if you do not have such radio end the informations included in this section may be misleading and bring to undefined error scenarios. Please contact Accelleran if your Radio End is not included in any section of this user guide Create the UDEV rules for the B210: sudo tee /etc/udev/rules.d/uhd-usrp.rules <<EOF # # Copyright 2011,2015 Ettus Research LLC # Copyright 2018 Ettus Research, a National Instruments Company # # SPDX-License-Identifier: GPL-3.0-or-later # #USRP1 SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"fffe\", ATTRS{idProduct}==\"0002\", MODE:=\"0666\" #B100 SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"2500\", ATTRS{idProduct}==\"0002\", MODE:=\"0666\" #B200 SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"2500\", ATTRS{idProduct}==\"0020\", MODE:=\"0666\" SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"2500\", ATTRS{idProduct}==\"0021\", MODE:=\"0666\" SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"2500\", ATTRS{idProduct}==\"0022\", MODE:=\"0666\" SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"3923\", ATTRS{idProduct}==\"7813\", MODE:=\"0666\" SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"3923\", ATTRS{idProduct}==\"7814\", MODE:=\"0666\" EOF Connect the B210 to the machine. Make sure it is enumerated as USB3 by executing: lsusb -d 2500 :0020 -v | grep -F bcdUSB This should print: bcdUSB 3.00 Add the Ettus Research APT repositories: sudo add-apt-repository ppa:ettusresearch/uhd sudo apt update Install the software required by the B210: sudo apt install libuhd-dev uhd-host libuhd3.15.0 Download the UHD images: sudo uhd_images_downloader sudo /usr/lib/uhd/utils/uhd_images_downloader.py Check if the B210 is detecting using the following command: uhd_find_devices This should output something similar to: [INFO] [UHD] linux; GNU C++ version 7.5.0; Boost_106501; UHD_3.15.0.0-release -------------------------------------------------- -- UHD Device 0 -------------------------------------------------- Device Address: serial: 3218C86 name: MyB210 product: B210 type: b200 Burn the correct EEPROM for the B210: /usr/lib/uhd/utils/usrp_burn_mb_eeprom* --values = 'name=B210-#4' If everything goes well this should output something similar to: Creating USRP device from address: [INFO] [UHD] linux; GNU C++ version 7.5.0; Boost_106501; UHD_3.15.0.0-release [INFO] [B200] Detected Device: B210 [INFO] [B200] Loading FPGA image: /usr/share/uhd/images/usrp_b210_fpga.bin... [INFO] [B200] Operating over USB 3. [INFO] [B200] Detecting internal GPSDO.... [INFO] [GPS] No GPSDO found [INFO] [B200] Initialize CODEC control... [INFO] [B200] Initialize Radio control... [INFO] [B200] Performing register loopback test... [INFO] [B200] Register loopback test passed [INFO] [B200] Performing register loopback test... [INFO] [B200] Register loopback test passed [INFO] [B200] Setting master clock rate selection to 'automatic'. [INFO] [B200] Asking for clock rate 16.000000 MHz... [INFO] [B200] Actually got clock rate 16.000000 MHz. Fetching current settings from EEPROM... EEPROM [\"name\"] is \"MyB210\" Setting EEPROM [\"name\"] to \"B210-#4\"... Power-cycle the USRP device for the changes to take effect. Done Check if the current EEPROM was flashed by executing: uhd_find_devices The output should look like: [INFO] [UHD] linux; GNU C++ version 7.5.0; Boost_106501; UHD_3.15.0.0-release -------------------------------------------------- -- UHD Device 0 -------------------------------------------------- Device Address: serial: 3218C86 name: B210-#4 product: B210 type: b200 DU/L1/RRU Configuration and docker compose \u00b6 Before starting the configuration of the components it is important to avoid confusion so please create a folder file and move in all the configuration files you find for the L1, RRU and the DU configuration and remove the docker-compose as well: mkdir accelleran-du-phluido/accelleran-du-phluido-2022-01-31/phluido/cfg mv accelleran-du-phluido/accelleran-du-phluido-2022-01-31/phluido/*.cfg accelleran-du-phluido/accelleran-du-phluido-2022-01-31/phluido/cfg/ mkdir accelleran-du-phluido/accelleran-du-phluido-2022-01-31/json mv accelleran-du-phluido/accelleran-du-phluido-2022-01-31/*.json accelleran-du-phluido/accelleran-du-phluido-2022-01-31/json/ rm accelleran-du-phluido/accelleran-du-phluido-2022-01-31/docker-compose.yml Create a configuration file for the Phluido RRU: only for B210 tee accelleran-du-phluido/accelleran-du-phluido-2022-01-31/phluido/PhluidoRRU_NR_EffnetTDD_B210.cfg <<EOF /****************************************************************** * * This file is subject to the terms and conditions defined in * file 'LICENSE.txt', which is part of this source code package. * ******************************************************************/ //BBU IPv4 address. bbuFronthaulServerAddr = \"127.0.0.1\"; // Number of slots in one subframe. numSubframeSlots = 2; //Number of TX antennas for fronthaul data exchange. numTxAntennas = 1; //Number of RX antennas for fronthaul data exchange. numRxAntennas = 1; /// Frequency [kHz] for TX \"point A\" (see NR definition). txFreqPointA_kHz = 3301680; /// Frequency [kHz] for RX \"point A\" (see NR definition). rxFreqPointA_kHz = 3301680; /// Number of PRBs for both downlink and uplink. Must match the L2-L3 configuration. numPrbs = 51; uhdClockMode = 0; uhdSendAdvance_ns = 8000; /// Parameters used for PhluidoPrototypeBPP. bppMode = 10; uhdSamplingRate_kHz = 23040; EOF Create a configuration file for the Phluido L1. Make sure to set the value LicenseKey option to the received Phluido license key. This key has been delivered by Phluido upon receipt of the .bin file generated by the sysTest you have performed at start of this installation. tee l1-config.cfg <<EOF /****************************************************************** * * This file is subject to the terms and conditions defined in * file 'LICENSE'. * ******************************************************************/ // Enables verbose binary logging. WARNING: very CPU intensive and creates potentially huge output files. Use with caution. //logLevel_verbose = \"DEBUG\"; bbuFronthaulServerMode = 1; bbuFronthaulServerAddr = \"10.10.0.1\" bbuFronthaulServerPort = 44000; /// BBU fronthaul server \"busy poll\" for the receive socket, in microseconds, used as value for the relevant (SOL_SOCKET,SO_BUSY_POLL) socket option. numWorkers = 4; // Enable 64-QAM support for PUSCH (license-specific) maxPuschModOrder = 6; maxNumPdschLayers = 2; maxNumPuschLayers = 1; maxNumPrachDetectionSymbols = 1; targetRecvDelay_us = 2500; //targetCirPosition = 0.0078125; // License key //LicenseKey = \"2B2A-962F-783F-40B9-7064-2DE3-3906-9D2E\" EOF **IMPORTANT: After this replace the LicenseKey value with the effective license sequence you obtained from Accelleran Create a configuration file for the Effnet DU: nr_cell_identity ( in binary format eg 3 fill in ...00011 ) nr_pci ( decimal format eg 51 fill in 51 ) plmn_identity ( eg 235 88 fill in 235f88. fill in 2 times in this file) arfcn ( decimal format calculated from the center frequency, see chapter ) nr_frequency_band ( 77 or 78 ) 5gs_tac ( 3 byte array. eg 1 fill in 000001 ) mkdir -p ~/install- $DU_VERSION / cd !$ tee du-config.json <<EOF { \"configuration\": { \"du_address\": \"du\", \"cu_address\": \"cu\", \"gtp_listen_address\": \"du\", \"f1c_bind_address\": \"du\", \"vphy_listen_address\": \"127.0.0.1\", \"vphy_port\": 13337, \"vphy_tick_multiplier\": 1, \"gnb_du_id\": 38209903575, \"gnb_du_name\": \"cab-03-cell\", \"phy_control\": { \"crnti_range\": { \"min\": 42000, \"max\": 42049 } }, \"rrc_version\": { \"x\": 15, \"y\": 6, \"z\": 0 }, \"served_cells_list\": [ { \"served_cell_information\": { \"nr_cgi\": { \"plmn_identity\": \"235f88\", \"nr_cell_identity\": \"000000000000000000000000000000000011\" }, \"nr_pci\": 2, \"5gs_tac\": \"000001\", \"ran_area_code\": 1, \"served_plmns\": [ { \"plmn_identity\": \"235f88\", \"tai_slice_support_list\": [ { \"sst\": 1 } ] } ], \"nr_mode_info\": { \"nr_freq_info\": { \"nr_arfcn\": 662664, \"frequency_band_list\": [ { \"nr_frequency_band\": 77 } ] }, \"transmission_bandwidth\": { \"bandwidth_mhz\": 40, \"scs_khz\": 30, \"nrb\": 106 }, \"pattern\": { \"periodicity_in_slots\": 10, \"downlink\": { \"slots\": 7, \"symbols\": 6 }, \"uplink\": { \"slots\": 2, \"symbols\": 4 } } }, \"measurement_timing_configuration\": [ 222, 173, 190, 239 ], \"dmrs_type_a_position\": \"pos2\", \"intra_freq_reselection\": \"allowed\", \"ssb_pattern\": \"1000000000000000000000000000000000000000000000000000000000000000\", \"ssb_periodicity_serving_cell_ms\": 20, \"prach_configuration_index\": 202, \"ssb_pbch_scs\": 30, \"offset_point_a\": 6, \"k_ssb\": 0, \"coreset_zero_index\": 3, \"search_space_zero_index\": 2, \"ra_response_window_slots\": 20, \"sr_slot_periodicity\": 40, \"sr_slot_offset\": 7, \"search_space_other_si\": 1, \"paging_search_space\": 1, \"ra_search_space\": 1, \"bwps\": [ { \"id\": 0, \"start_crb\": 0, \"num_rb\": 106, \"scs\": 30, \"cyclic_prefix\": \"normal\" } ], \"coresets\": [ { \"id\": 1, \"bwp_id\": 0, \"fd_resources\": \"111100000000000000000000000000000000000000000\", \"duration\": 2, \"interleaved\": { \"reg_bundle_size\": 6, \"interleaver_size\": 2 }, \"precoder_granularity\": \"same_as_reg_bundle\" } ], \"search_spaces\": [ { \"id\": 1, \"control_resource_set_id\": 0, \"common\": {} }, { \"id\": 2, \"control_resource_set_id\": 1, \"ue_specific\": { \"dci_formats\": \"formats0-1-And-1-1\" } } ], \"maximum_ru_power_dbm\": 53.0, \"num_tx_antennas\": 2, \"trs\": { \"periodicity_and_offset\": { \"period\": 80, \"offset\": 1 }, \"symbol_pair\": \"four_eight\", \"subcarrier_location\": 1 }, \"csi_rs\": { \"periodicity_and_offset\": { \"period\": 40, \"offset\": 15 } }, \"harq_processes_for_pdsch\": 16, \"minimum_k1_delay\": 1, \"minimum_k2_delay\": 3, \"force_rlc_buffer_size\": 2500000 } } ] } } EOF Before creating the docker-compose.yml file, make sure to set the $CU_IP environment variable where you will store the F1 IP address of the CUCP that you have already deployed using the dRAX Dashboard (section CUCP Installation ) This IP address can be determined by executing the following command: kubectl get services | grep f1 The CUCP F1 SCTP interface external address is the second IP address and should be in the IP pool that was assigned to MetalLB in dRax Installation . Now, create a docker-compose configuration file: remove the rru: when NOT using a b210. eg when using a b650 mkdir -p ~/install- $DU_VERSION / cd !$ tee docker-compose.yml <<EOF version: \"2\" services: phluido_l1: image: phluido_l1:v0.8.4.2 container_name: phluido_l1 tty: true privileged: true ipc: shareable shm_size: 2gb command: /config.cfg volumes: - \"$PWD/l1-config.cfg:/config.cfg:ro\" - \"/run/logs-du/l1:/workdir\" - \"/etc/machine-id:/etc/machine-id:ro\" working_dir: \"/workdir\" network_mode: host cpuset: \"0,2,4,6,8,10,12,14\" du: image: gnb_du_main_phluido:2022-07-01-q2-pre-release volumes: - \"$PWD/du-config.json:/config.json:ro\" - \"/run/logs-du/du:/workdir\" - /run/pcscd/pcscd.comm:/run/pcscd/pcscd.comm ipc: container:phluido_l1 tty: true privileged: true depends_on: - phluido_l1 entrypoint: [\"/bin/sh\", \"-c\", \"sleep 4 && exec /gnb_du_main_phluido /config.json\"] working_dir: \"/workdir\" extra_hosts: - \"cu:$F1_CU_IP\" - \"du:$SERVER_IP\" cpuset: \"0,2,4,6,8,10,12,14\" phluido_rru: image: phluido_rru:v0.8.4.2 tty: true privileged: true depends_on: - du - phluido_l1 network_mode: host volumes: - \"$PWD/phluido/PhluidoRRU_NR_EffnetTDD_B210.cfg:/config.cfg:ro\" - \"$PWD/logs/rru:/workdir\" entrypoint: [\"/bin/sh\", \"-c\", \"sleep 20 && exec /PhluidoRRU_NR /config.cfg\"] working_dir: \"/workdir\" EOF Start the DU \u00b6 Start the DU by running the following command: docker-compose up -f accelleran-du-phluido/accelleran-du-phluido-2022-01-31/docker-compose.yml If all goes well this will produce output similar to: Starting phluido_l1 ... done Recreating accelleran-du-phluido-2022-01-31_du_1 ... done Recreating accelleran-du-phluido-2022-01-31_phluido_rru_1 ... done Attaching to phluido_l1, accelleran-du-phluido-2022-01-31_du_1, accelleran-du-phluido-2022-01-31_phluido_rru_1 phluido_l1 | Reading configuration from config file \"/config.cfg\"... phluido_l1 | ******************************************************************************************************* phluido_l1 | * * phluido_l1 | * Phluido 5G-NR virtualized L1 implementation * phluido_l1 | * * phluido_l1 | * Copyright (c) 2014-2020 Phluido Inc. * phluido_l1 | * All rights reserved. * phluido_l1 | * * phluido_l1 | * The User shall not, and shall not permit others to: * phluido_l1 | * - integrate Phluido Software within its own products; * phluido_l1 | * - mass produce products that are designed, developed or derived from Phluido Software; * phluido_l1 | * - sell products which use Phluido Software; * phluido_l1 | * - modify, correct, adapt, translate, enhance or otherwise prepare derivative works or * phluido_l1 | * improvements to Phluido Software; * phluido_l1 | * - rent, lease, lend, sell, sublicense, assign, distribute, publish, transfer or otherwise * phluido_l1 | * make available the PHLUIDO Solution or any portion thereof to any third party; * phluido_l1 | * - reverse engineer, disassemble and/or decompile Phluido Software. * phluido_l1 | * * phluido_l1 | * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, * phluido_l1 | * INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A * phluido_l1 | * PARTICULAR PURPOSE ARE DISCLAIMED. * phluido_l1 | * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, * phluido_l1 | * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF * phluido_l1 | * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) * phluido_l1 | * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR * phluido_l1 | * TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS * phluido_l1 | * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. * phluido_l1 | * * phluido_l1 | ******************************************************************************************************* phluido_l1 | phluido_l1 | Copyright information already accepted on 2020-11-27, 08:56:08. phluido_l1 | Starting Phluido 5G-NR L1 software... phluido_l1 | PHAPI version = 0.5 (12/10/2020) phluido_l1 | L1 SW version = 0.8.1 phluido_l1 | L1 SW internal rev = r3852 phluido_l1 | Parsed configuration parameters: phluido_l1 | LicenseKey = XXXX-XXXX-XXXX-XXXX-XXXX-XXXX-XXXX-XXXX phluido_l1 | maxNumPrachDetectionSymbols = 1 phluido_l1 | maxNumPdschLayers = 2 phluido_l1 | maxNumPuschLayers = 1 phluido_l1 | maxPuschModOrder = 6 phluido_l1 | phluido_rru_1 | linux; GNU C++ version 7.3.0; Boost_106501; UHD_003.010.003.000-0-unknown phluido_rru_1 | phluido_rru_1 | Logs will be written to file \"phluidoRru.log\". phluido_rru_1 | -- Detected Device: B210 phluido_rru_1 | -- Operating over USB 3. phluido_rru_1 | -- Initialize CODEC control... phluido_rru_1 | -- Initialize Radio control... phluido_rru_1 | -- Performing register loopback test... pass phluido_rru_1 | -- Performing register loopback test... pass phluido_rru_1 | -- Performing CODEC loopback test... pass phluido_rru_1 | -- Performing CODEC loopback test... pass phluido_rru_1 | -- Setting master clock rate selection to 'automatic'. phluido_rru_1 | -- Asking for clock rate 16.000000 MHz... phluido_rru_1 | -- Actually got clock rate 16.000000 MHz. phluido_rru_1 | -- Performing timer loopback test... pass phluido_rru_1 | -- Performing timer loopback test... pass phluido_rru_1 | -- Setting master clock rate selection to 'manual'. phluido_rru_1 | -- Asking for clock rate 23.040000 MHz... phluido_rru_1 | -- Actually got clock rate 23.040000 MHz. phluido_rru_1 | -- Performing timer loopback test... pass phluido_rru_1 | -- Performing timer loopback test... pass Prepare and bring on air the Benetel 650 Radio \u00b6 This section is exclusively applicable to the user/customer that intends to use the Benetel B650 Radio End with our Accellleran 5G end to end solution, if you do not have such radio end the informations included in this section may be misleading and bring to undefined error scenarios. Please contact Accelleran if your Radio End is not included in any section of this user guide Diagram \u00b6 In the picture below we schematically show what will be run on the server by Docker and how the RRU is linked to the server itself: as mentioned early in this case the two components run by Docker are the L1 and the DU, while the RRU is supposedly served by a dedicated NIC Card capable of handling a 10 Gbps fiber link. If this is not your case please consult the section dedicted to Ettus B210 bring up or contact Accelleran for further information 10.10.0.100:ssh +-------------+ | | | | +-----------+ +-----------+ | | | | | | | RU +----fiber----+ L1 | | DU | | | | | | | | | +-----------+ +-----------+ | | +-------------+ aa:bb:cc:dd:ee:ff 11:22:33:44:55:66 10.10.0.2:44000 10.10.0.1:44000 eth0 enp45s0f0 port FIBER1 Server installtion \u00b6 DU/L1 Configuration and docker compose \u00b6 Differently from the Ettus B210, Benetel runs the RRU software on board, therefore we only need to prepare 2 software components in the server, that is, 2 Containers, Effnet DU and Phluido L1. Create the configuration file for the Phluido L1 component the PhluidoL1_NR_Benetel.cfg file delivered by effnet Make sure to set the value LicenseKey option to the received Phluido license key: mkdir -p ~/install- $DU_VERSION / cd !$ tee PhluidoL1_NR_Benetel.cfg <<EOF /****************************************************************** * * This file is subject to the terms and conditions defined in * file 'LICENSE'. * ******************************************************************/ //Enables verbose binary logging. WARNING: very CPU intensive and creates potentially huge output files. Use with caution. // // DEVEL: 1.2G/67 // DEBUG: 953M/67 // INFORMATIVE: 29K/67 // default: 26K/67 // CRITICAL: 0/393 // WARNING: 0/393 (x/y x=log size in L1.encr.log file, y=log size in L1.open.log file. In a time of 1 minute ) //logLevel_verbose = \"WARNING\"; //bbuFronthaulServerMode = 1; bbuFronthaulServerAddr = \"10.10.0.1\"; // Enable 64-QAM support for PUSCH (license-specific). maxPuschModOrder = 6; maxNumPdschLayers = 2; maxNumPuschLayers = 1; cccServerPort = 44444; cccInterfaceMode = 1; kpiOutputFormat = 2; targetRecvDelay_us = 2500; numWorkers = 6; // shall be less or equal to the number of cores assigned to L1 with the CPU pinning //License key put here please the effective 32 digits sequence you received for this deployment LicenseKey = \"$L1_PHLUIDO_KEY\"; EOF **IMPORTANT: After this replace the LicenseKey value with the effective license sequence you obtained from Accelleran, all the other parameters shall not be modified Create a configuration file for the Effnet DU: mkdir -p ~/install- $DU_VERSION / cd !$ tee b650_config_40mhz.json <<EOF { \"configuration\": { \"du_address\": \"du\", \"cu_address\": \"cu\", \"f1c_bind_address\": \"du\", \"gtp_listen_address\": \"du\", \"vphy_listen_address\": \"127.0.0.1\", \"vphy_port\": 13337, \"vphy_tick_multiplier\": 1, \"gnb_du_id\": 38209903575, \"gnb_du_name\": \"This is the dell two HO setup cell one\", \"phy_control\": { \"crnti_range\": { \"min\": 42000, \"max\": 42049 } }, \"rrc_version\": { \"x\": 15, \"y\": 6, \"z\": 0 }, \"served_cells_list\": [ { \"served_cell_information\": { \"nr_cgi\": { \"plmn_identity\": \"001f01\", \"nr_cell_identity\": \"000000000000000000000000000000000001\" }, \"nr_pci\": 51, \"5gs_tac\": \"000001\", \"ran_area_code\": 1, \"served_plmns\": [ { \"plmn_identity\": \"001f01\", \"tai_slice_support_list\": [ { \"sst\": 1 } ] } ], \"nr_mode_info\": { \"nr_freq_info\": { \"nr_arfcn\": 648840, \"frequency_band_list\": [ { \"nr_frequency_band\": 78 } ] }, \"transmission_bandwidth\": { \"bandwidth_mhz\": 40, \"scs_khz\": 30, \"nrb\": 106 }, \"pattern\": { \"periodicity_in_slots\": 10, \"downlink\": { \"slots\": 7, \"symbols\": 6 }, \"uplink\": { \"slots\": 2, \"symbols\": 4 } } }, \"measurement_timing_configuration\": [ 222, 173, 190, 239 ], \"dmrs_type_a_position\": \"pos2\", \"intra_freq_reselection\": \"allowed\", \"ssb_pattern\": \"1000000000000000000000000000000000000000000000000000000000000000\", \"ssb_periodicity_serving_cell_ms\": 20, \"prach_configuration_index\": 202, \"ssb_pbch_scs\": 30, \"offset_point_a\": 6, \"k_ssb\": 0, \"coreset_zero_index\": 3, \"search_space_zero_index\": 2, \"ra_response_window_slots\": 20, \"sr_slot_periodicity\": 40, \"sr_slot_offset\": 7, \"search_space_other_si\": 1, \"paging_search_space\": 1, \"ra_search_space\": 1, \"bwps\": [ { \"id\": 0, \"start_crb\": 0, \"num_rb\": 106, \"scs\": 30, \"cyclic_prefix\": \"normal\" } ], \"coresets\": [ { \"id\": 1, \"bwp_id\": 0, \"fd_resources\": \"111100000000000000000000000000000000000000000\", \"duration\": 2, \"interleaved\": { \"reg_bundle_size\": 6, \"interleaver_size\": 2 }, \"precoder_granularity\": \"same_as_reg_bundle\" } ], \"search_spaces\": [ { \"id\": 1, \"control_resource_set_id\": 0, \"common\": {} }, { \"id\": 2, \"control_resource_set_id\": 1, \"ue_specific\": { \"dci_formats\": \"formats0-1-And-1-1\" } } ], \"maximum_ru_power_dbm\": 35.0, \"num_tx_antennas\": 2, \"trs\": { \"periodicity_and_offset\": { \"period\": 80, \"offset\": 1 }, \"symbol_pair\": \"four_eight\", \"subcarrier_location\": 1 }, \"periodic_srs_periodicity\": 64, \"csi_rs\": { \"periodicity_and_offset\": { \"period\": 40, \"offset\": 15 } }, \"force_rlc_buffer_size\": 8388608, \"harq_processes_for_pdsch\": 16, \"minimum_k1_delay\": 1, \"minimum_k2_delay\": 3 } } ] } } EOF Frequency, Offsets, Point A Calculation \u00b6 This section is essential to proceed correctly and determine the exact parameters that will allow the Benetel Radio to go on air correctly and the UEs to be able to see the cell and attempt an attach so it is particularly important to proceed carefully on this point. there are currently several limitations on the Frequencies that go beyond the simple definition of 5G NR Band: the selected frequency should be above 3700 MHz the selected band can be B77 or B78 the selected frequency must be devisable by 3.84 the K_ssb must be 0 the offset to point A must be 6 all the subcarriers shal be 30 KHz Let's proceed with an example: We want to set a center frequency of 3750 MHz, this is not devisable by 3.84, the first next frequencies that meet this condition are 3747,84 (976 3.84) 3751.68 (977 3,84) so let's consider first 3747,84 MHz and verify the conditions on the K_ssb and Offset to Point A with this online tool (link at: (https://www.sqimway.com/nr_refA.php) ) We remember to set the Band 78, SCs at 30 KHz, the Bandwidth at 40 MHz and the ARFCN of the center frequency 3747,84 which is 649856 and when we hit the RUN button we obtain: This Frequency, however does not meet the GSCN Synchronisation requirements as in fact the Offset to Point A of the first channel is 2 and the K_ssb is 16, this will cause the UE to listen on the wrong channel so the SIBs will never be seen and therefore the cell is \"invisible\" We then repeat the exercise with the higher center frequency 3751,68 MHz, which yelds a center frequency ARFCN of 650112 and a point A ARFCN of 648840 and giving another run we will see that now the K_ssb and the Offset to Point A are correct: With these information at hand we are going to determine: point A frequency : 3732.60 ( arfcn : 648840 ) - edit du configuration in the appropriate json file center Frequency : 3751.68 ( arfcn : 650112 ) - edit RU configuration directly on the Benetel Radio End (see next sections) Create docker compose \u00b6 Before creating the docker-compose.yml file, make sure to set the $CU_IP environment variable where you will store the F1 IP address of the CUCP that you have already deployed using the dRAX Dashboard (section CUCP Installation ) This IP address can be determined by executing the following command: kubectl get services | grep f1 The CUCP F1 SCTP interface external address is the second IP address and should be in the IP pool that was assigned to MetalLB in dRax Installation . Now, create a docker-compose configuration file: mkdir -p ~/install- $DU_VERSION / cd !$ tee docker-compose-B650.yml <<EOF version: \"2\" services: phluido_l1: image: phluido_l1:v0.8.4.2 container_name: phluido_l1 tty: true privileged: true ipc: shareable shm_size: 2gb command: /config.cfg volumes: - \"$PWD/PhluidoL1_NR_Benetel.cfg:/config.cfg:ro\" - \"/run/logs-du/l1:/workdir\" - \"/etc/machine-id:/etc/machine-id:ro\" working_dir: \"/workdir\" network_mode: host cpuset: \"$CORE_SET_DU\" du: image: gnb_du_main_phluido:2022-07-01-q2-pre-release volumes: - \"$PWD/b650_config_40mhz.json:/config.json:ro\" - \"/run/logs-du/du:/workdir\" - /run/pcscd/pcscd.comm:/run/pcscd/pcscd.comm ipc: container:phluido_l1 tty: true privileged: true depends_on: - phluido_l1 entrypoint: [\"/bin/sh\", \"-c\", \"sleep 2 && exec /gnb_du_main_phluido /config.json\"] working_dir: \"/workdir\" extra_hosts: - \"cu:$F1_CU_IP\" - \"du:$SERVER_IP\" network_mode: host cpuset: \"$CORE_SET_DU\" EOF Prepare to configure the Benetel 650 \u00b6 The benetel is connected with a fiber to the server. 1. The port on the physical B650 RRU is labeled port FIBER1 2. The port on the server is one of these listed below. $ lshw | grep SFP -C 5 WARNING: you should run this program as super-user. capabilities: pci normal_decode bus_master cap_list configuration: driver = pcieport resources: irq:29 ioport:f000 ( size = 4096 ) memory:f8000000-f86fffff *-network:0 DISABLED description: Ethernet interface product: 82599ES 10 -Gigabit SFI/SFP+ Network Connection vendor: Intel Corporation physical id: 0 bus info: pci@0000:2d:00.0 logical name: enp45s0f0 version: 01 -- capabilities: bus_master cap_list rom ethernet physical fibre 10000bt-fd configuration: autonegotiation = off broadcast = yes driver = ixgbe driverversion = 5 .1.0-k firmware = 0x2b2c0001 latency = 0 link = no multicast = yes resources: irq:202 memory:f8000000-f807ffff ioport:f020 ( size = 32 ) memory:f8200000-f8203fff memory:f8080000-f80fffff memory:f8204000-f8303fff memory:f8304000-f8403fff *-network:1 DISABLED description: Ethernet interface product: 82599ES 10 -Gigabit SFI/SFP+ Network Connection vendor: Intel Corporation physical id: 0 .1 bus info: pci@0000:2d:00.1 logical name: enp45s0f1 version: 01 by setting both network devices to UP you find out which one is connected. In this example it's enp45s0f0. This port is the one we connected the fiber to. :ad@5GCN:~$ sudo ip link set dev enp45s0f0 up :ad@5GCN:~$ sudo ip link set dev enp45s0f1 up :ad@5GCN:~$ ip -br a : enp45s0f0 UP fe80::6eb3:11ff:fe08:a4e0/64 enp45s0f1 DOWN : configure the static ip 10.10.0.1 of port enp45s0f0 on your server netplan (typically /etc/netplan/50-cloud-init.yaml ) as follows: network: ethernets: enp45s0f0: dhcp4: false dhcp6: false optional: true addresses: - 10 .10.0.1/24 mtu: 9000 To apply this configuration you can use sudo netplan apply Double check the result $ ip -br a | grep enp45 enp45s0f0 UP 10 .10.0.1/24 fe80::6eb3:11ff:fe08:a4e0/64 enp45s0f1 DOWN The default ip of the benetel radio is 10.10.0.100 . This is the MGMT ip. We can ssh to it as root@10.10.0.100 without password We can anyway find that IP out using nmap $ nmap 10 .10.0.0/24 Starting Nmap 7 .60 ( https://nmap.org ) at 2021 -09-21 10 :15 CEST Nmap scan report for 10 .10.0.1 Host is up ( 0 .000040s latency ) . Not shown: 996 closed ports PORT STATE SERVICE 22 /tcp open ssh 111 /tcp open rpcbind 5900 /tcp open vnc 9100 /tcp open jetdirect Nmap scan report for 10 .10.0.100 Host is up ( 0 .0053s latency ) . Not shown: 998 closed ports PORT STATE SERVICE 22 /tcp open ssh 111 /tcp open rpcbind Nmap done : 256 IP addresses ( 2 hosts up ) scanned in 3 .10 seconds A route is added also in the routing table automatically $ route -n | grep 10 .10.0.0 10 .10.0.0 0 .0.0.0 255 .255.255.0 U 0 0 0 enp45s0f0 ```` now you can ssh to the benetel ``` bash $ ssh root@10.10.0.100 Last login: Fri Feb 7 16 :45:59 2020 from 10 .10.0.1 root@benetelru:~# ls -l -rwxrwxrwx 1 root root 1572 Sep 10 2021 DPLL3_1PPS_REGISTER_PATCH.txt drwxrwxrwx 2 root root 0 Feb 7 16 :44 adrv9025 -rwxrwxrwx 1 root root 1444 Feb 7 16 :40 dev_struct.dat -rwxrwxrwx 1 root root 17370 Sep 10 2021 dpdModelReadback.txt -rwxrwxrwx 1 root root 5070 Feb 7 17 :00 dpdModelcoefficients.txt -rwxrwxrwx 1 root root 24036 Sep 10 2021 eeprog_cp60 -rwxrwxrwx 1 root root 1825062 Feb 7 15 :58 madura_log_file.txt -rw------- 1 root root 1230 Feb 7 2020 nohup.out -rwxr-xr-x 1 root root 57 Feb 7 2020 nohup_handshake -rwxrwxrwx 1 root root 571 Feb 7 2020 progBenetelDuMAC_CATB -rwxr-xr-x 1 root root 1121056 Feb 7 16 :24 quickRadioControl -rwxrwxrwx 1 root root 1151488 Sep 10 2021 radiocontrol_prv-nk-cliupdate -rwxrwxrwx 1 root root 22904 Aug 24 2021 registercontrol -rwxrwxrwx 1 root root 164 Feb 7 16 :35 removeResetRU_CATB -rwxrwxrwx 1 root root 163 Feb 7 2020 reportRuStatus -rwxrwxrwx 1 root root 162 Feb 7 16 :35 resetRU_CATB -rwxr-xr-x 1 root root 48 Feb 7 15 :57 runSync -rwxrwxrwx 1 root root 21848 Sep 10 2021 smuconfig -rwxrwxrwx 1 root root 17516 Sep 10 2021 statmon -rwxrwxrwx 1 root root 23248 Sep 10 2021 syncmon -rwxr-xr-x 1 root root 182 Feb 7 16 :41 trialHandshake root@benetelru:~# However, as mentioned, that above is the management IP address, whereas for the data interface the Benetel RU has a different MAC on 10.10.0.2 for instance aa:bb:cc:dd:ee:ff and we can put this on the Server where the DU runs in the file: /etc/networkd-dispatcher/routable.d/macs.sh Add mac entry script in routable.d. To find out the $MAC_RU ( the mac address of the RU interface ) use sudo tcpdump -i $SERVER_RU_INT port 44000 -en a trace like this appears 21 :19:20.285848 aa:bb:cc:dd:ee:ff > 00 :1e:67:fd:f5:51, ethertype IPv4 ( 0x0800 ) , length 64 : 10 .10.0.2.44000 > 10 .10.0.1.44000: UDP, length 20 sudo tee /etc/networkd-dispatcher/routable.d/macs.sh <<EOF #!/bin/sh sudo arp -s $RU_IP $MAC_RU -i $SERVER_RU_INT EOF Benetel650 does not answer arp requests. With this arp entry in the arp table the server knows to which mac address the ip packets with destination ip 10.10.0.2 should go the macs.sh script is executes automatically if it has the correct permissions. Set the correct permissions. sudo chown root /etc/networkd-dispatcher/routable.d/macs.sh sudo chgrp root /etc/networkd-dispatcher/routable.d/macs.sh sudo chmod 755 /etc/networkd-dispatcher/routable.d/macs.sh The macs.sh script runs when the interface to the RU goes UP. Run this to bring the RU interface UP sudo ip link set $SERVER_RU_INT down sleep 1 sudo ip link set $SERVER_RU_INT up now check if the entry for $RU_IP is in the arp table. $ arp -a | grep 10 .10. ? ( $RU_IP ) at $MAC_RU [ ether ] PERM on $SERVER_RU_INT When the fiber port comes up at the server side eno2 UP 10.10.0.1/24 fe80::266e:96ff:fe43:64e2/64 test the automatic execution of macs.sh by running journalctl -f and plugging in the fiber. Each time it is plugged in you will see the the execution of the arp which has been put in the macs.sh script above. Version Check \u00b6 finding out the version and commit hash of the benetel650 commit hash root@benetelru:~# registercontrol -v Lightweight HPS-to-FPGA Control Program Version : V1.2.0 ****BENETEL PRODUCT VERSIONING BLOCK**** This Build Was Created Locally. Please Use Git Pipeline! Project ID NUMBER: 0 Git # Number: f6366d7adf84933ab2b242a345bd63c07fedb9e5 Build ID: 0 Version Number: 0.0.1 Build Date: 2/12/2021 Build Time H:M:S: 18:20:3 ****BENETEL PRODUCT VERSIONING BLOCK END**** The version which is referred to. This is version 0.3. Depending on the version different configuration commands apply. root@benetelru:~# cat /etc/benetel-rootfs-version RAN650-2V0.3 Configure the physical Benetel Radio End - Release V0.5.x \u00b6 There are several parameters that can be checked and modified by reading writing the EEPROM, for this we recommend to make use of the original Benetel Software User Guide for RANx50-02 CAT-B O-RUs, in case of doubt ask for clarification to Accelleran Customer Support . Here we just present two of the most used parameters, that will need an adjustment for each deployment. CFR enabled \u00b6 By default the RU ships with CFR enabled. What still needs to be done is set register 0366 to value 0xFFF . Do this by altering file /usr/sbin/radio_setup_ran650_b.sh with following line. registercontrol -w c0366 -x 0xFFF >> ${ LOG_RAD_STAT_FP } MAC Address of the DU \u00b6 Create this script to program the mac address of the DU inside the RRU. Remember the RRU does not request arp, so we have to manually configure that. If the MAC address of the server port you use to connect to the Benetel B650 Radio End (the NIC Card port where the fiber originates from) is $MAC_DU 11:22:33:44:55:66 then you can program the EEPROM of your B650 unit as follows: Here the value of $MAC_DU need to be used. Run this on the bare metal host server to generate the script that will run in the RU to set the mac. echo \" registercontrol -w 0xC036B -x 0x88000088 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1a:0x01:0x $( echo $MAC_DU | cut -c1-2 ) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1b:0x01:0x $( echo $MAC_DU | cut -c4-5 ) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1c:0x01:0x $( echo $MAC_DU | cut -c7-8 ) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1d:0x01:0x $( echo $MAC_DU | cut -c10-11 ) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1e:0x01:0x $( echo $MAC_DU | cut -c13-14 ) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1f:0x01:0x $( echo $MAC_DU | cut -c16-17 ) registercontrol -w 0xC036B -x 0x88000488 \" Something like this will get generated. Copy and Paste this generated script into the RU. registercontrol -w 0xC036B -x 0x88000088 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1a:0x01:0x11 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1b:0x01:0x22 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1c:0x01:0x33 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1d:0x01:0x44 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1e:0x01:0x55 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1f:0x01:0x66 registercontrol -w 0xC036B -x 0x88000488 Login in the RU ssh root@ $RU_MGMT_IP and paste the script here. You can read the EEPROM now and double check what you did: eeprog_cp60 -q -f -x -16 /dev/i2c-0 0x57 -x -r 26:6 !! Finally, reboot your Radio End to make the changes effective Set the Frequency of the Radio End \u00b6 Create this script to program the Center Frequency in MHz of your B650 RRU. Remember to determine a valid frequency as indicated previously in the document, taking into account all the constraints and the relationship to the Offset Point A. If the Center Frequency you want to is for instance 3751,680 MHz then you can program the EEPROM of your B650 unit as follows: Run the below script on the bare metal host. It will product a script that needs to run on the RU. echo\" registercontrol -w 0xC036B -x 0x88000088 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x174:0x01:0x3$(echo $FREQ_CENTER | cut -c1) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x175:0x01:0x3$(echo $FREQ_CENTER | cut -c2) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x176:0x01:0x3$(echo $FREQ_CENTER | cut -c3) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x177:0x01:0x3$(echo $FREQ_CENTER | cut -c4) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x178:0x01:0x2E eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x179:0x01:0x3$(echo $FREQ_CENTER | cut -c6) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17A:0x01:0x3$(echo $FREQ_CENTER | cut -c7) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17B:0x01:0x3$(echo $FREQ_CENTER | cut -c8) registercontrol -w 0xC036B -x 0x88000488 registercontrol -w 0xC036B -x 0x88000088 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17C:0x01:0x3$(echo $FREQ_CENTER | cut -c1) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17D:0x01:0x3$(echo $FREQ_CENTER | cut -c2) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17E:0x01:0x3$(echo $FREQ_CENTER | cut -c3) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17F:0x01:0x3$(echo $FREQ_CENTER | cut -c4) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x180:0x01:0x2E eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x181:0x01:0x3$(echo $FREQ_CENTER | cut -c6) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x182:0x01:0x3$(echo $FREQ_CENTER | cut -c7) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x183:0x01:0x3$(echo $FREQ_CENTER | cut -c8) registercontrol -w 0xC036B -x 0x88000488 \" The script that is produces looks like this. echo\" registercontrol -w 0xC036B -x 0x88000088 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x174:0x01:0x33 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x175:0x01:0x37 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x176:0x01:0x35 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x177:0x01:0x31 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x178:0x01:0x2E eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x179:0x01:0x36 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17A:0x01:0x38 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17B:0x01:0x30 registercontrol -w 0xC036B -x 0x88000488 registercontrol -w 0xC036B -x 0x88000088 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17C:0x01:0x33 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17D:0x01:0x37 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17E:0x01:0x35 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17F:0x01:0x31 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x180:0x01:0x2E eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x181:0x01:0x36 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x182:0x01:0x38 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x183:0x01:0x30 registercontrol -w 0xC036B -x 0x88000488 \" Verify the script if it has the ascii codes for the frequency digits. Each byte 0x33,0x37,0x35, ... is the ascii value of a numbers 3751,680, often the calculation stops at two digits after the comma, consider the last digit always as a zero You may then want to double check what you did by reading the EEPROM: eeprog_cp60 -q -f -16 /dev/i2c-0 0x57 -r 372:8 Copy/Paste this script and run in in the RU. ssh to the RU and pasted it. ssh root@ $RU_MGMT_IP Once again, this is the CENTER FREQUENCY IN MHz that we calculated in the previous sections, and has to go hand in hand with the point A Frequency as discussed above Example for frequency 3751.68MHz (ARFCN=650112) you have set make sure to edit/check the pointA frequency ARFCN value back in the DU config json file in the server (in this example PointA_ARFCN=648840) Reboot the BNTL650 to make changes effective When the RU comes online ( 5 minutes ) run the following to see what the new frequency shows. ssh root $RU_MGMT_IP radiocontrol -o G a Set attenuation level \u00b6 This operation allows to temporary modify the attenuation of the transmitting channels of your B650 unit. Temporarily means that at the next reboot the Cell will default to the originally calibrated values, by default the transmission power is set to 25 dBm hence the attenuation is 15000 mdB (offset to the max TX Power). To adjust this power for the transmitter the user must edit the attenuation setting: For increasing the power the attenuation must be reduced For decreasing the power, the attenuation must be increased IMPORTANT NOTE: As of now, channel 2 and 4 are off and are not up for modification please do not try and modify those attenuation parameters So if we want, for instance, to REDUCE the Tx Power by 5 dB, we will then INCREASE the attenuation by 5000 mdB. Let's consider that each cell is calibrated individually so the first thing to do is to take note of the default values and offset from there to obtained the desired TX Power per channel So here are the steps: read current attenuations ~# radiocontrol -o G a Benetel radiocontrol Version : 0.9.0 Madura API Version : 5.1.0.21 Madura ARM FW version : 5.0.0.32 Madura ARM DPD FW version : 5.0.0.32 Madura Stream version : 8.0.0.5 Madura Product ID : 0x84 Madura Device Revision : 0xb0 Tx1 Attenuation (mdB) : 16100 Tx2 Attenuation (mdB) : 40000 Tx3 Attenuation (mdB) : 15800 Tx4 Attenuation (mdB) : 40000 PLL1 Frequency (Hz) : 0 PLL2 Frequency (Hz) : 3751680000 Front-end Control : 0x2aa491 Madura Deframer 0 : 0x87 Madura Framer 0 : 0xa Internal Temperature (degC) : 47 External Temperature (degC) : 42.789063 RX1 Power Level (dBFS) : -60.750000 RX2 Power Level (dBFS) : -60.750000 RX3 Power Level (dBFS) : -60.750000 RX4 Power Level (dBFS) : -60.750000 ORX1 Peak/Mean Power Level (dBFS) : -10.839418/-22.709361 ORX2 Peak/Mean Power Level (dBFS) : -inf/-inf ORX3 Peak/Mean Power Level (dBFS) : -10.748048/-21.656226 ORX4 Peak/Mean Power Level (dBFS) : -inf/-inf We can then conclude that our Antenna has been originally calibrated to have +1100 mdB on channel 1 and +800 mdB to obtain exactly 25 dBm Tx power on those chanels, so that we will then offset our 5000 dBm of extra attenuation and therefore the new attenuation levels are Tx1=16100+5000=21100 mdB and Tx2=15800+5000=20800mdB set attenuation for antenna 1 /usr/bin/radiocontrol -o A 21100 1 set attenuation for antenna 3 /usr/bin/radiocontrol -o A 20800 4 yes the 4 at the end seems to be correct Bear in mind these settings will stay as long as you don't reboot the Radio and default back to the original calibration values once you reboot the unit assess the new status of your radio: ~# radiocontrol -o G a Benetel radiocontrol Version : 0.9.0 Madura API Version : 5.1.0.21 Madura ARM FW version : 5.0.0.32 Madura ARM DPD FW version : 5.0.0.32 Madura Stream version : 8.0.0.5 Madura Product ID : 0x84 Madura Device Revision : 0xb0 Tx1 Attenuation (mdB) : 21100 Tx2 Attenuation (mdB) : 40000 Tx3 Attenuation (mdB) : 20800 Tx4 Attenuation (mdB) : 40000 PLL1 Frequency (Hz) : 0 PLL2 Frequency (Hz) : 3751680000 Front-end Control : 0x2aa491 Madura Deframer 0 : 0x87 Madura Framer 0 : 0xa Internal Temperature (degC) : 47 External Temperature (degC) : 42.789063 RX1 Power Level (dBFS) : -60.750000 RX2 Power Level (dBFS) : -60.750000 RX3 Power Level (dBFS) : -60.750000 RX4 Power Level (dBFS) : -60.750000 ORX1 Peak/Mean Power Level (dBFS) : -10.839418/-22.709361 ORX2 Peak/Mean Power Level (dBFS) : -inf/-inf ORX3 Peak/Mean Power Level (dBFS) : -10.748048/-21.656226 ORX4 Peak/Mean Power Level (dBFS) : -inf/-inf Configure the physical Benetel Radio End - Older then Release V0.7.0 \u00b6 auto reset dpd \u00b6 For releases older then V0.7.0 the dpd has to get reset every 30 minutes. This is not yet built inside and has to get created manually. These 3 steps need to be done. create these 2 files by copy/past the below cat <<EOF > /lib/systemd/system/dpd_reset.service [Unit] Description=Start DPD reset every 30 mins After=eth0ipset.service [Service] Type=forking ExecStart=/bin/sh /usr/sbin/dpd_reset.sh [Install] WantedBy=multi-user.target EOF cat <<EOF > /usr/sbin/dpd_reset.sh #! /bin/sh while true do sleep 1800 date '+%Y-%m-%d %H:%M:%S ##########' cd /home/root; radiocontrol -o D r 15 1 done >> /tmp/dpd_reset_status & EOF enable the service that just has been defined. systemctl enable dpd_reset.service and start the service systemctl start dpd_reset.service Starting RU Benetel 650 - manual way \u00b6 prepare cell \u00b6 When the CELL is OFF this traffic can be in this state shown below. ifstat -i $SERVER_RU_INT enp1s0f0 KB/s in KB/s out 71308.34 0.0 71318.21 0.0 In this case execute $ ssh root@10.10.0.100 handshake After execution you will have ifstat -i $SERVER_RU_INT enp1s0f0 KB/s in KB/s out 0.0 0.0 0.0 0.0 In this traffic state the dell is ready to start. start cell \u00b6 Bring the components up with docker compose cd ~/install- $DU_VERSION / docker-compose up -f docker-compose-B650.yml If all goes well this will produce output similar to: Starting phluido_l1 ... done Recreating accelleran-du-phluido-2022-01-31_du_1 ... done Attaching to phluido_l1, accelleran-du-phluido-2022-01-31_du_1 phluido_l1 | Reading configuration from config file \"/config.cfg\"... phluido_l1 | ******************************************************************************************************* phluido_l1 | * * phluido_l1 | * Phluido 5G-NR virtualized L1 implementation * phluido_l1 | * * phluido_l1 | * Copyright (c) 2014-2020 Phluido Inc. * phluido_l1 | * All rights reserved. * phluido_l1 | * * phluido_l1 | * The User shall not, and shall not permit others to: * phluido_l1 | * - integrate Phluido Software within its own products; * phluido_l1 | * - mass produce products that are designed, developed or derived from Phluido Software; * phluido_l1 | * - sell products which use Phluido Software; * phluido_l1 | * - modify, correct, adapt, translate, enhance or otherwise prepare derivative works or * phluido_l1 | * improvements to Phluido Software; * phluido_l1 | * - rent, lease, lend, sell, sublicense, assign, distribute, publish, transfer or otherwise * phluido_l1 | * make available the PHLUIDO Solution or any portion thereof to any third party; * phluido_l1 | * - reverse engineer, disassemble and/or decompile Phluido Software. * phluido_l1 | * * phluido_l1 | * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, * phluido_l1 | * INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A * phluido_l1 | * PARTICULAR PURPOSE ARE DISCLAIMED. * phluido_l1 | * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, * phluido_l1 | * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF * phluido_l1 | * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) * phluido_l1 | * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR * phluido_l1 | * TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS * phluido_l1 | * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. * phluido_l1 | * * phluido_l1 | ******************************************************************************************************* phluido_l1 | phluido_l1 | Copyright information already accepted on 2020-11-27, 08:56:08. phluido_l1 | Starting Phluido 5G-NR L1 software... phluido_l1 | PHAPI version = 0.5 (12/10/2020) phluido_l1 | L1 SW version = 0.8.1 phluido_l1 | L1 SW internal rev = r3852 phluido_l1 | Parsed configuration parameters: phluido_l1 | LicenseKey = XXXX-XXXX-XXXX-XXXX-XXXX-XXXX-XXXX-XXXX phluido_l1 | maxNumPrachDetectionSymbols = 1 phluido_l1 | maxNumPdschLayers = 2 phluido_l1 | maxNumPuschLayers = 1 phluido_l1 | maxPuschModOrder = 6 phluido_l1 | Perform these steps to get a running active cell. * When the RU is still sending traffic use ssh root@10.10.0.100 handshake to stop this traffic. * Start L1 and DU (run docker-compose up inside the install directory ). * Use wireshark to follow the CPlane traffic, at this point following sequence: DU CU | F1SetupRequest---> | | <---F1SetupResponse | | | | <---GNBCUConfigurationUpdate | | | The L1 starts listening on ip:port 10.10.0.1:44000 After less than 30 seconds communication between rru and du starts. The related fiber port will report around 100 Mbytes/second of traffic in both directions DU CU | GNBCUConfigurationUpdateAck---> | | | NOTE : type ssh root@10.10.0.100 handshake again to stop the traffic. Make sure you stop the handshake explicitly at the end of your session else, even when stopping the DU/L1 manually, the RRU will keep the link alive and the next docker-compose up will find a cell busy transmitting on the fiber and the synchronization will not happen Starting RU Benetel 650 - cell wrapper way \u00b6 Install cell wrapper \u00b6 On the HOST \u00b6 To make the CU VM have access to the DU host ( bare metal server ) some privileges need to be given. printf \" $USER ALL=(ALL) NOPASSWD:ALL\\n\" | sudo tee /etc/sudoers.d/ $USER sudo usermod -aG sudo $USER On the CU VM \u00b6 Go to the VM. In the VM a cell wrapper will get installed that controls the DU and RU ( cell ). Going inside the CU VM. ssh $USER@$NODE_IP Add some prerequisites if it is necessary sudo apt update sudo apt install zip Create a public/private key pair and add it to kubernetes ssh-keygen -t ed25519 -f id_ed25519 -C cell-wrapper kubectl create secret generic cw-private --from-file = private = id_ed25519 kubectl create secret generic cw-public --from-file = public = id_ed25519.pub and copy the public key to the bare metal server ( DU host ) ssh-copy-id -i id_ed25519.pub ad@$SERVER_IP Create a .yaml file containing the configuration. Also fill in the values you have prepared on the first page of the install guide. It will install the cell-wrapper the will take care of the cell's health. In this configuration the cell-wrapper will reboot the RU every night at 2:00 AM. <reboot>true</reboot> mkdir -p ~/install_$INSTALL_VERSION/ cd !$ tee cw.yaml < <EOF global: instanceId: \"cw\" natsUrl: \"$NODE_IP\" natsPort: \"31100\" redisHostname: \"$NODE_IP\" redisPort: \"32220\" redis: backup: enabled: true deleteAfterDay: 7 jobs: deleteExistingData: true nats: enabled: false #jobs: # - name: reboot-ru-1 # schedule: \"0 2 * * *\" # rpc: | # <cell-wrapper xmlns= \"http://accelleran.com/ns/yang/accelleran-granny\" # xmlns:xc= \"urn:ietf:params:xml:ns:netconf:base:1.0\" xc:operation= \"replace\" > # <radio-unit xc:operation= \"replace\" > # <name> vi su-1 </name> # <reboot> true </reboot> # </radio-unit> # </cell-wrapper> netconf: netconfService: nodePort: 31832 configOnBoot: enabled: true deleteExistingConfig: true host: 'localhost' config: | <cell-wrapper xmlns= \"http://accelleran.com/ns/yang/accelleran-granny\" xmlns:xc= \"urn:ietf:params:xml:ns:netconf:base:1.0\" xc:operation= \"create\" > <admin-state> unlocked </admin-state> <ssh-key-pair xc:operation= \"create\" > <public-key> /home/accelleran/5G/ssh/public </public-key> <private-key> /home/accelleran/5G/ssh/private </private-key> </ssh-key-pair> <auto-repair xc:operation= \"create\" > <enable> true </enable> <health-check xc:operation= \"create\" > <rate xc:operation= \"create\" > <seconds> 5 </seconds> <milli-seconds> 0 </milli-seconds> </rate> <unacknowledged-counter-threshold> 3 </unacknowledged-counter-threshold> </health-check> <container-not-running-counter-threshold> 2 </container-not-running-counter-threshold> <l1-not-listening-to-ru-counter-threshold> 6 </l1-not-listening-to-ru-counter-threshold> <l1-rru-traffic-counter-threshold> 6 </l1-rru-traffic-counter-threshold> </auto-repair> <distributed-unit xc:operation= \"create\" > <name> du-1 </name> <type> effnet </type> <connection-details xc:operation= \"create\" > <host> $SERVER_IP </host> <port> 22 </port> <username> $USER </username> </connection-details> <ssh-timeout> 30 </ssh-timeout> <config xc:operation= \"create\" > <cgi-plmn-id> $PLMN_ID </cgi-plmn-id> <cgi-cell-id> 000000000000000000000000000000000001 </cgi-cell-id> <pci> $PCI_ID </pci> <tac> 000001 </tac> <arfcn> $ARFCN_POINT_A </arfcn> <frequency-band> $FREQ_BAND </frequency-band> <plmns-id> $PLMN_ID </plmns-id> <plmns-sst> 1 </plmns-sst> <l1-license-key> $L1_PHLUIDO_KEY </l1-license-key> <l1-bbu-addr> 10.10.0.1 </l1-bbu-addr> <l1-max-pusch-mod-order> 6 </l1-max-pusch-mod-order> <l1-max-num-pdsch-layers> 2 </l1-max-num-pdsch-layers> <l1-max-num-pusch-layers> 1 </l1-max-num-pusch-layers> <l1-num-workers> 8 </l1-num-workers> <l1-target-recv-delay-us> 2500 </l1-target-recv-delay-us> <l1-pucch-format0-threshold> 0.01 </l1-pucch-format0-threshold> <l1-timing-offset-threshold-nsec> 10000 </l1-timing-offset-threshold-nsec> </config> <enable-auto-repair> true </enable-auto-repair> <working-directory> /run </working-directory> <storage-directory> /var/log </storage-directory> <pcscd-socket> /run/pcscd/pcscd.comm </pcscd-socket> <enable-log-saving> false </enable-log-saving> <max-storage-disk-usage> 80% </max-storage-disk-usage> <enable-log-rotation> false </enable-log-rotation> <log-rotation-pattern> *.0 </log-rotation-pattern> <log-rotation-count> 1 </log-rotation-count> <centralized-unit-host> $F1_CU_IP </centralized-unit-host> <l1-listening-port> 44000 </l1-listening-port> <traffic-threshold xc:operation= \"create\" > <uplink> 10000 </uplink> <downlink> 10000 </downlink> </traffic-threshold> <du-image-tag> $DU_VERSION </du-image-tag> <l1-image-tag> $L1_VERSION </l1-image-tag> <du-extra-args> --cpuset-cpus=$CORE_SET_DU </du-extra-args> <l1-extra-args> --cpuset-cpus=$CORE_SET_DU </l1-extra-args> <du-base-config-file> /home/accelleran/5G/config/duEffnetConfig.json </du-base-config-file> <radio-unit xc:operation= \"create\" > ru-1 </radio-unit> </distributed-unit> <radio-unit xc:operation= \"create\" > <name> ru-1 </name> <type> benetel650 </type> <connection-details xc:operation= \"create\" > <host> 10.10.0.100 </host> <port> 22 </port> <username> root </username> </connection-details> <enable-ssh> false </enable-ssh> <ssh-timeout> 30 </ssh-timeout> </radio-unit> </cell-wrapper> EOF NOTE : uncomment the jobs: part if the RU should restart every night at 2am. Install using helm. helm repo update helm install cw acc-helm/cw-cell-wrapper --values cw.yaml Now you can see the kubernetes pods being created. Follow there progress with. watch -d kubectl get pod scripts to steer cell and cell-wrapper \u00b6 Following script are delivered. They are located in the install_$CU_VERSION/accelleran/bin directory. The $PATH variable is set accordingly. cw-verify.sh - verifies if the cw.yaml file is parsed correctly after installation cw-enable.sh - will enable the cell-wrapper. cell-start.sh cell-stop.sh cell-restart.sh cw-disable.sh - cell-wrapper will not restar the cell when it is defect. cw-debug-on.sh - turns on more logging cw-debug-off.sh - turns on normal logging The script do what there name says verify good operation of the B650 (all releases) \u00b6 GPS \u00b6 See if GPS is locked root@benetelru:~# syncmon DPLL0 State (SyncE/Ethernet clock): LOCKED DPLL1 State (FPGA clocks): FREERUN DPLL2 State (FPGA clocks): FREERUN DPLL3 State (RF/PTP clock): LOCKED CLK0 SyncE LIVE: OK CLK0 SyncE STICKY: LOS + No Activity CLK2 10MHz LIVE: LOS + No Activity CLK2 10MHz STICKY: LOS + No Activity CLK5 GPS LIVE: OK CLK5 GPS STICKY: LOS and Frequency Offset CLK6 EXT 1PPS LIVE: LOS and Frequency Offset CLK6 EXT 1PPS STICKY: LOS and Frequency Offset Cell Status Report \u00b6 Verify if the boot sequence ended up correctly, by checking the radio status, the ouput shall mention explicitly the up time and the succesful bringup > NOTE : this file is not present the first minute after reboot. root@benetelru:~# cat /tmp/radio_status [INFO] Platform: RAN650_B [INFO] Radio bringup begin [INFO] Load EEPROM Data [INFO] Tx1 Attenuation set to 15000 mdB [INFO] Tx3 Attenuation set to 15730 mdB [INFO] Operating Frequency set to 3774.720 MHz [INFO] Waiting for Sync [INFO] Sync completed [INFO] Start Radio Configuration [INFO] Initialize RF IC [INFO] Disabled CFR for Antenna 1 [INFO] Disabled CFR for Antenna 3 [INFO] Move platform to TDD mode [INFO] Set CP60 as TDD control master [INFO] Enable TX on FEM [INFO] FEM to full MIMO1_3 mode [INFO] DPD Tx1 configuration [INFO] DPD Tx3 configuration [INFO] Set attn at 3774.720 MHz [INFO] Reg 0xC0366 to 0x3FF [INFO] Tuning the UE TA to reduce timing_offset [INFO] The O-RU is ready for System Integration [INFO] Radio bringup complete 15:54:47 up 4 min, load average: 0.09, 0.19, 0.08 ``` ### RU Status Report some important registers must be checked to determine if the boot sequence has completed correctly: ```bash root@benetelru:~# reportRuStatus [INFO] Sync status is: Register 0xc0367, Value : 0x1 ------------------------------- [INFO] RU Status information is: Register 0xc0306, Value : 0x470800 ------------------------------- [INFO] Fill level of Reception Window is: Register 0xc0308, Value : 0x6c12 ------------------------------- [INFO] Sample Count is: Register 0xc0311, Value : 0x56f49 ------------------------------- ============================================================ RU Status Register description: ============================================================ [31:19] not used [18] set to 1 if handshake is successful [17] set to 1 when settling time (fronthaul) has completed [16] set to 1 if symbolndex=0 was captured [15] set to 1 if payload format is invalid [14] set to 1 if symbol index error has been detected [13:12] not used [11] set to 1 if DU MAC address is correct [10:2] not used [1] Reception Window Buffer is empty [0] Reception Window Buffer is full ------------------------------------------------------------ =========================================================== [NOTE] Max buffer depth is 53424 (112 symbols, 2 antennas) =========================================================== Handshake messages are sent by the RU every second. When phluido L1 is starting or running it will Listen on port 44000 and reply to these messages. Login to the server and check if the handshakes are happening: these are short messages sent periodically from the B650 to the server DU MAC address that was set as discussed and can be seen with a simple tcp dump command on the fiber interface of your server (enp45s0f0 for this example): tcpdump -i enp45s0f0 -c 5 port 44000 -en 19:22:47.096453 02:00:5e:01:01:01 > 6c:b3:11:08:a4:e0, ethertype IPv4 (0x0800), length 64: 10.10.0.2.44000 > 10.10.0.1.44000: UDP, length 20 The above shows that 10.10.0.2 (U plane default IP address of the B650 Cell) is sending a Handshake message from the MAC address 02:00:5e:01:01:01 (default MAC address of the B650 Uplane interface) to 10.10.0.1 (Server Fiber interface IP address) on MAC 6c:b3:11:08:a4:e0 (the MAC address of that fiber interface) Such initial message may repeat a certain number of times, this is normal. Trace traffic between RU and L1. \u00b6 As said, the first packet goes out from the Radio End to the DU, this is the handshake packet. The second packet is the Handshake response of the DU and we have to make sure that as described the MAC address used in such response from the DU has been set correctly so that the DATA Interface MAC address of the Radio End is used (by default in the Benetel Radio this MAC address is 02:00:5e:01:01:01 ) When data flows the udp packet lengths are 3874. Remember we increased the MTU size to 9000. Without increasing the L1 would crash on the fragmented udp packets. $ tcpdump -i enp45s0f0 -c 20 port 44000 -en tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on enp45s0f0, link-type EN10MB (Ethernet), capture size 262144 bytes 19:22:47.096453 02:00:5e:01:01:01 > 6c:b3:11:08:a4:e0, ethertype IPv4 (0x0800), length 64: 10.10.0.2.44000 > 10.10.0.1.44000: UDP, length 20 19:22:47.106677 6c:b3:11:08:a4:e0 > 02:00:5e:01:01:01, ethertype IPv4 (0x0800), length 54: 10.10.0.1.44000 > 10.10.0.2.44000: UDP, length 12 19:23:14.596247 02:00:5e:01:01:01 > 6c:b3:11:08:a4:e0, ethertype IPv4 (0x0800), length 64: 10.10.0.2.44000 > 10.10.0.1.44000: UDP, length 12 19:23:14.596621 6c:b3:11:08:a4:e0 > 02:00:5e:01:01:01, ethertype IPv4 (0x0800), length 3874: 10.10.0.1.44000 > 10.10.0.2.44000: UDP, length 3832 19:23:14.596631 6c:b3:11:08:a4:e0 > 02:00:5e:01:01:01, ethertype IPv4 (0x0800), length 3874: 10.10.0.1.44000 > 10.10.0.2.44000: UDP, length 3832 Cell is ON \u00b6 Check if the L1 is listening \u00b6 $ while true ; do sleep 1 ; netstat -ano | grep 44000 ;echo $RANDOM; done udp 0 118272 10.10.0.1:44000 0.0.0.0:* off (0.00/0/0) 1427 udp 0 16896 10.10.0.1:44000 0.0.0.0:* off (0.00/0/0) 11962 udp 0 42240 10.10.0.1:44000 0.0.0.0:* off (0.00/0/0) 16780 udp 0 0 10.10.0.1:44000 0.0.0.0:* off (0.00/0/0) 502 Show the traffic between rru and l1 \u00b6 $ ifstat -i enp45s0f0 enp45s0f0 KB/s in KB/s out 71320.01 105959.7 71313.36 105930.1 Troubleshooting \u00b6 DEBUG Configuration \u00b6 To enable the L1 and DU logs to get saved after a cellwrapper intervention, install the cellwrapper with these settings. <enable-log-saving>true</enable-log-saving> Fiber Port not showing up \u00b6 https://www.serveradminz.com/blog/unsupported-sfp-linux/ L1 is not listening \u00b6 Check if L1 is listening on port 44000 by typing $ netstat -ano | grep 44000 If nothing is shown L1 is not listening. In this case do a trace on the F1 port like this. tcpdump -i any port 38472 18:26:30.940491 IP 10.244.0.208.38472 > bare-metal-node-cab-3.59910: sctp (1) [HB REQ] 18:26:30.940491 IP 10.244.0.208.38472 > bare-metal-node-cab-3.maas.56153: sctp (1) [HB REQ] 18:26:30.940530 IP bare-metal-node-cab-3.59910 > 10.244.0.208.38472: sctp (1) [HB ACK] 18:26:30.940532 IP bare-metal-node-cab-3.59910 > 10.244.0.208.38472: sctp (1) [HB ACK] you should see the HB REQ and ACK messages. If not Check * the docker-compose.yml file if the cu ip address matches the following bullet * check kubectl get services if the F1 service is running with the that maches previous bullet check SCTP connections \u00b6 There are 3 UDP ports you can check. When the system starts up it will setup 3 SCTP connections on following ports in the order mentioned here : 38462 - E1 SCTP connection - SCTP between DU and CU 38472 - F1 SCTP connection - SCTP between CU UP and CU CP 38412 - NGAP SCTP connection - SCTP between CU CP and CORE Appendix: Engineering tips and tricks \u00b6 pcscd debug \u00b6 It occurs rarely that the du software throws DU license check failed when this happens you have to recreate the docker container and try again. If this does not help increase the pcscd logging. change ENTRYPOINT [\"/usr/sbin/pcscd\", \"--foreground\"] into ENTRYPOINT [\"/usr/sbin/pcscd\", \"-d --foreground\"] and use docker logs on the container to see more logging about what pcscd is doing Run RU in freerun mode \u00b6 This is the mode where it does not need a GPS sync. By default a benetel only boots when a GPS signal is present which the RU can be synced with. The boot process indicated this with ``Waiting for Sync in the /tmp/logs/radio_status``` file The following steps make the benetel boot without needing GPS signal. At boottime you kill the syncmon process killall syncmon and set the sync-state to succesfull manually echo 0 > /var/syncmon/sync-state Now the boot process will continue. Wait at least a minute. custatus \u00b6 install \u00b6 unzip custatus.zip so you get create a directory $HOME/5g-engineering/utilities/custatus sudo apt install tmux create the .tmux.conf file with following content. cat $HOME/.tmux.conf set -g mouse on bind q killw add this line in $HOME/.profile export PATH=$HOME/5g-engineering/utilities/custatus:$PATH use \u00b6 to start custatus.sh tmux to quit * type \"CTRL-b\" followed by \"q\" NOTE : you might need to quit the first time you have started. Start a second time and see the difference. example \u00b6","title":"DU Installation"},{"location":"du-install/#du-installation","text":"","title":"DU Installation"},{"location":"du-install/#introduction","text":"The DU will be installed in several Docker containers that run on metal on the host machine. As mentioned in the introduction, a separate Virtual Machine will host the RIC and the CU and their relative pods will be handled by Kubernetes inside that VM. Here we focus on the steps to get DU and L1 up and running.","title":"Introduction"},{"location":"du-install/#variables-needed-for-this-install","text":"Before proceeding you may want to crosscheck and modify some paramters that caracterise each deployment and depends on the desired provisioning of the components. The parameters that should be considered for this purpose and can be safely modified are:","title":"Variables needed for this install"},{"location":"du-install/#5g-variables","text":"plmn_identity [ eg 235 88 ] nr_cell_identity [ eg 1 any number ] nr_pci [ eg 1 not any number. Ask Accelleran to do the PCI planning ] 5gs_tac [ eg 1 ]","title":"5G variables"},{"location":"du-install/#frequency-variables","text":"center_frequency_band [ eg 3751.680 ] point_a_arfcn [ 648840 consistent with center freq, scs 30khz ] band [ 77 consistent with center frequency ]","title":"Frequency variables"},{"location":"du-install/#licenses-and-files-needed-see-intro","text":"accelleran-du-phluido-%Y-%m-%d-release.zip phluido_docker_xxxx.tar effnet-license-activation-%Y-%m-%d.zip 32 digit phluido license key, [ ex 2B2A-962F-783F-40B9-7064-2DE3-3906-9D2E ] For any other modification it is advisable to make contact with the Accelleran service desk as of course, if in principle every paramter in the confuguration file is up for modification, it is certainly not recommendable to proceed in that declaration as each front end may or may not work as a consequence and the analysis and recovery from error scenario will be less than intuitive","title":"licenses and files needed (see intro)"},{"location":"du-install/#docker-installation-on-the-server","text":"Make sure Docker and docker-compose have been installed and that docker can be run without superuser privileges, this is a prerequisite. DO NOT install Kubernetes where DU and L1 will run Add the Docker APT repository: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt update Install the required packages: sudo apt install docker-ce sudo apt install docker-ce-cli sudo apt install containerd.io sudo apt install docker-compose Add your user to the docker group to be able to run docker commands without sudo access. You might have to reboot or log out and in again for this change to take effect. sudo usermod -aG docker $USER To check if your installation is working you can try to run a test image in a container: sudo docker run hello-world Restart Docker and enable on boot: sudo systemctl enable docker sudo systemctl daemon-reload sudo systemctl restart docker","title":"Docker installation on the Server"},{"location":"du-install/#obtain-the-effnet-and-phluido-licenses","text":"","title":"Obtain the Effnet and Phluido licenses"},{"location":"du-install/#preparation-steps","text":"In this phase we will need to act in parallel for the DU and the L1/RRU licenses, which depend on our partner company so it is essential to give priority and possibly anticipate these two steps as there is no specific effort involved from the user/customer perspective and they may require longer than one working day before we can proceed further.","title":"Preparation steps"},{"location":"du-install/#phluido-install-a-low-latency-kernel","text":"The PHY layer has very stringent latency requirements, therefore we install a low latency kernel: sudo apt install linux-image-lowlatency make sure this line is present in the /etc/default/grub file. It forces the GRUB to start a lowlatency kernel. $ cat /etc/default/grub | grep FLAV GRUB_FLAVOUR_ORDER=lowlatency Create a sysctl configuration file to configure the low latency kernel: sudo tee /etc/sysctl.d/10-phluido.conf <<EOF # Improves scheduling responsiveness for Phluido L1 kernel.sched_min_granularity_ns = 100000 kernel.sched_wakeup_granularity_ns = 20000 kernel.sched_latency_ns = 500000 kernel.sched_rt_runtime_us = -1 # Message queue fs.mqueue.msg_max = 64 EOF Remove the generic kernel to avoid the low latency kernel to be replaced by a generic kernel when updates are performed: sudo apt remove linux-image-generic sudo apt autoremove In order to avoid possible system performance degradation, CPU scaling must be disabled: sudo apt install cpufrequtils echo 'GOVERNOR=\"performance\"' | sudo tee /etc/default/cpufrequtils sudo systemctl disable ondemand Restart the machine to make the changes take effect: sudo reboot","title":"Phluido : Install a Low Latency Kernel"},{"location":"du-install/#phluido-license-run-the-systest-utility-from-phluido","text":"to go to the directory where the Phluido sysTest utility is : $ ./sysTest Running system test... 01234567890123456789012345678901 System test completed, output written to file \"sysTest.bin\". ( The test takes around 90 seconds) This will run a test of the system that will allow to determine if the server is properly configured and capable of running the demanding L1/RRU components Once it is finsihed it produces a file sysTest.bin in the same directory Send this file to Accelleran, to obtain the Phluido license key. Send this .bin file to phluido to receive a proper license.","title":"Phluido License: Run the sysTest utility from Phluido"},{"location":"du-install/#effnet-license-install-and-check-your-yubikey","text":"For the license activation file we indicate the generic format yyyy_mm_dd as the file name may vary from case to case, your Accelleran point of contact will make sure you receive the correct license activation archive file which will have a certain timestamp on it, example effnet-license-activation-2021-12-16.zip if you don't have yet the effnet license activation bundle, in order to obatin one you must comunicate to Accelleran the serial number of the Yubikey you intend to use so to be enabled for usage. You can obtain this information by using the following command on your server where the Yubikey has been installed physically to a USB port: To check if the server can see the key do (in this example Device004 is your key): lsusb ~$ lsusb Bus 002 Device 002 : ID 8087 :8002 Intel Corp. Bus 002 Device 001 : ID 1d6b:0002 Linux Foundation 2 .0 root hub Bus 001 Device 002 : ID 8087 :800a Intel Corp. Bus 001 Device 001 : ID 1d6b:0002 Linux Foundation 2 .0 root hub Bus 004 Device 003 : ID 2500 :0020 Ettus Research LLC USRP B200 Bus 004 Device 001 : ID 1d6b:0003 Linux Foundation 3 .0 root hub **Bus 003 Device 004 : ID 1050 :0407 Yubico.com Yubikey 4 OTP+U2F+CCID** Bus 003 Device 029 : ID 2a70:9024 OnePlus AC2003 Bus 003 Device 006 : ID 413c:a001 Dell Computer Corp. Hub Bus 003 Device 016 : ID 20ce:0023 Minicircuits Mini-Circuits Bus 003 Device 001 : ID 1d6b:0002 Linux Foundation 2 .0 root hub Then you can find the serial number (yubikey-manager needed, install if it's not already): sudo apt install yubikey-manager ykman list --serials #13134288","title":"Effnet License: Install and check your Yubikey"},{"location":"du-install/#effnet-license-create-a-pcscd-docker-image","text":"The DU software needs access to a YubiKey that contains its license. The license in the YubiKey is shared by the PCSCD daemon, which itself can run in a Docker container to satisfy OS dependencies. Plug the YubiKey in a USB port of the machine. Then, create a Dockerfile named Dockerfile.pcscd for this Docker image: mkdir -p pcscd tee pcscd/Dockerfile.pcscd <<EOF FROM ubuntu:20.04 RUN \\ set -xe && \\ apt-get update && \\ DEBIAN_FRONTEND=\"noninteractive\" apt-get install -y \\ pcscd # Cleanup RUN \\ set -xe && \\ apt-get clean && \\ rm -rf \\ /var/lib/apt/lists/* \\ /var/tmp/* \\ /tmp/* ENTRYPOINT [\"/usr/sbin/pcscd\", \"--foreground\"] EOF The Docker image can now be built and started with: docker build --rm -t pcscd_yubikey - <pcscd/Dockerfile.pcscd docker run --restart always -id --privileged --name pcscd_yubikey_c -v /run/pcscd:/run/pcscd pcscd_yubikey You can verify it is running correct with the command: docker container ls --filter name = pcscd_yubikey_c If every step was performed correctly this command should produce output similar to: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES df4f41eb70c9 pcscd_yubikey \"/usr/sbin/pcscd --f\u2026\" 1 minute ago Up 1 minute pcscd_yubikey_c","title":"Effnet License: Create a PCSCD Docker Image"},{"location":"du-install/#effnet-license-activate-the-yubikey","text":"In order to activate the license dongles unzip the received license activation bundle effnet-license-activation-yyyy-mm-dd.zip (as mentioned the date may differ on each case so let's use the generic format) and then you need to load the included Docker image into your docker-daemon, i.e. bunzip2 --stdout license-activation-yyyy-mm-dd.tar.bz2 | docker load Then run the image mapping the folder containing the pcscd daemon socket into the container: docker run -it -v /var/run/pcscd:/var/run/pcscd effnet/license-activation-yyyy-mm-dd If you get warnings similar to: WARNING: No dongle with serial-number 13134288 found It means that a dongle for the bundled license was not found, i.e. in this case the dongle with the serial number 13134288 has not been activated, or the licens bundle file you have received is not the correct one, contact Accelleran in such case Successful activation of a license-dongle should produce an output similar to: Loading certificate to Yubico YubiKey CCID 00 00 ( serial: 13134288 ) Which means that a license for the dongle with serial-number 13134288 was loaded to the dongle (i.e., it was bundled in the license-activation image).","title":"Effnet License: activate the yubikey"},{"location":"du-install/#install-the-phluido-l1-docker","text":"docker image load -i phluido_docker_ $L1_VERSION .tar","title":"Install the Phluido L1 ( docker )"},{"location":"du-install/#install-effnet-du-docker","text":"unzip accelleran-du-phluido- $DU_VERSION .zip bzcat accelleran-du-phluido/accelleran-du-phluido- $DU_VERSION /gnb_du_main_phluido- $DU_VERSION .tar.bz2 | docker image load","title":"Install Effnet DU ( docker )"},{"location":"du-install/#docker-compose-file-with-cpu-pinning","text":"To achieve maximum stability and performance it is necessary to optimise the CPU load and this can be done by distributing the available CPUs among the components and assign different priorities to the most demanding processes. We split therefore the CPUs in two groups, one group of CPUs for the VM where the RIC/CU run and one group of CPUs for the containers that run L1 and DU. The CPU pinning allows for ceertain components to run only on certain CPUs, however it doesn't impede other processes to use the same CPUs, so the full optimisation of the CPU usage and the exclusive allocation of the CPUs are beyond the scope of this document, here we illustrate one possible split as an example. First thing to find out is what resources are available on our system: These have been found during the preperation fase and shared over DU and CU. In this chapter the DU core are assigned. In this specific example, there are two banks of 4 cores, each capable of hyperthreading, so in total we can count on 16 CPUs, let's then set 8 CPUs aside to run the VM for kubernetes and CU, and the other 8 CPUs to run L1/L2 so that they never compete for the same CPU. The assumption is that the rest of the processes on the system (very light load) is equally spread over all cores. If a large number of cores is available, probably the core with a higher number will be mostly free and can be then dedicated to L1 and DU, as mentioned there is no specific rule. For the sake of argument let's assign the even cores to the L1 and DU equally, so the Docker compose looks as follows: mkdir -p ~/install- $DU_VERSION / cd !$ cat > docker-compose.yaml << EOF version: \"2\" services: phluido_l1: image: phluido_l1:$L1_VERSION container_name: phluido_l1 tty: true privileged: true ipc: shareable shm_size: 2gb command: /config.cfg volumes: - \"$PWD/l1-config-0.cfg:/config.cfg:ro\" - \"$PWD/logs-0/l1:/workdir\" - \"/etc/machine-id:/etc/machine-id:ro\" working_dir: \"/workdir\" network_mode: host cpuset: \"$CORE_SET_DU\" du: image: gnb_du_main_phluido:$DU_VERSION volumes: - \"$PWD/du-config-0.json:/config.json:ro\" - \"$PWD/logs-0/du:/workdir\" - /run/pcscd/pcscd.comm:/run/pcscd/pcscd.comm ipc: container:phluido_l1 tty: true privileged: true depends_on: - phluido_l1 entrypoint: [\"/bin/sh\", \"-c\", \"sleep 2 && exec /gnb_du_main_phluido /config.json\"] working_dir: \"/workdir\" extra_hosts: - \"cu:$F1_CU_IP\" - \"du:$SERVER_IP\" network_mode: host cpuset: \"$CORE_SET_DU\" EOF Notes: 1) the version has to be changed to '2' as version 3 does not support cpuset option 2) the ip address for the cu is the one of the f1 external ip interface of the relative cucp service 3) the DU ip address is the one of the server where the DU runs NOTE : verify the cpu pinning of the VM's are different then those we used in above compose file. We then need to take care of the CPUs that the VMs hosted on this server are intended to use. Earlier in the installation the CPU pinning has been done during creation. : virsh edit $CU_VM_NAME virsh edit $OPEN5GS_VM_NAME Other ways of creating a VM may not produce a configuration file in xml format, making things more difficult. We also recommend to identify the xml configuration file by searching the directory: /etc/libvirt/qemu/ But we definitely discourage the direct editing of such file as it will reset to default at the first reboot Once done, you can check the content of the xml configuration file, that in this case will show we decided to assign the odd CPUs to the VM: ubuntu@bbu3:~$ sudo cat /etc/libvirt/qemu/Ubuntu123.xml <!-- WARNING: THIS IS AN AUTO-GENERATED FILE. CHANGES TO IT ARE LIKELY TO BE OVERWRITTEN AND LOST. Changes to this xml configuration should be made using: virsh edit Ubuntu123 or other application using the libvirt API. --> <domain type = 'kvm' > <name>Ubuntu123</name> <uuid>f18a2a01-7b67-4f00-ad11-5920ec2b6f16</uuid> <metadata> <libosinfo:libosinfo xmlns:libosinfo = \"http://libosinfo.org/xmlns/libvirt/domain/1.0\" > <libosinfo:os id = \"http://ubuntu.com/ubuntu/20.04\" /> </libosinfo:libosinfo> </metadata> <memory unit = 'KiB' >33554432</memory> <currentMemory unit = 'KiB' >33554432</currentMemory> <vcpu placement = 'static' cpuset = '1,3,5,7,9,11,13,15' >8</vcpu> <os> < type arch = 'x86_64' machine = 'pc-q35-4.2' >hvm</type> <bootmenu enable = 'yes' /> </os> <features>","title":"docker compose file ( with CPU PINNING )"},{"location":"du-install/#set-softirq-priorities-to-realtime","text":"In a normal setup, the softirq processes will run at priority 20, equal to all user processes. Here they need to run at -2, which corresponds to real time priority. They are scheduled on all cores but will get strict priority over any other user processes. To adapt the priority of the ksoft, you can use spcific commands: to set to realtime priority 1 (lowest prio, but still \"run to completion\" before other default processes are executed): ps -A | grep ksoftirq | awk '{print $1}' | xargs -L1 sudo chrt -p 1 NOTE: to revert the priority to \"other policy\": ps -A | grep ksoftirq | awk '{print $1}' | xargs -L1 sudo chrt --other -p 0 finally to check all the priorities set: ps -A | grep ksoftirq | awk '{print $1}' | xargs -L1 chrt -p Use htop to verify the priorities of the softirq processes. The only thing remaining is now prioritise the softirq processes . One can use htop and work out the options to show priority and CPU ID * Press F2 for ```Setup```, navigate to ```Columns```, add ```PRIORITY``` * Press F2 for ```Setup```, navigate to ```Display Options```, unselect ```Hide kernel threads```","title":"set softirq priorities to realtime"},{"location":"du-install/#for-b210-ru-only-install-phluido-rru-docker","text":"Load the Phluido RRU Docker image (this step does not have to be taken when using Benetel RUs): docker build -f accelleran-du-phluido/accelleran-du-phluido-2022-01-31/phluido/docker/Dockerfile.rru -t phluido_rru:v0.8.1 Phluido5GL1/Phluido5GL1_v0.8.1","title":"FOR B210 RU ONLY Install Phluido RRU ( docker )"},{"location":"du-install/#prepare-and-bring-on-air-the-usrp-b210-radio","text":"This section is exclusively applicable to the user/customer that intends to use the Ettus USRP B210 Radio End with our Accellleran 5G end to end solution, if you do not have such radio end the informations included in this section may be misleading and bring to undefined error scenarios. Please contact Accelleran if your Radio End is not included in any section of this user guide Create the UDEV rules for the B210: sudo tee /etc/udev/rules.d/uhd-usrp.rules <<EOF # # Copyright 2011,2015 Ettus Research LLC # Copyright 2018 Ettus Research, a National Instruments Company # # SPDX-License-Identifier: GPL-3.0-or-later # #USRP1 SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"fffe\", ATTRS{idProduct}==\"0002\", MODE:=\"0666\" #B100 SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"2500\", ATTRS{idProduct}==\"0002\", MODE:=\"0666\" #B200 SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"2500\", ATTRS{idProduct}==\"0020\", MODE:=\"0666\" SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"2500\", ATTRS{idProduct}==\"0021\", MODE:=\"0666\" SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"2500\", ATTRS{idProduct}==\"0022\", MODE:=\"0666\" SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"3923\", ATTRS{idProduct}==\"7813\", MODE:=\"0666\" SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"3923\", ATTRS{idProduct}==\"7814\", MODE:=\"0666\" EOF Connect the B210 to the machine. Make sure it is enumerated as USB3 by executing: lsusb -d 2500 :0020 -v | grep -F bcdUSB This should print: bcdUSB 3.00 Add the Ettus Research APT repositories: sudo add-apt-repository ppa:ettusresearch/uhd sudo apt update Install the software required by the B210: sudo apt install libuhd-dev uhd-host libuhd3.15.0 Download the UHD images: sudo uhd_images_downloader sudo /usr/lib/uhd/utils/uhd_images_downloader.py Check if the B210 is detecting using the following command: uhd_find_devices This should output something similar to: [INFO] [UHD] linux; GNU C++ version 7.5.0; Boost_106501; UHD_3.15.0.0-release -------------------------------------------------- -- UHD Device 0 -------------------------------------------------- Device Address: serial: 3218C86 name: MyB210 product: B210 type: b200 Burn the correct EEPROM for the B210: /usr/lib/uhd/utils/usrp_burn_mb_eeprom* --values = 'name=B210-#4' If everything goes well this should output something similar to: Creating USRP device from address: [INFO] [UHD] linux; GNU C++ version 7.5.0; Boost_106501; UHD_3.15.0.0-release [INFO] [B200] Detected Device: B210 [INFO] [B200] Loading FPGA image: /usr/share/uhd/images/usrp_b210_fpga.bin... [INFO] [B200] Operating over USB 3. [INFO] [B200] Detecting internal GPSDO.... [INFO] [GPS] No GPSDO found [INFO] [B200] Initialize CODEC control... [INFO] [B200] Initialize Radio control... [INFO] [B200] Performing register loopback test... [INFO] [B200] Register loopback test passed [INFO] [B200] Performing register loopback test... [INFO] [B200] Register loopback test passed [INFO] [B200] Setting master clock rate selection to 'automatic'. [INFO] [B200] Asking for clock rate 16.000000 MHz... [INFO] [B200] Actually got clock rate 16.000000 MHz. Fetching current settings from EEPROM... EEPROM [\"name\"] is \"MyB210\" Setting EEPROM [\"name\"] to \"B210-#4\"... Power-cycle the USRP device for the changes to take effect. Done Check if the current EEPROM was flashed by executing: uhd_find_devices The output should look like: [INFO] [UHD] linux; GNU C++ version 7.5.0; Boost_106501; UHD_3.15.0.0-release -------------------------------------------------- -- UHD Device 0 -------------------------------------------------- Device Address: serial: 3218C86 name: B210-#4 product: B210 type: b200","title":"Prepare and bring on air the USRP B210 Radio"},{"location":"du-install/#dul1rru-configuration-and-docker-compose","text":"Before starting the configuration of the components it is important to avoid confusion so please create a folder file and move in all the configuration files you find for the L1, RRU and the DU configuration and remove the docker-compose as well: mkdir accelleran-du-phluido/accelleran-du-phluido-2022-01-31/phluido/cfg mv accelleran-du-phluido/accelleran-du-phluido-2022-01-31/phluido/*.cfg accelleran-du-phluido/accelleran-du-phluido-2022-01-31/phluido/cfg/ mkdir accelleran-du-phluido/accelleran-du-phluido-2022-01-31/json mv accelleran-du-phluido/accelleran-du-phluido-2022-01-31/*.json accelleran-du-phluido/accelleran-du-phluido-2022-01-31/json/ rm accelleran-du-phluido/accelleran-du-phluido-2022-01-31/docker-compose.yml Create a configuration file for the Phluido RRU: only for B210 tee accelleran-du-phluido/accelleran-du-phluido-2022-01-31/phluido/PhluidoRRU_NR_EffnetTDD_B210.cfg <<EOF /****************************************************************** * * This file is subject to the terms and conditions defined in * file 'LICENSE.txt', which is part of this source code package. * ******************************************************************/ //BBU IPv4 address. bbuFronthaulServerAddr = \"127.0.0.1\"; // Number of slots in one subframe. numSubframeSlots = 2; //Number of TX antennas for fronthaul data exchange. numTxAntennas = 1; //Number of RX antennas for fronthaul data exchange. numRxAntennas = 1; /// Frequency [kHz] for TX \"point A\" (see NR definition). txFreqPointA_kHz = 3301680; /// Frequency [kHz] for RX \"point A\" (see NR definition). rxFreqPointA_kHz = 3301680; /// Number of PRBs for both downlink and uplink. Must match the L2-L3 configuration. numPrbs = 51; uhdClockMode = 0; uhdSendAdvance_ns = 8000; /// Parameters used for PhluidoPrototypeBPP. bppMode = 10; uhdSamplingRate_kHz = 23040; EOF Create a configuration file for the Phluido L1. Make sure to set the value LicenseKey option to the received Phluido license key. This key has been delivered by Phluido upon receipt of the .bin file generated by the sysTest you have performed at start of this installation. tee l1-config.cfg <<EOF /****************************************************************** * * This file is subject to the terms and conditions defined in * file 'LICENSE'. * ******************************************************************/ // Enables verbose binary logging. WARNING: very CPU intensive and creates potentially huge output files. Use with caution. //logLevel_verbose = \"DEBUG\"; bbuFronthaulServerMode = 1; bbuFronthaulServerAddr = \"10.10.0.1\" bbuFronthaulServerPort = 44000; /// BBU fronthaul server \"busy poll\" for the receive socket, in microseconds, used as value for the relevant (SOL_SOCKET,SO_BUSY_POLL) socket option. numWorkers = 4; // Enable 64-QAM support for PUSCH (license-specific) maxPuschModOrder = 6; maxNumPdschLayers = 2; maxNumPuschLayers = 1; maxNumPrachDetectionSymbols = 1; targetRecvDelay_us = 2500; //targetCirPosition = 0.0078125; // License key //LicenseKey = \"2B2A-962F-783F-40B9-7064-2DE3-3906-9D2E\" EOF **IMPORTANT: After this replace the LicenseKey value with the effective license sequence you obtained from Accelleran Create a configuration file for the Effnet DU: nr_cell_identity ( in binary format eg 3 fill in ...00011 ) nr_pci ( decimal format eg 51 fill in 51 ) plmn_identity ( eg 235 88 fill in 235f88. fill in 2 times in this file) arfcn ( decimal format calculated from the center frequency, see chapter ) nr_frequency_band ( 77 or 78 ) 5gs_tac ( 3 byte array. eg 1 fill in 000001 ) mkdir -p ~/install- $DU_VERSION / cd !$ tee du-config.json <<EOF { \"configuration\": { \"du_address\": \"du\", \"cu_address\": \"cu\", \"gtp_listen_address\": \"du\", \"f1c_bind_address\": \"du\", \"vphy_listen_address\": \"127.0.0.1\", \"vphy_port\": 13337, \"vphy_tick_multiplier\": 1, \"gnb_du_id\": 38209903575, \"gnb_du_name\": \"cab-03-cell\", \"phy_control\": { \"crnti_range\": { \"min\": 42000, \"max\": 42049 } }, \"rrc_version\": { \"x\": 15, \"y\": 6, \"z\": 0 }, \"served_cells_list\": [ { \"served_cell_information\": { \"nr_cgi\": { \"plmn_identity\": \"235f88\", \"nr_cell_identity\": \"000000000000000000000000000000000011\" }, \"nr_pci\": 2, \"5gs_tac\": \"000001\", \"ran_area_code\": 1, \"served_plmns\": [ { \"plmn_identity\": \"235f88\", \"tai_slice_support_list\": [ { \"sst\": 1 } ] } ], \"nr_mode_info\": { \"nr_freq_info\": { \"nr_arfcn\": 662664, \"frequency_band_list\": [ { \"nr_frequency_band\": 77 } ] }, \"transmission_bandwidth\": { \"bandwidth_mhz\": 40, \"scs_khz\": 30, \"nrb\": 106 }, \"pattern\": { \"periodicity_in_slots\": 10, \"downlink\": { \"slots\": 7, \"symbols\": 6 }, \"uplink\": { \"slots\": 2, \"symbols\": 4 } } }, \"measurement_timing_configuration\": [ 222, 173, 190, 239 ], \"dmrs_type_a_position\": \"pos2\", \"intra_freq_reselection\": \"allowed\", \"ssb_pattern\": \"1000000000000000000000000000000000000000000000000000000000000000\", \"ssb_periodicity_serving_cell_ms\": 20, \"prach_configuration_index\": 202, \"ssb_pbch_scs\": 30, \"offset_point_a\": 6, \"k_ssb\": 0, \"coreset_zero_index\": 3, \"search_space_zero_index\": 2, \"ra_response_window_slots\": 20, \"sr_slot_periodicity\": 40, \"sr_slot_offset\": 7, \"search_space_other_si\": 1, \"paging_search_space\": 1, \"ra_search_space\": 1, \"bwps\": [ { \"id\": 0, \"start_crb\": 0, \"num_rb\": 106, \"scs\": 30, \"cyclic_prefix\": \"normal\" } ], \"coresets\": [ { \"id\": 1, \"bwp_id\": 0, \"fd_resources\": \"111100000000000000000000000000000000000000000\", \"duration\": 2, \"interleaved\": { \"reg_bundle_size\": 6, \"interleaver_size\": 2 }, \"precoder_granularity\": \"same_as_reg_bundle\" } ], \"search_spaces\": [ { \"id\": 1, \"control_resource_set_id\": 0, \"common\": {} }, { \"id\": 2, \"control_resource_set_id\": 1, \"ue_specific\": { \"dci_formats\": \"formats0-1-And-1-1\" } } ], \"maximum_ru_power_dbm\": 53.0, \"num_tx_antennas\": 2, \"trs\": { \"periodicity_and_offset\": { \"period\": 80, \"offset\": 1 }, \"symbol_pair\": \"four_eight\", \"subcarrier_location\": 1 }, \"csi_rs\": { \"periodicity_and_offset\": { \"period\": 40, \"offset\": 15 } }, \"harq_processes_for_pdsch\": 16, \"minimum_k1_delay\": 1, \"minimum_k2_delay\": 3, \"force_rlc_buffer_size\": 2500000 } } ] } } EOF Before creating the docker-compose.yml file, make sure to set the $CU_IP environment variable where you will store the F1 IP address of the CUCP that you have already deployed using the dRAX Dashboard (section CUCP Installation ) This IP address can be determined by executing the following command: kubectl get services | grep f1 The CUCP F1 SCTP interface external address is the second IP address and should be in the IP pool that was assigned to MetalLB in dRax Installation . Now, create a docker-compose configuration file: remove the rru: when NOT using a b210. eg when using a b650 mkdir -p ~/install- $DU_VERSION / cd !$ tee docker-compose.yml <<EOF version: \"2\" services: phluido_l1: image: phluido_l1:v0.8.4.2 container_name: phluido_l1 tty: true privileged: true ipc: shareable shm_size: 2gb command: /config.cfg volumes: - \"$PWD/l1-config.cfg:/config.cfg:ro\" - \"/run/logs-du/l1:/workdir\" - \"/etc/machine-id:/etc/machine-id:ro\" working_dir: \"/workdir\" network_mode: host cpuset: \"0,2,4,6,8,10,12,14\" du: image: gnb_du_main_phluido:2022-07-01-q2-pre-release volumes: - \"$PWD/du-config.json:/config.json:ro\" - \"/run/logs-du/du:/workdir\" - /run/pcscd/pcscd.comm:/run/pcscd/pcscd.comm ipc: container:phluido_l1 tty: true privileged: true depends_on: - phluido_l1 entrypoint: [\"/bin/sh\", \"-c\", \"sleep 4 && exec /gnb_du_main_phluido /config.json\"] working_dir: \"/workdir\" extra_hosts: - \"cu:$F1_CU_IP\" - \"du:$SERVER_IP\" cpuset: \"0,2,4,6,8,10,12,14\" phluido_rru: image: phluido_rru:v0.8.4.2 tty: true privileged: true depends_on: - du - phluido_l1 network_mode: host volumes: - \"$PWD/phluido/PhluidoRRU_NR_EffnetTDD_B210.cfg:/config.cfg:ro\" - \"$PWD/logs/rru:/workdir\" entrypoint: [\"/bin/sh\", \"-c\", \"sleep 20 && exec /PhluidoRRU_NR /config.cfg\"] working_dir: \"/workdir\" EOF","title":"DU/L1/RRU Configuration and docker compose"},{"location":"du-install/#start-the-du","text":"Start the DU by running the following command: docker-compose up -f accelleran-du-phluido/accelleran-du-phluido-2022-01-31/docker-compose.yml If all goes well this will produce output similar to: Starting phluido_l1 ... done Recreating accelleran-du-phluido-2022-01-31_du_1 ... done Recreating accelleran-du-phluido-2022-01-31_phluido_rru_1 ... done Attaching to phluido_l1, accelleran-du-phluido-2022-01-31_du_1, accelleran-du-phluido-2022-01-31_phluido_rru_1 phluido_l1 | Reading configuration from config file \"/config.cfg\"... phluido_l1 | ******************************************************************************************************* phluido_l1 | * * phluido_l1 | * Phluido 5G-NR virtualized L1 implementation * phluido_l1 | * * phluido_l1 | * Copyright (c) 2014-2020 Phluido Inc. * phluido_l1 | * All rights reserved. * phluido_l1 | * * phluido_l1 | * The User shall not, and shall not permit others to: * phluido_l1 | * - integrate Phluido Software within its own products; * phluido_l1 | * - mass produce products that are designed, developed or derived from Phluido Software; * phluido_l1 | * - sell products which use Phluido Software; * phluido_l1 | * - modify, correct, adapt, translate, enhance or otherwise prepare derivative works or * phluido_l1 | * improvements to Phluido Software; * phluido_l1 | * - rent, lease, lend, sell, sublicense, assign, distribute, publish, transfer or otherwise * phluido_l1 | * make available the PHLUIDO Solution or any portion thereof to any third party; * phluido_l1 | * - reverse engineer, disassemble and/or decompile Phluido Software. * phluido_l1 | * * phluido_l1 | * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, * phluido_l1 | * INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A * phluido_l1 | * PARTICULAR PURPOSE ARE DISCLAIMED. * phluido_l1 | * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, * phluido_l1 | * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF * phluido_l1 | * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) * phluido_l1 | * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR * phluido_l1 | * TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS * phluido_l1 | * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. * phluido_l1 | * * phluido_l1 | ******************************************************************************************************* phluido_l1 | phluido_l1 | Copyright information already accepted on 2020-11-27, 08:56:08. phluido_l1 | Starting Phluido 5G-NR L1 software... phluido_l1 | PHAPI version = 0.5 (12/10/2020) phluido_l1 | L1 SW version = 0.8.1 phluido_l1 | L1 SW internal rev = r3852 phluido_l1 | Parsed configuration parameters: phluido_l1 | LicenseKey = XXXX-XXXX-XXXX-XXXX-XXXX-XXXX-XXXX-XXXX phluido_l1 | maxNumPrachDetectionSymbols = 1 phluido_l1 | maxNumPdschLayers = 2 phluido_l1 | maxNumPuschLayers = 1 phluido_l1 | maxPuschModOrder = 6 phluido_l1 | phluido_rru_1 | linux; GNU C++ version 7.3.0; Boost_106501; UHD_003.010.003.000-0-unknown phluido_rru_1 | phluido_rru_1 | Logs will be written to file \"phluidoRru.log\". phluido_rru_1 | -- Detected Device: B210 phluido_rru_1 | -- Operating over USB 3. phluido_rru_1 | -- Initialize CODEC control... phluido_rru_1 | -- Initialize Radio control... phluido_rru_1 | -- Performing register loopback test... pass phluido_rru_1 | -- Performing register loopback test... pass phluido_rru_1 | -- Performing CODEC loopback test... pass phluido_rru_1 | -- Performing CODEC loopback test... pass phluido_rru_1 | -- Setting master clock rate selection to 'automatic'. phluido_rru_1 | -- Asking for clock rate 16.000000 MHz... phluido_rru_1 | -- Actually got clock rate 16.000000 MHz. phluido_rru_1 | -- Performing timer loopback test... pass phluido_rru_1 | -- Performing timer loopback test... pass phluido_rru_1 | -- Setting master clock rate selection to 'manual'. phluido_rru_1 | -- Asking for clock rate 23.040000 MHz... phluido_rru_1 | -- Actually got clock rate 23.040000 MHz. phluido_rru_1 | -- Performing timer loopback test... pass phluido_rru_1 | -- Performing timer loopback test... pass","title":"Start the DU"},{"location":"du-install/#prepare-and-bring-on-air-the-benetel-650-radio","text":"This section is exclusively applicable to the user/customer that intends to use the Benetel B650 Radio End with our Accellleran 5G end to end solution, if you do not have such radio end the informations included in this section may be misleading and bring to undefined error scenarios. Please contact Accelleran if your Radio End is not included in any section of this user guide","title":"Prepare and bring on air the Benetel 650 Radio"},{"location":"du-install/#diagram","text":"In the picture below we schematically show what will be run on the server by Docker and how the RRU is linked to the server itself: as mentioned early in this case the two components run by Docker are the L1 and the DU, while the RRU is supposedly served by a dedicated NIC Card capable of handling a 10 Gbps fiber link. If this is not your case please consult the section dedicted to Ettus B210 bring up or contact Accelleran for further information 10.10.0.100:ssh +-------------+ | | | | +-----------+ +-----------+ | | | | | | | RU +----fiber----+ L1 | | DU | | | | | | | | | +-----------+ +-----------+ | | +-------------+ aa:bb:cc:dd:ee:ff 11:22:33:44:55:66 10.10.0.2:44000 10.10.0.1:44000 eth0 enp45s0f0 port FIBER1","title":"Diagram"},{"location":"du-install/#server-installtion","text":"","title":"Server installtion"},{"location":"du-install/#dul1-configuration-and-docker-compose","text":"Differently from the Ettus B210, Benetel runs the RRU software on board, therefore we only need to prepare 2 software components in the server, that is, 2 Containers, Effnet DU and Phluido L1. Create the configuration file for the Phluido L1 component the PhluidoL1_NR_Benetel.cfg file delivered by effnet Make sure to set the value LicenseKey option to the received Phluido license key: mkdir -p ~/install- $DU_VERSION / cd !$ tee PhluidoL1_NR_Benetel.cfg <<EOF /****************************************************************** * * This file is subject to the terms and conditions defined in * file 'LICENSE'. * ******************************************************************/ //Enables verbose binary logging. WARNING: very CPU intensive and creates potentially huge output files. Use with caution. // // DEVEL: 1.2G/67 // DEBUG: 953M/67 // INFORMATIVE: 29K/67 // default: 26K/67 // CRITICAL: 0/393 // WARNING: 0/393 (x/y x=log size in L1.encr.log file, y=log size in L1.open.log file. In a time of 1 minute ) //logLevel_verbose = \"WARNING\"; //bbuFronthaulServerMode = 1; bbuFronthaulServerAddr = \"10.10.0.1\"; // Enable 64-QAM support for PUSCH (license-specific). maxPuschModOrder = 6; maxNumPdschLayers = 2; maxNumPuschLayers = 1; cccServerPort = 44444; cccInterfaceMode = 1; kpiOutputFormat = 2; targetRecvDelay_us = 2500; numWorkers = 6; // shall be less or equal to the number of cores assigned to L1 with the CPU pinning //License key put here please the effective 32 digits sequence you received for this deployment LicenseKey = \"$L1_PHLUIDO_KEY\"; EOF **IMPORTANT: After this replace the LicenseKey value with the effective license sequence you obtained from Accelleran, all the other parameters shall not be modified Create a configuration file for the Effnet DU: mkdir -p ~/install- $DU_VERSION / cd !$ tee b650_config_40mhz.json <<EOF { \"configuration\": { \"du_address\": \"du\", \"cu_address\": \"cu\", \"f1c_bind_address\": \"du\", \"gtp_listen_address\": \"du\", \"vphy_listen_address\": \"127.0.0.1\", \"vphy_port\": 13337, \"vphy_tick_multiplier\": 1, \"gnb_du_id\": 38209903575, \"gnb_du_name\": \"This is the dell two HO setup cell one\", \"phy_control\": { \"crnti_range\": { \"min\": 42000, \"max\": 42049 } }, \"rrc_version\": { \"x\": 15, \"y\": 6, \"z\": 0 }, \"served_cells_list\": [ { \"served_cell_information\": { \"nr_cgi\": { \"plmn_identity\": \"001f01\", \"nr_cell_identity\": \"000000000000000000000000000000000001\" }, \"nr_pci\": 51, \"5gs_tac\": \"000001\", \"ran_area_code\": 1, \"served_plmns\": [ { \"plmn_identity\": \"001f01\", \"tai_slice_support_list\": [ { \"sst\": 1 } ] } ], \"nr_mode_info\": { \"nr_freq_info\": { \"nr_arfcn\": 648840, \"frequency_band_list\": [ { \"nr_frequency_band\": 78 } ] }, \"transmission_bandwidth\": { \"bandwidth_mhz\": 40, \"scs_khz\": 30, \"nrb\": 106 }, \"pattern\": { \"periodicity_in_slots\": 10, \"downlink\": { \"slots\": 7, \"symbols\": 6 }, \"uplink\": { \"slots\": 2, \"symbols\": 4 } } }, \"measurement_timing_configuration\": [ 222, 173, 190, 239 ], \"dmrs_type_a_position\": \"pos2\", \"intra_freq_reselection\": \"allowed\", \"ssb_pattern\": \"1000000000000000000000000000000000000000000000000000000000000000\", \"ssb_periodicity_serving_cell_ms\": 20, \"prach_configuration_index\": 202, \"ssb_pbch_scs\": 30, \"offset_point_a\": 6, \"k_ssb\": 0, \"coreset_zero_index\": 3, \"search_space_zero_index\": 2, \"ra_response_window_slots\": 20, \"sr_slot_periodicity\": 40, \"sr_slot_offset\": 7, \"search_space_other_si\": 1, \"paging_search_space\": 1, \"ra_search_space\": 1, \"bwps\": [ { \"id\": 0, \"start_crb\": 0, \"num_rb\": 106, \"scs\": 30, \"cyclic_prefix\": \"normal\" } ], \"coresets\": [ { \"id\": 1, \"bwp_id\": 0, \"fd_resources\": \"111100000000000000000000000000000000000000000\", \"duration\": 2, \"interleaved\": { \"reg_bundle_size\": 6, \"interleaver_size\": 2 }, \"precoder_granularity\": \"same_as_reg_bundle\" } ], \"search_spaces\": [ { \"id\": 1, \"control_resource_set_id\": 0, \"common\": {} }, { \"id\": 2, \"control_resource_set_id\": 1, \"ue_specific\": { \"dci_formats\": \"formats0-1-And-1-1\" } } ], \"maximum_ru_power_dbm\": 35.0, \"num_tx_antennas\": 2, \"trs\": { \"periodicity_and_offset\": { \"period\": 80, \"offset\": 1 }, \"symbol_pair\": \"four_eight\", \"subcarrier_location\": 1 }, \"periodic_srs_periodicity\": 64, \"csi_rs\": { \"periodicity_and_offset\": { \"period\": 40, \"offset\": 15 } }, \"force_rlc_buffer_size\": 8388608, \"harq_processes_for_pdsch\": 16, \"minimum_k1_delay\": 1, \"minimum_k2_delay\": 3 } } ] } } EOF","title":"DU/L1 Configuration and docker compose"},{"location":"du-install/#frequency-offsets-point-a-calculation","text":"This section is essential to proceed correctly and determine the exact parameters that will allow the Benetel Radio to go on air correctly and the UEs to be able to see the cell and attempt an attach so it is particularly important to proceed carefully on this point. there are currently several limitations on the Frequencies that go beyond the simple definition of 5G NR Band: the selected frequency should be above 3700 MHz the selected band can be B77 or B78 the selected frequency must be devisable by 3.84 the K_ssb must be 0 the offset to point A must be 6 all the subcarriers shal be 30 KHz Let's proceed with an example: We want to set a center frequency of 3750 MHz, this is not devisable by 3.84, the first next frequencies that meet this condition are 3747,84 (976 3.84) 3751.68 (977 3,84) so let's consider first 3747,84 MHz and verify the conditions on the K_ssb and Offset to Point A with this online tool (link at: (https://www.sqimway.com/nr_refA.php) ) We remember to set the Band 78, SCs at 30 KHz, the Bandwidth at 40 MHz and the ARFCN of the center frequency 3747,84 which is 649856 and when we hit the RUN button we obtain: This Frequency, however does not meet the GSCN Synchronisation requirements as in fact the Offset to Point A of the first channel is 2 and the K_ssb is 16, this will cause the UE to listen on the wrong channel so the SIBs will never be seen and therefore the cell is \"invisible\" We then repeat the exercise with the higher center frequency 3751,68 MHz, which yelds a center frequency ARFCN of 650112 and a point A ARFCN of 648840 and giving another run we will see that now the K_ssb and the Offset to Point A are correct: With these information at hand we are going to determine: point A frequency : 3732.60 ( arfcn : 648840 ) - edit du configuration in the appropriate json file center Frequency : 3751.68 ( arfcn : 650112 ) - edit RU configuration directly on the Benetel Radio End (see next sections)","title":"Frequency, Offsets, Point A Calculation"},{"location":"du-install/#create-docker-compose","text":"Before creating the docker-compose.yml file, make sure to set the $CU_IP environment variable where you will store the F1 IP address of the CUCP that you have already deployed using the dRAX Dashboard (section CUCP Installation ) This IP address can be determined by executing the following command: kubectl get services | grep f1 The CUCP F1 SCTP interface external address is the second IP address and should be in the IP pool that was assigned to MetalLB in dRax Installation . Now, create a docker-compose configuration file: mkdir -p ~/install- $DU_VERSION / cd !$ tee docker-compose-B650.yml <<EOF version: \"2\" services: phluido_l1: image: phluido_l1:v0.8.4.2 container_name: phluido_l1 tty: true privileged: true ipc: shareable shm_size: 2gb command: /config.cfg volumes: - \"$PWD/PhluidoL1_NR_Benetel.cfg:/config.cfg:ro\" - \"/run/logs-du/l1:/workdir\" - \"/etc/machine-id:/etc/machine-id:ro\" working_dir: \"/workdir\" network_mode: host cpuset: \"$CORE_SET_DU\" du: image: gnb_du_main_phluido:2022-07-01-q2-pre-release volumes: - \"$PWD/b650_config_40mhz.json:/config.json:ro\" - \"/run/logs-du/du:/workdir\" - /run/pcscd/pcscd.comm:/run/pcscd/pcscd.comm ipc: container:phluido_l1 tty: true privileged: true depends_on: - phluido_l1 entrypoint: [\"/bin/sh\", \"-c\", \"sleep 2 && exec /gnb_du_main_phluido /config.json\"] working_dir: \"/workdir\" extra_hosts: - \"cu:$F1_CU_IP\" - \"du:$SERVER_IP\" network_mode: host cpuset: \"$CORE_SET_DU\" EOF","title":"Create docker compose"},{"location":"du-install/#prepare-to-configure-the-benetel-650","text":"The benetel is connected with a fiber to the server. 1. The port on the physical B650 RRU is labeled port FIBER1 2. The port on the server is one of these listed below. $ lshw | grep SFP -C 5 WARNING: you should run this program as super-user. capabilities: pci normal_decode bus_master cap_list configuration: driver = pcieport resources: irq:29 ioport:f000 ( size = 4096 ) memory:f8000000-f86fffff *-network:0 DISABLED description: Ethernet interface product: 82599ES 10 -Gigabit SFI/SFP+ Network Connection vendor: Intel Corporation physical id: 0 bus info: pci@0000:2d:00.0 logical name: enp45s0f0 version: 01 -- capabilities: bus_master cap_list rom ethernet physical fibre 10000bt-fd configuration: autonegotiation = off broadcast = yes driver = ixgbe driverversion = 5 .1.0-k firmware = 0x2b2c0001 latency = 0 link = no multicast = yes resources: irq:202 memory:f8000000-f807ffff ioport:f020 ( size = 32 ) memory:f8200000-f8203fff memory:f8080000-f80fffff memory:f8204000-f8303fff memory:f8304000-f8403fff *-network:1 DISABLED description: Ethernet interface product: 82599ES 10 -Gigabit SFI/SFP+ Network Connection vendor: Intel Corporation physical id: 0 .1 bus info: pci@0000:2d:00.1 logical name: enp45s0f1 version: 01 by setting both network devices to UP you find out which one is connected. In this example it's enp45s0f0. This port is the one we connected the fiber to. :ad@5GCN:~$ sudo ip link set dev enp45s0f0 up :ad@5GCN:~$ sudo ip link set dev enp45s0f1 up :ad@5GCN:~$ ip -br a : enp45s0f0 UP fe80::6eb3:11ff:fe08:a4e0/64 enp45s0f1 DOWN : configure the static ip 10.10.0.1 of port enp45s0f0 on your server netplan (typically /etc/netplan/50-cloud-init.yaml ) as follows: network: ethernets: enp45s0f0: dhcp4: false dhcp6: false optional: true addresses: - 10 .10.0.1/24 mtu: 9000 To apply this configuration you can use sudo netplan apply Double check the result $ ip -br a | grep enp45 enp45s0f0 UP 10 .10.0.1/24 fe80::6eb3:11ff:fe08:a4e0/64 enp45s0f1 DOWN The default ip of the benetel radio is 10.10.0.100 . This is the MGMT ip. We can ssh to it as root@10.10.0.100 without password We can anyway find that IP out using nmap $ nmap 10 .10.0.0/24 Starting Nmap 7 .60 ( https://nmap.org ) at 2021 -09-21 10 :15 CEST Nmap scan report for 10 .10.0.1 Host is up ( 0 .000040s latency ) . Not shown: 996 closed ports PORT STATE SERVICE 22 /tcp open ssh 111 /tcp open rpcbind 5900 /tcp open vnc 9100 /tcp open jetdirect Nmap scan report for 10 .10.0.100 Host is up ( 0 .0053s latency ) . Not shown: 998 closed ports PORT STATE SERVICE 22 /tcp open ssh 111 /tcp open rpcbind Nmap done : 256 IP addresses ( 2 hosts up ) scanned in 3 .10 seconds A route is added also in the routing table automatically $ route -n | grep 10 .10.0.0 10 .10.0.0 0 .0.0.0 255 .255.255.0 U 0 0 0 enp45s0f0 ```` now you can ssh to the benetel ``` bash $ ssh root@10.10.0.100 Last login: Fri Feb 7 16 :45:59 2020 from 10 .10.0.1 root@benetelru:~# ls -l -rwxrwxrwx 1 root root 1572 Sep 10 2021 DPLL3_1PPS_REGISTER_PATCH.txt drwxrwxrwx 2 root root 0 Feb 7 16 :44 adrv9025 -rwxrwxrwx 1 root root 1444 Feb 7 16 :40 dev_struct.dat -rwxrwxrwx 1 root root 17370 Sep 10 2021 dpdModelReadback.txt -rwxrwxrwx 1 root root 5070 Feb 7 17 :00 dpdModelcoefficients.txt -rwxrwxrwx 1 root root 24036 Sep 10 2021 eeprog_cp60 -rwxrwxrwx 1 root root 1825062 Feb 7 15 :58 madura_log_file.txt -rw------- 1 root root 1230 Feb 7 2020 nohup.out -rwxr-xr-x 1 root root 57 Feb 7 2020 nohup_handshake -rwxrwxrwx 1 root root 571 Feb 7 2020 progBenetelDuMAC_CATB -rwxr-xr-x 1 root root 1121056 Feb 7 16 :24 quickRadioControl -rwxrwxrwx 1 root root 1151488 Sep 10 2021 radiocontrol_prv-nk-cliupdate -rwxrwxrwx 1 root root 22904 Aug 24 2021 registercontrol -rwxrwxrwx 1 root root 164 Feb 7 16 :35 removeResetRU_CATB -rwxrwxrwx 1 root root 163 Feb 7 2020 reportRuStatus -rwxrwxrwx 1 root root 162 Feb 7 16 :35 resetRU_CATB -rwxr-xr-x 1 root root 48 Feb 7 15 :57 runSync -rwxrwxrwx 1 root root 21848 Sep 10 2021 smuconfig -rwxrwxrwx 1 root root 17516 Sep 10 2021 statmon -rwxrwxrwx 1 root root 23248 Sep 10 2021 syncmon -rwxr-xr-x 1 root root 182 Feb 7 16 :41 trialHandshake root@benetelru:~# However, as mentioned, that above is the management IP address, whereas for the data interface the Benetel RU has a different MAC on 10.10.0.2 for instance aa:bb:cc:dd:ee:ff and we can put this on the Server where the DU runs in the file: /etc/networkd-dispatcher/routable.d/macs.sh Add mac entry script in routable.d. To find out the $MAC_RU ( the mac address of the RU interface ) use sudo tcpdump -i $SERVER_RU_INT port 44000 -en a trace like this appears 21 :19:20.285848 aa:bb:cc:dd:ee:ff > 00 :1e:67:fd:f5:51, ethertype IPv4 ( 0x0800 ) , length 64 : 10 .10.0.2.44000 > 10 .10.0.1.44000: UDP, length 20 sudo tee /etc/networkd-dispatcher/routable.d/macs.sh <<EOF #!/bin/sh sudo arp -s $RU_IP $MAC_RU -i $SERVER_RU_INT EOF Benetel650 does not answer arp requests. With this arp entry in the arp table the server knows to which mac address the ip packets with destination ip 10.10.0.2 should go the macs.sh script is executes automatically if it has the correct permissions. Set the correct permissions. sudo chown root /etc/networkd-dispatcher/routable.d/macs.sh sudo chgrp root /etc/networkd-dispatcher/routable.d/macs.sh sudo chmod 755 /etc/networkd-dispatcher/routable.d/macs.sh The macs.sh script runs when the interface to the RU goes UP. Run this to bring the RU interface UP sudo ip link set $SERVER_RU_INT down sleep 1 sudo ip link set $SERVER_RU_INT up now check if the entry for $RU_IP is in the arp table. $ arp -a | grep 10 .10. ? ( $RU_IP ) at $MAC_RU [ ether ] PERM on $SERVER_RU_INT When the fiber port comes up at the server side eno2 UP 10.10.0.1/24 fe80::266e:96ff:fe43:64e2/64 test the automatic execution of macs.sh by running journalctl -f and plugging in the fiber. Each time it is plugged in you will see the the execution of the arp which has been put in the macs.sh script above.","title":"Prepare to configure the Benetel 650"},{"location":"du-install/#version-check","text":"finding out the version and commit hash of the benetel650 commit hash root@benetelru:~# registercontrol -v Lightweight HPS-to-FPGA Control Program Version : V1.2.0 ****BENETEL PRODUCT VERSIONING BLOCK**** This Build Was Created Locally. Please Use Git Pipeline! Project ID NUMBER: 0 Git # Number: f6366d7adf84933ab2b242a345bd63c07fedb9e5 Build ID: 0 Version Number: 0.0.1 Build Date: 2/12/2021 Build Time H:M:S: 18:20:3 ****BENETEL PRODUCT VERSIONING BLOCK END**** The version which is referred to. This is version 0.3. Depending on the version different configuration commands apply. root@benetelru:~# cat /etc/benetel-rootfs-version RAN650-2V0.3","title":"Version Check"},{"location":"du-install/#configure-the-physical-benetel-radio-end-release-v05x","text":"There are several parameters that can be checked and modified by reading writing the EEPROM, for this we recommend to make use of the original Benetel Software User Guide for RANx50-02 CAT-B O-RUs, in case of doubt ask for clarification to Accelleran Customer Support . Here we just present two of the most used parameters, that will need an adjustment for each deployment.","title":"Configure the physical Benetel Radio End - Release V0.5.x"},{"location":"du-install/#cfr-enabled","text":"By default the RU ships with CFR enabled. What still needs to be done is set register 0366 to value 0xFFF . Do this by altering file /usr/sbin/radio_setup_ran650_b.sh with following line. registercontrol -w c0366 -x 0xFFF >> ${ LOG_RAD_STAT_FP }","title":"CFR enabled"},{"location":"du-install/#mac-address-of-the-du","text":"Create this script to program the mac address of the DU inside the RRU. Remember the RRU does not request arp, so we have to manually configure that. If the MAC address of the server port you use to connect to the Benetel B650 Radio End (the NIC Card port where the fiber originates from) is $MAC_DU 11:22:33:44:55:66 then you can program the EEPROM of your B650 unit as follows: Here the value of $MAC_DU need to be used. Run this on the bare metal host server to generate the script that will run in the RU to set the mac. echo \" registercontrol -w 0xC036B -x 0x88000088 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1a:0x01:0x $( echo $MAC_DU | cut -c1-2 ) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1b:0x01:0x $( echo $MAC_DU | cut -c4-5 ) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1c:0x01:0x $( echo $MAC_DU | cut -c7-8 ) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1d:0x01:0x $( echo $MAC_DU | cut -c10-11 ) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1e:0x01:0x $( echo $MAC_DU | cut -c13-14 ) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1f:0x01:0x $( echo $MAC_DU | cut -c16-17 ) registercontrol -w 0xC036B -x 0x88000488 \" Something like this will get generated. Copy and Paste this generated script into the RU. registercontrol -w 0xC036B -x 0x88000088 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1a:0x01:0x11 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1b:0x01:0x22 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1c:0x01:0x33 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1d:0x01:0x44 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1e:0x01:0x55 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x1f:0x01:0x66 registercontrol -w 0xC036B -x 0x88000488 Login in the RU ssh root@ $RU_MGMT_IP and paste the script here. You can read the EEPROM now and double check what you did: eeprog_cp60 -q -f -x -16 /dev/i2c-0 0x57 -x -r 26:6 !! Finally, reboot your Radio End to make the changes effective","title":"MAC Address of the DU"},{"location":"du-install/#set-the-frequency-of-the-radio-end","text":"Create this script to program the Center Frequency in MHz of your B650 RRU. Remember to determine a valid frequency as indicated previously in the document, taking into account all the constraints and the relationship to the Offset Point A. If the Center Frequency you want to is for instance 3751,680 MHz then you can program the EEPROM of your B650 unit as follows: Run the below script on the bare metal host. It will product a script that needs to run on the RU. echo\" registercontrol -w 0xC036B -x 0x88000088 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x174:0x01:0x3$(echo $FREQ_CENTER | cut -c1) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x175:0x01:0x3$(echo $FREQ_CENTER | cut -c2) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x176:0x01:0x3$(echo $FREQ_CENTER | cut -c3) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x177:0x01:0x3$(echo $FREQ_CENTER | cut -c4) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x178:0x01:0x2E eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x179:0x01:0x3$(echo $FREQ_CENTER | cut -c6) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17A:0x01:0x3$(echo $FREQ_CENTER | cut -c7) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17B:0x01:0x3$(echo $FREQ_CENTER | cut -c8) registercontrol -w 0xC036B -x 0x88000488 registercontrol -w 0xC036B -x 0x88000088 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17C:0x01:0x3$(echo $FREQ_CENTER | cut -c1) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17D:0x01:0x3$(echo $FREQ_CENTER | cut -c2) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17E:0x01:0x3$(echo $FREQ_CENTER | cut -c3) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17F:0x01:0x3$(echo $FREQ_CENTER | cut -c4) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x180:0x01:0x2E eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x181:0x01:0x3$(echo $FREQ_CENTER | cut -c6) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x182:0x01:0x3$(echo $FREQ_CENTER | cut -c7) eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x183:0x01:0x3$(echo $FREQ_CENTER | cut -c8) registercontrol -w 0xC036B -x 0x88000488 \" The script that is produces looks like this. echo\" registercontrol -w 0xC036B -x 0x88000088 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x174:0x01:0x33 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x175:0x01:0x37 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x176:0x01:0x35 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x177:0x01:0x31 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x178:0x01:0x2E eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x179:0x01:0x36 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17A:0x01:0x38 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17B:0x01:0x30 registercontrol -w 0xC036B -x 0x88000488 registercontrol -w 0xC036B -x 0x88000088 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17C:0x01:0x33 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17D:0x01:0x37 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17E:0x01:0x35 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x17F:0x01:0x31 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x180:0x01:0x2E eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x181:0x01:0x36 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x182:0x01:0x38 eeprog_cp60 -f -x -16 /dev/i2c-0 0x57 -w 0x183:0x01:0x30 registercontrol -w 0xC036B -x 0x88000488 \" Verify the script if it has the ascii codes for the frequency digits. Each byte 0x33,0x37,0x35, ... is the ascii value of a numbers 3751,680, often the calculation stops at two digits after the comma, consider the last digit always as a zero You may then want to double check what you did by reading the EEPROM: eeprog_cp60 -q -f -16 /dev/i2c-0 0x57 -r 372:8 Copy/Paste this script and run in in the RU. ssh to the RU and pasted it. ssh root@ $RU_MGMT_IP Once again, this is the CENTER FREQUENCY IN MHz that we calculated in the previous sections, and has to go hand in hand with the point A Frequency as discussed above Example for frequency 3751.68MHz (ARFCN=650112) you have set make sure to edit/check the pointA frequency ARFCN value back in the DU config json file in the server (in this example PointA_ARFCN=648840) Reboot the BNTL650 to make changes effective When the RU comes online ( 5 minutes ) run the following to see what the new frequency shows. ssh root $RU_MGMT_IP radiocontrol -o G a","title":"Set the Frequency of the Radio End"},{"location":"du-install/#set-attenuation-level","text":"This operation allows to temporary modify the attenuation of the transmitting channels of your B650 unit. Temporarily means that at the next reboot the Cell will default to the originally calibrated values, by default the transmission power is set to 25 dBm hence the attenuation is 15000 mdB (offset to the max TX Power). To adjust this power for the transmitter the user must edit the attenuation setting: For increasing the power the attenuation must be reduced For decreasing the power, the attenuation must be increased IMPORTANT NOTE: As of now, channel 2 and 4 are off and are not up for modification please do not try and modify those attenuation parameters So if we want, for instance, to REDUCE the Tx Power by 5 dB, we will then INCREASE the attenuation by 5000 mdB. Let's consider that each cell is calibrated individually so the first thing to do is to take note of the default values and offset from there to obtained the desired TX Power per channel So here are the steps: read current attenuations ~# radiocontrol -o G a Benetel radiocontrol Version : 0.9.0 Madura API Version : 5.1.0.21 Madura ARM FW version : 5.0.0.32 Madura ARM DPD FW version : 5.0.0.32 Madura Stream version : 8.0.0.5 Madura Product ID : 0x84 Madura Device Revision : 0xb0 Tx1 Attenuation (mdB) : 16100 Tx2 Attenuation (mdB) : 40000 Tx3 Attenuation (mdB) : 15800 Tx4 Attenuation (mdB) : 40000 PLL1 Frequency (Hz) : 0 PLL2 Frequency (Hz) : 3751680000 Front-end Control : 0x2aa491 Madura Deframer 0 : 0x87 Madura Framer 0 : 0xa Internal Temperature (degC) : 47 External Temperature (degC) : 42.789063 RX1 Power Level (dBFS) : -60.750000 RX2 Power Level (dBFS) : -60.750000 RX3 Power Level (dBFS) : -60.750000 RX4 Power Level (dBFS) : -60.750000 ORX1 Peak/Mean Power Level (dBFS) : -10.839418/-22.709361 ORX2 Peak/Mean Power Level (dBFS) : -inf/-inf ORX3 Peak/Mean Power Level (dBFS) : -10.748048/-21.656226 ORX4 Peak/Mean Power Level (dBFS) : -inf/-inf We can then conclude that our Antenna has been originally calibrated to have +1100 mdB on channel 1 and +800 mdB to obtain exactly 25 dBm Tx power on those chanels, so that we will then offset our 5000 dBm of extra attenuation and therefore the new attenuation levels are Tx1=16100+5000=21100 mdB and Tx2=15800+5000=20800mdB set attenuation for antenna 1 /usr/bin/radiocontrol -o A 21100 1 set attenuation for antenna 3 /usr/bin/radiocontrol -o A 20800 4 yes the 4 at the end seems to be correct Bear in mind these settings will stay as long as you don't reboot the Radio and default back to the original calibration values once you reboot the unit assess the new status of your radio: ~# radiocontrol -o G a Benetel radiocontrol Version : 0.9.0 Madura API Version : 5.1.0.21 Madura ARM FW version : 5.0.0.32 Madura ARM DPD FW version : 5.0.0.32 Madura Stream version : 8.0.0.5 Madura Product ID : 0x84 Madura Device Revision : 0xb0 Tx1 Attenuation (mdB) : 21100 Tx2 Attenuation (mdB) : 40000 Tx3 Attenuation (mdB) : 20800 Tx4 Attenuation (mdB) : 40000 PLL1 Frequency (Hz) : 0 PLL2 Frequency (Hz) : 3751680000 Front-end Control : 0x2aa491 Madura Deframer 0 : 0x87 Madura Framer 0 : 0xa Internal Temperature (degC) : 47 External Temperature (degC) : 42.789063 RX1 Power Level (dBFS) : -60.750000 RX2 Power Level (dBFS) : -60.750000 RX3 Power Level (dBFS) : -60.750000 RX4 Power Level (dBFS) : -60.750000 ORX1 Peak/Mean Power Level (dBFS) : -10.839418/-22.709361 ORX2 Peak/Mean Power Level (dBFS) : -inf/-inf ORX3 Peak/Mean Power Level (dBFS) : -10.748048/-21.656226 ORX4 Peak/Mean Power Level (dBFS) : -inf/-inf","title":"Set attenuation level"},{"location":"du-install/#configure-the-physical-benetel-radio-end-older-then-release-v070","text":"","title":"Configure the physical Benetel Radio End - Older then Release V0.7.0"},{"location":"du-install/#auto-reset-dpd","text":"For releases older then V0.7.0 the dpd has to get reset every 30 minutes. This is not yet built inside and has to get created manually. These 3 steps need to be done. create these 2 files by copy/past the below cat <<EOF > /lib/systemd/system/dpd_reset.service [Unit] Description=Start DPD reset every 30 mins After=eth0ipset.service [Service] Type=forking ExecStart=/bin/sh /usr/sbin/dpd_reset.sh [Install] WantedBy=multi-user.target EOF cat <<EOF > /usr/sbin/dpd_reset.sh #! /bin/sh while true do sleep 1800 date '+%Y-%m-%d %H:%M:%S ##########' cd /home/root; radiocontrol -o D r 15 1 done >> /tmp/dpd_reset_status & EOF enable the service that just has been defined. systemctl enable dpd_reset.service and start the service systemctl start dpd_reset.service","title":"auto reset dpd"},{"location":"du-install/#starting-ru-benetel-650-manual-way","text":"","title":"Starting RU Benetel 650 - manual way"},{"location":"du-install/#prepare-cell","text":"When the CELL is OFF this traffic can be in this state shown below. ifstat -i $SERVER_RU_INT enp1s0f0 KB/s in KB/s out 71308.34 0.0 71318.21 0.0 In this case execute $ ssh root@10.10.0.100 handshake After execution you will have ifstat -i $SERVER_RU_INT enp1s0f0 KB/s in KB/s out 0.0 0.0 0.0 0.0 In this traffic state the dell is ready to start.","title":"prepare cell"},{"location":"du-install/#start-cell","text":"Bring the components up with docker compose cd ~/install- $DU_VERSION / docker-compose up -f docker-compose-B650.yml If all goes well this will produce output similar to: Starting phluido_l1 ... done Recreating accelleran-du-phluido-2022-01-31_du_1 ... done Attaching to phluido_l1, accelleran-du-phluido-2022-01-31_du_1 phluido_l1 | Reading configuration from config file \"/config.cfg\"... phluido_l1 | ******************************************************************************************************* phluido_l1 | * * phluido_l1 | * Phluido 5G-NR virtualized L1 implementation * phluido_l1 | * * phluido_l1 | * Copyright (c) 2014-2020 Phluido Inc. * phluido_l1 | * All rights reserved. * phluido_l1 | * * phluido_l1 | * The User shall not, and shall not permit others to: * phluido_l1 | * - integrate Phluido Software within its own products; * phluido_l1 | * - mass produce products that are designed, developed or derived from Phluido Software; * phluido_l1 | * - sell products which use Phluido Software; * phluido_l1 | * - modify, correct, adapt, translate, enhance or otherwise prepare derivative works or * phluido_l1 | * improvements to Phluido Software; * phluido_l1 | * - rent, lease, lend, sell, sublicense, assign, distribute, publish, transfer or otherwise * phluido_l1 | * make available the PHLUIDO Solution or any portion thereof to any third party; * phluido_l1 | * - reverse engineer, disassemble and/or decompile Phluido Software. * phluido_l1 | * * phluido_l1 | * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, * phluido_l1 | * INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A * phluido_l1 | * PARTICULAR PURPOSE ARE DISCLAIMED. * phluido_l1 | * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, * phluido_l1 | * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF * phluido_l1 | * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) * phluido_l1 | * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR * phluido_l1 | * TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS * phluido_l1 | * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. * phluido_l1 | * * phluido_l1 | ******************************************************************************************************* phluido_l1 | phluido_l1 | Copyright information already accepted on 2020-11-27, 08:56:08. phluido_l1 | Starting Phluido 5G-NR L1 software... phluido_l1 | PHAPI version = 0.5 (12/10/2020) phluido_l1 | L1 SW version = 0.8.1 phluido_l1 | L1 SW internal rev = r3852 phluido_l1 | Parsed configuration parameters: phluido_l1 | LicenseKey = XXXX-XXXX-XXXX-XXXX-XXXX-XXXX-XXXX-XXXX phluido_l1 | maxNumPrachDetectionSymbols = 1 phluido_l1 | maxNumPdschLayers = 2 phluido_l1 | maxNumPuschLayers = 1 phluido_l1 | maxPuschModOrder = 6 phluido_l1 | Perform these steps to get a running active cell. * When the RU is still sending traffic use ssh root@10.10.0.100 handshake to stop this traffic. * Start L1 and DU (run docker-compose up inside the install directory ). * Use wireshark to follow the CPlane traffic, at this point following sequence: DU CU | F1SetupRequest---> | | <---F1SetupResponse | | | | <---GNBCUConfigurationUpdate | | | The L1 starts listening on ip:port 10.10.0.1:44000 After less than 30 seconds communication between rru and du starts. The related fiber port will report around 100 Mbytes/second of traffic in both directions DU CU | GNBCUConfigurationUpdateAck---> | | | NOTE : type ssh root@10.10.0.100 handshake again to stop the traffic. Make sure you stop the handshake explicitly at the end of your session else, even when stopping the DU/L1 manually, the RRU will keep the link alive and the next docker-compose up will find a cell busy transmitting on the fiber and the synchronization will not happen","title":"start cell"},{"location":"du-install/#starting-ru-benetel-650-cell-wrapper-way","text":"","title":"Starting RU Benetel 650 - cell wrapper way"},{"location":"du-install/#install-cell-wrapper","text":"","title":"Install cell wrapper"},{"location":"du-install/#on-the-host","text":"To make the CU VM have access to the DU host ( bare metal server ) some privileges need to be given. printf \" $USER ALL=(ALL) NOPASSWD:ALL\\n\" | sudo tee /etc/sudoers.d/ $USER sudo usermod -aG sudo $USER","title":"On the HOST"},{"location":"du-install/#on-the-cu-vm","text":"Go to the VM. In the VM a cell wrapper will get installed that controls the DU and RU ( cell ). Going inside the CU VM. ssh $USER@$NODE_IP Add some prerequisites if it is necessary sudo apt update sudo apt install zip Create a public/private key pair and add it to kubernetes ssh-keygen -t ed25519 -f id_ed25519 -C cell-wrapper kubectl create secret generic cw-private --from-file = private = id_ed25519 kubectl create secret generic cw-public --from-file = public = id_ed25519.pub and copy the public key to the bare metal server ( DU host ) ssh-copy-id -i id_ed25519.pub ad@$SERVER_IP Create a .yaml file containing the configuration. Also fill in the values you have prepared on the first page of the install guide. It will install the cell-wrapper the will take care of the cell's health. In this configuration the cell-wrapper will reboot the RU every night at 2:00 AM. <reboot>true</reboot> mkdir -p ~/install_$INSTALL_VERSION/ cd !$ tee cw.yaml < <EOF global: instanceId: \"cw\" natsUrl: \"$NODE_IP\" natsPort: \"31100\" redisHostname: \"$NODE_IP\" redisPort: \"32220\" redis: backup: enabled: true deleteAfterDay: 7 jobs: deleteExistingData: true nats: enabled: false #jobs: # - name: reboot-ru-1 # schedule: \"0 2 * * *\" # rpc: | # <cell-wrapper xmlns= \"http://accelleran.com/ns/yang/accelleran-granny\" # xmlns:xc= \"urn:ietf:params:xml:ns:netconf:base:1.0\" xc:operation= \"replace\" > # <radio-unit xc:operation= \"replace\" > # <name> vi su-1 </name> # <reboot> true </reboot> # </radio-unit> # </cell-wrapper> netconf: netconfService: nodePort: 31832 configOnBoot: enabled: true deleteExistingConfig: true host: 'localhost' config: | <cell-wrapper xmlns= \"http://accelleran.com/ns/yang/accelleran-granny\" xmlns:xc= \"urn:ietf:params:xml:ns:netconf:base:1.0\" xc:operation= \"create\" > <admin-state> unlocked </admin-state> <ssh-key-pair xc:operation= \"create\" > <public-key> /home/accelleran/5G/ssh/public </public-key> <private-key> /home/accelleran/5G/ssh/private </private-key> </ssh-key-pair> <auto-repair xc:operation= \"create\" > <enable> true </enable> <health-check xc:operation= \"create\" > <rate xc:operation= \"create\" > <seconds> 5 </seconds> <milli-seconds> 0 </milli-seconds> </rate> <unacknowledged-counter-threshold> 3 </unacknowledged-counter-threshold> </health-check> <container-not-running-counter-threshold> 2 </container-not-running-counter-threshold> <l1-not-listening-to-ru-counter-threshold> 6 </l1-not-listening-to-ru-counter-threshold> <l1-rru-traffic-counter-threshold> 6 </l1-rru-traffic-counter-threshold> </auto-repair> <distributed-unit xc:operation= \"create\" > <name> du-1 </name> <type> effnet </type> <connection-details xc:operation= \"create\" > <host> $SERVER_IP </host> <port> 22 </port> <username> $USER </username> </connection-details> <ssh-timeout> 30 </ssh-timeout> <config xc:operation= \"create\" > <cgi-plmn-id> $PLMN_ID </cgi-plmn-id> <cgi-cell-id> 000000000000000000000000000000000001 </cgi-cell-id> <pci> $PCI_ID </pci> <tac> 000001 </tac> <arfcn> $ARFCN_POINT_A </arfcn> <frequency-band> $FREQ_BAND </frequency-band> <plmns-id> $PLMN_ID </plmns-id> <plmns-sst> 1 </plmns-sst> <l1-license-key> $L1_PHLUIDO_KEY </l1-license-key> <l1-bbu-addr> 10.10.0.1 </l1-bbu-addr> <l1-max-pusch-mod-order> 6 </l1-max-pusch-mod-order> <l1-max-num-pdsch-layers> 2 </l1-max-num-pdsch-layers> <l1-max-num-pusch-layers> 1 </l1-max-num-pusch-layers> <l1-num-workers> 8 </l1-num-workers> <l1-target-recv-delay-us> 2500 </l1-target-recv-delay-us> <l1-pucch-format0-threshold> 0.01 </l1-pucch-format0-threshold> <l1-timing-offset-threshold-nsec> 10000 </l1-timing-offset-threshold-nsec> </config> <enable-auto-repair> true </enable-auto-repair> <working-directory> /run </working-directory> <storage-directory> /var/log </storage-directory> <pcscd-socket> /run/pcscd/pcscd.comm </pcscd-socket> <enable-log-saving> false </enable-log-saving> <max-storage-disk-usage> 80% </max-storage-disk-usage> <enable-log-rotation> false </enable-log-rotation> <log-rotation-pattern> *.0 </log-rotation-pattern> <log-rotation-count> 1 </log-rotation-count> <centralized-unit-host> $F1_CU_IP </centralized-unit-host> <l1-listening-port> 44000 </l1-listening-port> <traffic-threshold xc:operation= \"create\" > <uplink> 10000 </uplink> <downlink> 10000 </downlink> </traffic-threshold> <du-image-tag> $DU_VERSION </du-image-tag> <l1-image-tag> $L1_VERSION </l1-image-tag> <du-extra-args> --cpuset-cpus=$CORE_SET_DU </du-extra-args> <l1-extra-args> --cpuset-cpus=$CORE_SET_DU </l1-extra-args> <du-base-config-file> /home/accelleran/5G/config/duEffnetConfig.json </du-base-config-file> <radio-unit xc:operation= \"create\" > ru-1 </radio-unit> </distributed-unit> <radio-unit xc:operation= \"create\" > <name> ru-1 </name> <type> benetel650 </type> <connection-details xc:operation= \"create\" > <host> 10.10.0.100 </host> <port> 22 </port> <username> root </username> </connection-details> <enable-ssh> false </enable-ssh> <ssh-timeout> 30 </ssh-timeout> </radio-unit> </cell-wrapper> EOF NOTE : uncomment the jobs: part if the RU should restart every night at 2am. Install using helm. helm repo update helm install cw acc-helm/cw-cell-wrapper --values cw.yaml Now you can see the kubernetes pods being created. Follow there progress with. watch -d kubectl get pod","title":"On the CU VM"},{"location":"du-install/#scripts-to-steer-cell-and-cell-wrapper","text":"Following script are delivered. They are located in the install_$CU_VERSION/accelleran/bin directory. The $PATH variable is set accordingly. cw-verify.sh - verifies if the cw.yaml file is parsed correctly after installation cw-enable.sh - will enable the cell-wrapper. cell-start.sh cell-stop.sh cell-restart.sh cw-disable.sh - cell-wrapper will not restar the cell when it is defect. cw-debug-on.sh - turns on more logging cw-debug-off.sh - turns on normal logging The script do what there name says","title":"scripts to steer cell and cell-wrapper"},{"location":"du-install/#verify-good-operation-of-the-b650-all-releases","text":"","title":"verify good operation of the B650 (all releases)"},{"location":"du-install/#gps","text":"See if GPS is locked root@benetelru:~# syncmon DPLL0 State (SyncE/Ethernet clock): LOCKED DPLL1 State (FPGA clocks): FREERUN DPLL2 State (FPGA clocks): FREERUN DPLL3 State (RF/PTP clock): LOCKED CLK0 SyncE LIVE: OK CLK0 SyncE STICKY: LOS + No Activity CLK2 10MHz LIVE: LOS + No Activity CLK2 10MHz STICKY: LOS + No Activity CLK5 GPS LIVE: OK CLK5 GPS STICKY: LOS and Frequency Offset CLK6 EXT 1PPS LIVE: LOS and Frequency Offset CLK6 EXT 1PPS STICKY: LOS and Frequency Offset","title":"GPS"},{"location":"du-install/#cell-status-report","text":"Verify if the boot sequence ended up correctly, by checking the radio status, the ouput shall mention explicitly the up time and the succesful bringup > NOTE : this file is not present the first minute after reboot. root@benetelru:~# cat /tmp/radio_status [INFO] Platform: RAN650_B [INFO] Radio bringup begin [INFO] Load EEPROM Data [INFO] Tx1 Attenuation set to 15000 mdB [INFO] Tx3 Attenuation set to 15730 mdB [INFO] Operating Frequency set to 3774.720 MHz [INFO] Waiting for Sync [INFO] Sync completed [INFO] Start Radio Configuration [INFO] Initialize RF IC [INFO] Disabled CFR for Antenna 1 [INFO] Disabled CFR for Antenna 3 [INFO] Move platform to TDD mode [INFO] Set CP60 as TDD control master [INFO] Enable TX on FEM [INFO] FEM to full MIMO1_3 mode [INFO] DPD Tx1 configuration [INFO] DPD Tx3 configuration [INFO] Set attn at 3774.720 MHz [INFO] Reg 0xC0366 to 0x3FF [INFO] Tuning the UE TA to reduce timing_offset [INFO] The O-RU is ready for System Integration [INFO] Radio bringup complete 15:54:47 up 4 min, load average: 0.09, 0.19, 0.08 ``` ### RU Status Report some important registers must be checked to determine if the boot sequence has completed correctly: ```bash root@benetelru:~# reportRuStatus [INFO] Sync status is: Register 0xc0367, Value : 0x1 ------------------------------- [INFO] RU Status information is: Register 0xc0306, Value : 0x470800 ------------------------------- [INFO] Fill level of Reception Window is: Register 0xc0308, Value : 0x6c12 ------------------------------- [INFO] Sample Count is: Register 0xc0311, Value : 0x56f49 ------------------------------- ============================================================ RU Status Register description: ============================================================ [31:19] not used [18] set to 1 if handshake is successful [17] set to 1 when settling time (fronthaul) has completed [16] set to 1 if symbolndex=0 was captured [15] set to 1 if payload format is invalid [14] set to 1 if symbol index error has been detected [13:12] not used [11] set to 1 if DU MAC address is correct [10:2] not used [1] Reception Window Buffer is empty [0] Reception Window Buffer is full ------------------------------------------------------------ =========================================================== [NOTE] Max buffer depth is 53424 (112 symbols, 2 antennas) =========================================================== Handshake messages are sent by the RU every second. When phluido L1 is starting or running it will Listen on port 44000 and reply to these messages. Login to the server and check if the handshakes are happening: these are short messages sent periodically from the B650 to the server DU MAC address that was set as discussed and can be seen with a simple tcp dump command on the fiber interface of your server (enp45s0f0 for this example): tcpdump -i enp45s0f0 -c 5 port 44000 -en 19:22:47.096453 02:00:5e:01:01:01 > 6c:b3:11:08:a4:e0, ethertype IPv4 (0x0800), length 64: 10.10.0.2.44000 > 10.10.0.1.44000: UDP, length 20 The above shows that 10.10.0.2 (U plane default IP address of the B650 Cell) is sending a Handshake message from the MAC address 02:00:5e:01:01:01 (default MAC address of the B650 Uplane interface) to 10.10.0.1 (Server Fiber interface IP address) on MAC 6c:b3:11:08:a4:e0 (the MAC address of that fiber interface) Such initial message may repeat a certain number of times, this is normal.","title":"Cell Status Report"},{"location":"du-install/#trace-traffic-between-ru-and-l1","text":"As said, the first packet goes out from the Radio End to the DU, this is the handshake packet. The second packet is the Handshake response of the DU and we have to make sure that as described the MAC address used in such response from the DU has been set correctly so that the DATA Interface MAC address of the Radio End is used (by default in the Benetel Radio this MAC address is 02:00:5e:01:01:01 ) When data flows the udp packet lengths are 3874. Remember we increased the MTU size to 9000. Without increasing the L1 would crash on the fragmented udp packets. $ tcpdump -i enp45s0f0 -c 20 port 44000 -en tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on enp45s0f0, link-type EN10MB (Ethernet), capture size 262144 bytes 19:22:47.096453 02:00:5e:01:01:01 > 6c:b3:11:08:a4:e0, ethertype IPv4 (0x0800), length 64: 10.10.0.2.44000 > 10.10.0.1.44000: UDP, length 20 19:22:47.106677 6c:b3:11:08:a4:e0 > 02:00:5e:01:01:01, ethertype IPv4 (0x0800), length 54: 10.10.0.1.44000 > 10.10.0.2.44000: UDP, length 12 19:23:14.596247 02:00:5e:01:01:01 > 6c:b3:11:08:a4:e0, ethertype IPv4 (0x0800), length 64: 10.10.0.2.44000 > 10.10.0.1.44000: UDP, length 12 19:23:14.596621 6c:b3:11:08:a4:e0 > 02:00:5e:01:01:01, ethertype IPv4 (0x0800), length 3874: 10.10.0.1.44000 > 10.10.0.2.44000: UDP, length 3832 19:23:14.596631 6c:b3:11:08:a4:e0 > 02:00:5e:01:01:01, ethertype IPv4 (0x0800), length 3874: 10.10.0.1.44000 > 10.10.0.2.44000: UDP, length 3832","title":"Trace traffic between RU and L1."},{"location":"du-install/#cell-is-on","text":"","title":"Cell is ON"},{"location":"du-install/#check-if-the-l1-is-listening","text":"$ while true ; do sleep 1 ; netstat -ano | grep 44000 ;echo $RANDOM; done udp 0 118272 10.10.0.1:44000 0.0.0.0:* off (0.00/0/0) 1427 udp 0 16896 10.10.0.1:44000 0.0.0.0:* off (0.00/0/0) 11962 udp 0 42240 10.10.0.1:44000 0.0.0.0:* off (0.00/0/0) 16780 udp 0 0 10.10.0.1:44000 0.0.0.0:* off (0.00/0/0) 502","title":"Check if the L1 is listening"},{"location":"du-install/#show-the-traffic-between-rru-and-l1","text":"$ ifstat -i enp45s0f0 enp45s0f0 KB/s in KB/s out 71320.01 105959.7 71313.36 105930.1","title":"Show the traffic between rru and l1"},{"location":"du-install/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"du-install/#debug-configuration","text":"To enable the L1 and DU logs to get saved after a cellwrapper intervention, install the cellwrapper with these settings. <enable-log-saving>true</enable-log-saving>","title":"DEBUG Configuration"},{"location":"du-install/#fiber-port-not-showing-up","text":"https://www.serveradminz.com/blog/unsupported-sfp-linux/","title":"Fiber Port not showing up"},{"location":"du-install/#l1-is-not-listening","text":"Check if L1 is listening on port 44000 by typing $ netstat -ano | grep 44000 If nothing is shown L1 is not listening. In this case do a trace on the F1 port like this. tcpdump -i any port 38472 18:26:30.940491 IP 10.244.0.208.38472 > bare-metal-node-cab-3.59910: sctp (1) [HB REQ] 18:26:30.940491 IP 10.244.0.208.38472 > bare-metal-node-cab-3.maas.56153: sctp (1) [HB REQ] 18:26:30.940530 IP bare-metal-node-cab-3.59910 > 10.244.0.208.38472: sctp (1) [HB ACK] 18:26:30.940532 IP bare-metal-node-cab-3.59910 > 10.244.0.208.38472: sctp (1) [HB ACK] you should see the HB REQ and ACK messages. If not Check * the docker-compose.yml file if the cu ip address matches the following bullet * check kubectl get services if the F1 service is running with the that maches previous bullet","title":"L1 is not listening"},{"location":"du-install/#check-sctp-connections","text":"There are 3 UDP ports you can check. When the system starts up it will setup 3 SCTP connections on following ports in the order mentioned here : 38462 - E1 SCTP connection - SCTP between DU and CU 38472 - F1 SCTP connection - SCTP between CU UP and CU CP 38412 - NGAP SCTP connection - SCTP between CU CP and CORE","title":"check SCTP connections"},{"location":"du-install/#appendix-engineering-tips-and-tricks","text":"","title":"Appendix: Engineering tips and tricks"},{"location":"du-install/#pcscd-debug","text":"It occurs rarely that the du software throws DU license check failed when this happens you have to recreate the docker container and try again. If this does not help increase the pcscd logging. change ENTRYPOINT [\"/usr/sbin/pcscd\", \"--foreground\"] into ENTRYPOINT [\"/usr/sbin/pcscd\", \"-d --foreground\"] and use docker logs on the container to see more logging about what pcscd is doing","title":"pcscd debug"},{"location":"du-install/#run-ru-in-freerun-mode","text":"This is the mode where it does not need a GPS sync. By default a benetel only boots when a GPS signal is present which the RU can be synced with. The boot process indicated this with ``Waiting for Sync in the /tmp/logs/radio_status``` file The following steps make the benetel boot without needing GPS signal. At boottime you kill the syncmon process killall syncmon and set the sync-state to succesfull manually echo 0 > /var/syncmon/sync-state Now the boot process will continue. Wait at least a minute.","title":"Run RU in freerun mode"},{"location":"du-install/#custatus","text":"","title":"custatus"},{"location":"du-install/#install","text":"unzip custatus.zip so you get create a directory $HOME/5g-engineering/utilities/custatus sudo apt install tmux create the .tmux.conf file with following content. cat $HOME/.tmux.conf set -g mouse on bind q killw add this line in $HOME/.profile export PATH=$HOME/5g-engineering/utilities/custatus:$PATH","title":"install"},{"location":"du-install/#use","text":"to start custatus.sh tmux to quit * type \"CTRL-b\" followed by \"q\" NOTE : you might need to quit the first time you have started. Start a second time and see the difference.","title":"use"},{"location":"du-install/#example","text":"","title":"example"},{"location":"kubernetes-install/","text":"CU installation ( VM & kubernetes ) \u00b6 Introduction \u00b6 This chapter will install the CU, using Flannel for the CNI. This guide defaults to using Docker as the container runtime. This chapter will guide you through following steps : install VM ( with cpu pinning ) install docker in the CU VM install kubernetes in the CU VM VM Minimum Requirements \u00b6 8 dedicated Cores ( cpuset planned in the preperation chapter ) 32GB DDR4 RAM 200GB Hard Disk ( includes space for logging/monitor/debugging the system ) Configure HOST server \u00b6 set a linux bridge \u00b6 create a linux bridge using netplan adapt your netplan file assuming that $SERVER_INT holds the physical interface name of your server that connects to the network. network: ethernets: $SERVER_INT : dhcp4: false : : bridges: br0: interfaces: [ $SERVER_INT ] addresses: - $SERVER_IP /24 gateway4: $GATEWAY_IP nameservers: addresses: [ 8 .8.8.8 ] version: 2 on the host uncomment the line in /etc/sysctl.conf so you get this. net.ipv4.ip_forward = 1 reboot the host. Install VM \u00b6 If not yet installed install sudo apt install virtinst sudo apt install libvirt-clients sudo apt install qemu sudo apt install qemu-kvm sudo apt install libvirt_daemon_system sudo apt install bridge-utils sudo apt install virt-manager reboot server Below a command line that creates a VM with the correct settings. IMPORTANT ! the $CORE_SET_CU can only be a comma seperated list. sudo virt-install --name \" $CU_VM_NAME \" --memory 16768 --vcpus \"sockets=1,cores= $CORE_AMOUNT_CU ,cpuset= $CORE_SET_CU \" --os-type linux --os-variant rhel7.0 --accelerate --disk \"/var/lib/libvirt/images/CU-ubuntu-20.04.4-live-server-amd64.img,device=disk,size=100,sparse=yes,cache=none,format=qcow2,bus=virtio\" --network \"source=br0,type=bridge\" --vnc --noautoconsole --cdrom \"./ubuntu-20.04.4-live-server-amd64.iso\" --console pty,target_type = virtio some notes about this command * --noautoconsole : if you ommit this, a graphical console window will popup. This works only when the remote server can export its graphical UI to your local graphical environment like an X-windows * --console pty,target_type=virtio will make sure you can use virsh console $CU_VM_NAME Continue in the console the complete the VM installation. console using command line \u00b6 virsh console $CU_VM_NAME NOTE ! This can take a few minutes before you see something appearing console using virt-manager \u00b6 start on your local machine virt-manager. connect to the remote baremetal server using the ip $SERVER_IP. You will see the virtual machine $CU_VM_NAME listed. double click it and proceed. screen 1 - basic mode \u00b6 select basic mode ================================================================================ Serial [ Help ] ================================================================================ As the installer is running on a serial console, it has started in basic mode, using only the ASCII character set and black and white colours. If you are connecting from a terminal emulator such as gnome-terminal that supports unicode and rich colours you can switch to \"rich mode\" which uses unicode, colours and supports many languages. You can also connect to the installer over the network via SSH, which will allow use of rich mode. [ Continue in rich mode > ] [ Continue in basic mode > ] [ View SSH instructions ] screen 2 - Continue without updating \u00b6 select \"Continue without updating\" ================================================================================ Installer update available [ Help ] ================================================================================ Version 22.07.2 of the installer is now available (22.02.2 is currently running). You can read the release notes for each version at: https://github.com/canonical/subiquity/releases If you choose to update, the update will be downloaded and the installation will continue from here. [ Update to the new installer ] [ Continue without updating ] [ Back ] screen 3 - English US keyboard \u00b6 select Engligh US keyboard ================================================================================ Keyboard configuration [ Help ] ================================================================================ Please select the layout of the keyboard directly attached to the system, if any. Layout: [ English (US) v ] Variant: [ English (US) v ] [ Done ] [ Back ] screen 4 and 5 - Set static ip \u00b6 select Edit IPv4 select Manual Set the subnet, ip, gateway and name servers in the next screen select Done ================================================================================ Network connections [ Help ] ================================================================================ Configure at least one interface this server can use to talk to other machines, and which preferably provides sufficient access for updates. NAME TYPE NOTES \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 [ ens3 eth - >\u2502< (close) \u2502 static 10.22.11.148/24 \u2502 Info >\u2502 52:54:00:68:47:29 / Red Hat, Inc\u2502 Edit IPv4 >\u2502vice \u2502 Edit IPv6 >\u2502 [ Create bond > ] \u2502 Add a VLAN tag >\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 [ Done ] [ Back ] ================================================================================ Network connections [ Help ] ================================================================================ \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Edit ens3 IPv4 configuration \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 IPv4 Method: [ Manual v ] ^ \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 Subnet: $NODE_SUBNET (fill in the value here) \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 Address: $NODE_IP (fill in the value here) \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 | \u2502 \u2502 Gateway: $GATEWAY_IP ( fill in the value here ) | \u2502 \u2502 | \u2502 \u2502 Name servers: 8.8.8.8 \u2502 \u2502 \u2502 IP addresses, comma separated \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 Search domains: \u2502 \u2502 \u2502 Domains, comma separated v \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 [ Save ] \u2502 \u2502 [ Cancel ] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 screen 6 - proxy \u00b6 Select Done ================================================================================ Configure proxy [ Help ] ================================================================================ If this system requires a proxy to connect to the internet, enter its details here. Proxy address: If you need to use a HTTP proxy to access the outside world, enter the proxy information here. Otherwise, leave this blank. The proxy information should be given in the standard form of \"http://[[user][:pass]@]host[:port]/\". [ Done ] [ Back ] screen 7 - archive mirror \u00b6 select Done ================================================================================ Configure Ubuntu archive mirror [ Help ] ================================================================================ If you use an alternative mirror for Ubuntu, enter its details here. Mirror address: http://be.archive.ubuntu.com/ubuntu You may provide an archive mirror that will be used instead of the default. [ Done ] [ Back ] screen 8 and 9 - storage configuration \u00b6 select Done select Done ================================================================================ Guided storage configuration [ Help ] ================================================================================ Configure a guided storage layout, or create a custom one: (X) Use an entire disk [ /dev/vda local disk 256.000G v ] [X] Set up this disk as an LVM group [ ] Encrypt the LVM group with LUKS Passphrase: Confirm passphrase: ( ) Custom storage layout [ Done ] [ Back ] ================================================================================ Storage configuration [ Help ] ================================================================================ FILE SYSTEM SUMMARY ^ \u2502 MOUNT POINT SIZE TYPE DEVICE TYPE \u2502 [ / 100.000G new ext4 new LVM logical volume > ] \u2502 [ /boot 1.500G new ext4 new partition of local disk > ] \u2502 \u2502 \u2502 AVAILABLE DEVICES \u2502 \u2502 DEVICE TYPE SIZE [ ubuntu-vg (new) LVM volume group 254.496G > ] free space 154.496G > [ Create software RAID (md) > ] [ Create volume group (LVM) > ] v [ Done ] [ Reset ] [ Back ] screen 10 and 11 - are you sure \u00b6 select Continue \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Confirm destructive action \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Selecting Continue below will begin the installation process and \u2502 \u2502 result in the loss of data on the disks selected to be formatted. \u2502 \u2502 \u2502 \u2502 You will not be able to return to this or a previous screen once the \u2502 \u2502 installation has started. \u2502 \u2502 \u2502 \u2502 Are you sure you want to continue? \u2502 \u2502 \u2502 \u2502 [ No ] \u2502 \u2502 [ Continue ] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 screen 12 - profile setup \u00b6 enter your name, the name of the person that does this installation enter the server name $CU_VM_NAME enter the username $USER enter the password Select Done Profile setup [ Help ] \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 Enter the username and password you will use to log in to the system. You can configure SSH access on the next screen but a password is still needed for sudo. Your name: Dennis Your server's name: $CU_HOSTNAME (put here the value from variable) Pick a username: $USER ( put here the value from variable ) Choose a password: ********* Confirm your password: ********* [ Done ] screen 13 - enable ubuntu advantage \u00b6 select Done ================================================================================ Enable Ubuntu Advantage [ Help ] ================================================================================ Enter your Ubuntu Advantage token if you want to enroll this system. Ubuntu Advantage token: If you want to enroll this system using your Ubuntu Advantage subscription, enter your Ubuntu Advantage token here. Otherwise, leave this blank. [ Done ] [ Back ] screen 14 - install openSSH server \u00b6 select Install openSSH server ================================================================================ SSH Setup [ Help ] ================================================================================ You can choose to install the OpenSSH server package to enable secure remote access to your server. [X] Install OpenSSH server Import SSH identity: [ No v ] You can import your SSH keys from GitHub or Launchpad. Import Username: [X] Allow password authentication over SSH [ Done ] [ Back ] screen 15 - Featured server snaps \u00b6 don't select any extra feature select Done ================================================================================ Featured Server Snaps [ Help ] ================================================================================ These are popular snaps in server environments. Select or deselect with SPACE, press ENTER to see more details of the package, publisher and versions available. [ ] microk8s Kubernetes for workstations and appliances >^ [ ] nextcloud Nextcloud Server - A safe home for all your data >\u2502 [ ] wekan The open-source kanban >\u2502 [ ] kata-containers Build lightweight VMs that seamlessly plug into t >\u2502 [ ] docker Docker container runtime >\u2502 [ ] canonical-livepatch Canonical Livepatch Client >\u2502 [ ] rocketchat-server Rocket.Chat server >\u2502 [ ] mosquitto Eclipse Mosquitto MQTT broker > [ ] etcd Resilient key-value store by CoreOS > [ ] powershell PowerShell for every system! > [ ] stress-ng tool to load and stress a computer > [ ] sabnzbd SABnzbd > [ ] wormhole get things from one computer to another, safely >v [ Done ] [ Back ] screen 16 - installation starts \u00b6 wait like around 5 minutes for the installation to complete ================================================================================ Installing system [ Help ] ================================================================================ \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 configuring iscsi service \u2502 \u2502 configuring raid (mdadm) service \u2502 \u2502 installing kernel \u2502 \u2502 setting up swap \u2502 \u2502 apply networking config \u2502 \u2502 writing etc/fstab \u2502 \u2502 configuring multipath \u2502 \u2502 updating packages on target system \u2502 \u2502 configuring pollinate user-agent on target \u2502 \u2502 updating initramfs configuration \u2502 \u2502 configuring target system bootloader \u2502 \u2502 installing grub to target devices \u2502 \u2502 finalizing installation \u2502 \u2502 running 'curtin hook' \u2502 \u2502 curtin command hook \u2502 \u2502 executing late commands / \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 [ View full log ] screen 17 - install complete \u00b6 select reboot now ================================================================================ Install complete! [ Help ] ================================================================================ \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 finalizing installation ^\u2502 \u2502 running 'curtin hook' \u2502 \u2502 curtin command hook \u2502 \u2502 executing late commands \u2502 \u2502final system configuration \u2502 \u2502 configuring cloud-init \u2502 \u2502 calculating extra packages to install \u2502 \u2502 installing openssh-server \u2502 \u2502 curtin command system-install \u2502 \u2502 downloading and installing security updates \u2502 \u2502 curtin command in-target \u2502 \u2502 restoring apt configuration \u2502 \u2502 curtin command in-target \u2502 \u2502 curtin command in-target \u2502\u2502 \u2502subiquity/Late/run v\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 [ View full log ] [ Reboot Now ] Wait untill you can click reboot server NOTE : if after some minutes the server has not rebooted yet you have to reboot it forcefully like this. Most likely the cdrom fails to unmount. virsh reset $CU_VM_NAME copy the install directory from the HOST to this newly created VM cd ; scp -r install_ $CU_VERSION $USER @ $NODE_IP : cd ; scp .profile $USER @ $NODE_IP : ssh into the VM. ssh $USER @ $NODE_IP from inside this VM you should be able to ping the internet's ip address 8.8.8.8 ping 8 .8.8.8 make sure all available disk space is being used inside the VM. lsblk sudo lvextend -r -l +100%FREE /dev/mapper/ubuntu--vg-ubuntu--lv lsblk Every heading that follows has to be done inside this VM. Install Docker in the CU VM \u00b6 Add the Docker APT repository: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt update Install the required packages: sudo apt install docker-ce sudo apt install docker-ce-cli sudo apt install containerd.io sudo apt install docker-compose rerun the .profile . profile Add your user to the docker group to be able to run docker commands without sudo access. now all the variable values sudo usermod -aG docker $USER sudo reboot To check if your installation is working you can try to run a test image in a container: docker run hello-world Configure Docker Daemon \u00b6 The recommended configuration of the Docker daemon is provided by the Kubernetes team - particularly to use systemd for the management of the container's cgroups: sudo mkdir /etc/docker sudo tee /etc/docker/daemon.json <<EOF { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF Restart Docker and enable on boot: sudo systemctl enable docker sudo systemctl daemon-reload sudo systemctl restart docker Disable Swap \u00b6 Kubernetes refuses to run if swap is enabled on the node, so we disable swap immediately and then also disable it following a reboot: sudo swapoff -a sudo sed -i '/\\sswap\\s/ s/^\\(.*\\)$/#\\1/g' /etc/fstab Install Kubernetes inside the VM \u00b6 Add the Kubernetes APT repository: sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt update Accelleran dRAX currently supports Kubernetes up to version 1.20. The following command installs specifically this version: sudo apt install -y kubelet = 1 .20.0-00 kubeadm = 1 .20.0-00 kubectl = 1 .20.0-00 sudo apt-mark hold kubelet kubeadm kubectl Configure Kubernetes \u00b6 To initialize the Kubernetes cluster, the IP address of the node needs to be fixed, i.e. if this IP changes, a full re-installation of Kubernetes will be required. This is generally the (primary) IP address of the network interface associated with the default gateway. From here on, this IP is referred to as $NODE_IP - this shell variable has been stored in the first page: NOTE : in this part these variables will be used. export NODE_IP # See Preperation paragraph for correct ip export POD_NETWORK This guide assumes we will use Flannel as the CNI-based Pod network for this Kubernetes instance, which uses the 10.244.0.0/16 subnet by default. We store it again as an environment variable for later use, and of course if you wish to use a different subnet, change the command accordingly: The following command initializes the cluster on this node: sudo kubeadm init --pod-network-cidr = $POD_NETWORK --apiserver-advertise-address = $NODE_IP If this succeeds, we should see information for joining other worker nodes to this cluster. We won't do that at this point, but it's a sign that the command completed successfully. To make kubectl work for our non-root user, run the following commands: mkdir -p \" $HOME /.kube\" sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown \" $( id -u ) : $( id -g ) \" \" $HOME /.kube/config\" Install Flannel \u00b6 Prepare the Manifest file: curl -sSOJ https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml sed -i '/net-conf.json/,/}/{ s#10.244.0.0/16#' \" $POD_NETWORK \" '#; }' kube-flannel.yml Apply the Manifest file: kubectl apply -f kube-flannel.yml Enable Pod Scheduling \u00b6 By default, Kubernetes will not schedule Pods on the control-plane node for security reasons. As we're running with a single Node, we need to remove the node-role.kubernetes.io/master taint, meaning that the scheduler will then be able to schedule Pods on it. kubectl taint nodes --all node-role.kubernetes.io/master- A small busybox pod for testing \u00b6 It is very convenient (however optional) to test the Kubernetes installation with a simple busybox pod for instance to test your DNS resolution inside a pod. To do so create the following yaml file (/tmp/busybox.yaml): cat << EOF > /tmp/busybox.yaml apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: - name: busybox image: busybox:1.28 command: - sleep - \"3600\" imagePullPolicy: IfNotPresent restartPolicy: Always EOF Then you can create the pod: NOTE : --kubeconfig is optional here because $HOME/.kube/config is the default config file kubectl --kubeconfig $HOME /.kube/config create -f /tmp/busybox.yaml If all went well a new POD was created, you can verify this with the following command kubectl --kubeconfig $HOME /.kube/config get pods #NAME READY STATUS RESTARTS AGE #busybox 1/1 Running 21 21h In order to verify if your Kubernetes is working correctly you could try some simple commands using the busybox POD. For instance to verify your name resolution works do: kubectl exec -ti busybox -- nslookup mirrors.ubuntu.com #Server: 10.96.0.10 #Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local #Name: mirrors.ubuntu.com #Address 1: 91.189.89.32 bilimbi.canonical.com APENDIX : Remove a full Kubernetes installation \u00b6 On occasion, it may be deemed necessary to fully remove Kubernetes, for instance if for any reason your server IP address will change, then the advertised Kubernetes IP address will have to follow. THe following command help making sure the previous installation is cleared up: sudo kubeadm reset sudo apt-get purge kubeadm sudo apt-get purge kubectl sudo apt-get purge kubelet sudo apt-get purge kubernetes-cni sudo rm -rf ~/.kube sudo rm -rf /etc/cni/net.d sudo ip link delete cni0 sudo ip link delete flannel.1","title":"CU installation ( VM & kubernetes )"},{"location":"kubernetes-install/#cu-installation-vm-kubernetes","text":"","title":"CU installation ( VM &amp; kubernetes )"},{"location":"kubernetes-install/#introduction","text":"This chapter will install the CU, using Flannel for the CNI. This guide defaults to using Docker as the container runtime. This chapter will guide you through following steps : install VM ( with cpu pinning ) install docker in the CU VM install kubernetes in the CU VM","title":"Introduction"},{"location":"kubernetes-install/#vm-minimum-requirements","text":"8 dedicated Cores ( cpuset planned in the preperation chapter ) 32GB DDR4 RAM 200GB Hard Disk ( includes space for logging/monitor/debugging the system )","title":"VM Minimum Requirements"},{"location":"kubernetes-install/#configure-host-server","text":"","title":"Configure HOST server"},{"location":"kubernetes-install/#set-a-linux-bridge","text":"create a linux bridge using netplan adapt your netplan file assuming that $SERVER_INT holds the physical interface name of your server that connects to the network. network: ethernets: $SERVER_INT : dhcp4: false : : bridges: br0: interfaces: [ $SERVER_INT ] addresses: - $SERVER_IP /24 gateway4: $GATEWAY_IP nameservers: addresses: [ 8 .8.8.8 ] version: 2 on the host uncomment the line in /etc/sysctl.conf so you get this. net.ipv4.ip_forward = 1 reboot the host.","title":"set a linux bridge"},{"location":"kubernetes-install/#install-vm","text":"If not yet installed install sudo apt install virtinst sudo apt install libvirt-clients sudo apt install qemu sudo apt install qemu-kvm sudo apt install libvirt_daemon_system sudo apt install bridge-utils sudo apt install virt-manager reboot server Below a command line that creates a VM with the correct settings. IMPORTANT ! the $CORE_SET_CU can only be a comma seperated list. sudo virt-install --name \" $CU_VM_NAME \" --memory 16768 --vcpus \"sockets=1,cores= $CORE_AMOUNT_CU ,cpuset= $CORE_SET_CU \" --os-type linux --os-variant rhel7.0 --accelerate --disk \"/var/lib/libvirt/images/CU-ubuntu-20.04.4-live-server-amd64.img,device=disk,size=100,sparse=yes,cache=none,format=qcow2,bus=virtio\" --network \"source=br0,type=bridge\" --vnc --noautoconsole --cdrom \"./ubuntu-20.04.4-live-server-amd64.iso\" --console pty,target_type = virtio some notes about this command * --noautoconsole : if you ommit this, a graphical console window will popup. This works only when the remote server can export its graphical UI to your local graphical environment like an X-windows * --console pty,target_type=virtio will make sure you can use virsh console $CU_VM_NAME Continue in the console the complete the VM installation.","title":"Install VM"},{"location":"kubernetes-install/#console-using-command-line","text":"virsh console $CU_VM_NAME NOTE ! This can take a few minutes before you see something appearing","title":"console using command line"},{"location":"kubernetes-install/#console-using-virt-manager","text":"start on your local machine virt-manager. connect to the remote baremetal server using the ip $SERVER_IP. You will see the virtual machine $CU_VM_NAME listed. double click it and proceed.","title":"console using virt-manager"},{"location":"kubernetes-install/#screen-1-basic-mode","text":"select basic mode ================================================================================ Serial [ Help ] ================================================================================ As the installer is running on a serial console, it has started in basic mode, using only the ASCII character set and black and white colours. If you are connecting from a terminal emulator such as gnome-terminal that supports unicode and rich colours you can switch to \"rich mode\" which uses unicode, colours and supports many languages. You can also connect to the installer over the network via SSH, which will allow use of rich mode. [ Continue in rich mode > ] [ Continue in basic mode > ] [ View SSH instructions ]","title":"screen 1 - basic mode"},{"location":"kubernetes-install/#screen-2-continue-without-updating","text":"select \"Continue without updating\" ================================================================================ Installer update available [ Help ] ================================================================================ Version 22.07.2 of the installer is now available (22.02.2 is currently running). You can read the release notes for each version at: https://github.com/canonical/subiquity/releases If you choose to update, the update will be downloaded and the installation will continue from here. [ Update to the new installer ] [ Continue without updating ] [ Back ]","title":"screen 2 - Continue without updating"},{"location":"kubernetes-install/#screen-3-english-us-keyboard","text":"select Engligh US keyboard ================================================================================ Keyboard configuration [ Help ] ================================================================================ Please select the layout of the keyboard directly attached to the system, if any. Layout: [ English (US) v ] Variant: [ English (US) v ] [ Done ] [ Back ]","title":"screen 3 - English US keyboard"},{"location":"kubernetes-install/#screen-4-and-5-set-static-ip","text":"select Edit IPv4 select Manual Set the subnet, ip, gateway and name servers in the next screen select Done ================================================================================ Network connections [ Help ] ================================================================================ Configure at least one interface this server can use to talk to other machines, and which preferably provides sufficient access for updates. NAME TYPE NOTES \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 [ ens3 eth - >\u2502< (close) \u2502 static 10.22.11.148/24 \u2502 Info >\u2502 52:54:00:68:47:29 / Red Hat, Inc\u2502 Edit IPv4 >\u2502vice \u2502 Edit IPv6 >\u2502 [ Create bond > ] \u2502 Add a VLAN tag >\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 [ Done ] [ Back ] ================================================================================ Network connections [ Help ] ================================================================================ \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Edit ens3 IPv4 configuration \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 IPv4 Method: [ Manual v ] ^ \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 Subnet: $NODE_SUBNET (fill in the value here) \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 Address: $NODE_IP (fill in the value here) \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 | \u2502 \u2502 Gateway: $GATEWAY_IP ( fill in the value here ) | \u2502 \u2502 | \u2502 \u2502 Name servers: 8.8.8.8 \u2502 \u2502 \u2502 IP addresses, comma separated \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 Search domains: \u2502 \u2502 \u2502 Domains, comma separated v \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 [ Save ] \u2502 \u2502 [ Cancel ] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"screen 4 and 5 - Set static ip"},{"location":"kubernetes-install/#screen-6-proxy","text":"Select Done ================================================================================ Configure proxy [ Help ] ================================================================================ If this system requires a proxy to connect to the internet, enter its details here. Proxy address: If you need to use a HTTP proxy to access the outside world, enter the proxy information here. Otherwise, leave this blank. The proxy information should be given in the standard form of \"http://[[user][:pass]@]host[:port]/\". [ Done ] [ Back ]","title":"screen 6 - proxy"},{"location":"kubernetes-install/#screen-7-archive-mirror","text":"select Done ================================================================================ Configure Ubuntu archive mirror [ Help ] ================================================================================ If you use an alternative mirror for Ubuntu, enter its details here. Mirror address: http://be.archive.ubuntu.com/ubuntu You may provide an archive mirror that will be used instead of the default. [ Done ] [ Back ]","title":"screen 7 - archive mirror"},{"location":"kubernetes-install/#screen-8-and-9-storage-configuration","text":"select Done select Done ================================================================================ Guided storage configuration [ Help ] ================================================================================ Configure a guided storage layout, or create a custom one: (X) Use an entire disk [ /dev/vda local disk 256.000G v ] [X] Set up this disk as an LVM group [ ] Encrypt the LVM group with LUKS Passphrase: Confirm passphrase: ( ) Custom storage layout [ Done ] [ Back ] ================================================================================ Storage configuration [ Help ] ================================================================================ FILE SYSTEM SUMMARY ^ \u2502 MOUNT POINT SIZE TYPE DEVICE TYPE \u2502 [ / 100.000G new ext4 new LVM logical volume > ] \u2502 [ /boot 1.500G new ext4 new partition of local disk > ] \u2502 \u2502 \u2502 AVAILABLE DEVICES \u2502 \u2502 DEVICE TYPE SIZE [ ubuntu-vg (new) LVM volume group 254.496G > ] free space 154.496G > [ Create software RAID (md) > ] [ Create volume group (LVM) > ] v [ Done ] [ Reset ] [ Back ]","title":"screen 8 and 9 - storage configuration"},{"location":"kubernetes-install/#screen-10-and-11-are-you-sure","text":"select Continue \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Confirm destructive action \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Selecting Continue below will begin the installation process and \u2502 \u2502 result in the loss of data on the disks selected to be formatted. \u2502 \u2502 \u2502 \u2502 You will not be able to return to this or a previous screen once the \u2502 \u2502 installation has started. \u2502 \u2502 \u2502 \u2502 Are you sure you want to continue? \u2502 \u2502 \u2502 \u2502 [ No ] \u2502 \u2502 [ Continue ] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"screen 10 and 11 - are you sure"},{"location":"kubernetes-install/#screen-12-profile-setup","text":"enter your name, the name of the person that does this installation enter the server name $CU_VM_NAME enter the username $USER enter the password Select Done Profile setup [ Help ] \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 Enter the username and password you will use to log in to the system. You can configure SSH access on the next screen but a password is still needed for sudo. Your name: Dennis Your server's name: $CU_HOSTNAME (put here the value from variable) Pick a username: $USER ( put here the value from variable ) Choose a password: ********* Confirm your password: ********* [ Done ]","title":"screen 12 - profile setup"},{"location":"kubernetes-install/#screen-13-enable-ubuntu-advantage","text":"select Done ================================================================================ Enable Ubuntu Advantage [ Help ] ================================================================================ Enter your Ubuntu Advantage token if you want to enroll this system. Ubuntu Advantage token: If you want to enroll this system using your Ubuntu Advantage subscription, enter your Ubuntu Advantage token here. Otherwise, leave this blank. [ Done ] [ Back ]","title":"screen 13 - enable ubuntu advantage"},{"location":"kubernetes-install/#screen-14-install-openssh-server","text":"select Install openSSH server ================================================================================ SSH Setup [ Help ] ================================================================================ You can choose to install the OpenSSH server package to enable secure remote access to your server. [X] Install OpenSSH server Import SSH identity: [ No v ] You can import your SSH keys from GitHub or Launchpad. Import Username: [X] Allow password authentication over SSH [ Done ] [ Back ]","title":"screen 14 - install openSSH server"},{"location":"kubernetes-install/#screen-15-featured-server-snaps","text":"don't select any extra feature select Done ================================================================================ Featured Server Snaps [ Help ] ================================================================================ These are popular snaps in server environments. Select or deselect with SPACE, press ENTER to see more details of the package, publisher and versions available. [ ] microk8s Kubernetes for workstations and appliances >^ [ ] nextcloud Nextcloud Server - A safe home for all your data >\u2502 [ ] wekan The open-source kanban >\u2502 [ ] kata-containers Build lightweight VMs that seamlessly plug into t >\u2502 [ ] docker Docker container runtime >\u2502 [ ] canonical-livepatch Canonical Livepatch Client >\u2502 [ ] rocketchat-server Rocket.Chat server >\u2502 [ ] mosquitto Eclipse Mosquitto MQTT broker > [ ] etcd Resilient key-value store by CoreOS > [ ] powershell PowerShell for every system! > [ ] stress-ng tool to load and stress a computer > [ ] sabnzbd SABnzbd > [ ] wormhole get things from one computer to another, safely >v [ Done ] [ Back ]","title":"screen 15 - Featured server snaps"},{"location":"kubernetes-install/#screen-16-installation-starts","text":"wait like around 5 minutes for the installation to complete ================================================================================ Installing system [ Help ] ================================================================================ \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 configuring iscsi service \u2502 \u2502 configuring raid (mdadm) service \u2502 \u2502 installing kernel \u2502 \u2502 setting up swap \u2502 \u2502 apply networking config \u2502 \u2502 writing etc/fstab \u2502 \u2502 configuring multipath \u2502 \u2502 updating packages on target system \u2502 \u2502 configuring pollinate user-agent on target \u2502 \u2502 updating initramfs configuration \u2502 \u2502 configuring target system bootloader \u2502 \u2502 installing grub to target devices \u2502 \u2502 finalizing installation \u2502 \u2502 running 'curtin hook' \u2502 \u2502 curtin command hook \u2502 \u2502 executing late commands / \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 [ View full log ]","title":"screen 16 - installation starts"},{"location":"kubernetes-install/#screen-17-install-complete","text":"select reboot now ================================================================================ Install complete! [ Help ] ================================================================================ \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 finalizing installation ^\u2502 \u2502 running 'curtin hook' \u2502 \u2502 curtin command hook \u2502 \u2502 executing late commands \u2502 \u2502final system configuration \u2502 \u2502 configuring cloud-init \u2502 \u2502 calculating extra packages to install \u2502 \u2502 installing openssh-server \u2502 \u2502 curtin command system-install \u2502 \u2502 downloading and installing security updates \u2502 \u2502 curtin command in-target \u2502 \u2502 restoring apt configuration \u2502 \u2502 curtin command in-target \u2502 \u2502 curtin command in-target \u2502\u2502 \u2502subiquity/Late/run v\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 [ View full log ] [ Reboot Now ] Wait untill you can click reboot server NOTE : if after some minutes the server has not rebooted yet you have to reboot it forcefully like this. Most likely the cdrom fails to unmount. virsh reset $CU_VM_NAME copy the install directory from the HOST to this newly created VM cd ; scp -r install_ $CU_VERSION $USER @ $NODE_IP : cd ; scp .profile $USER @ $NODE_IP : ssh into the VM. ssh $USER @ $NODE_IP from inside this VM you should be able to ping the internet's ip address 8.8.8.8 ping 8 .8.8.8 make sure all available disk space is being used inside the VM. lsblk sudo lvextend -r -l +100%FREE /dev/mapper/ubuntu--vg-ubuntu--lv lsblk Every heading that follows has to be done inside this VM.","title":"screen 17 - install complete"},{"location":"kubernetes-install/#install-docker-in-the-cu-vm","text":"Add the Docker APT repository: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt update Install the required packages: sudo apt install docker-ce sudo apt install docker-ce-cli sudo apt install containerd.io sudo apt install docker-compose rerun the .profile . profile Add your user to the docker group to be able to run docker commands without sudo access. now all the variable values sudo usermod -aG docker $USER sudo reboot To check if your installation is working you can try to run a test image in a container: docker run hello-world","title":"Install Docker in the CU VM"},{"location":"kubernetes-install/#configure-docker-daemon","text":"The recommended configuration of the Docker daemon is provided by the Kubernetes team - particularly to use systemd for the management of the container's cgroups: sudo mkdir /etc/docker sudo tee /etc/docker/daemon.json <<EOF { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF Restart Docker and enable on boot: sudo systemctl enable docker sudo systemctl daemon-reload sudo systemctl restart docker","title":"Configure Docker Daemon"},{"location":"kubernetes-install/#disable-swap","text":"Kubernetes refuses to run if swap is enabled on the node, so we disable swap immediately and then also disable it following a reboot: sudo swapoff -a sudo sed -i '/\\sswap\\s/ s/^\\(.*\\)$/#\\1/g' /etc/fstab","title":"Disable Swap"},{"location":"kubernetes-install/#install-kubernetes-inside-the-vm","text":"Add the Kubernetes APT repository: sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt update Accelleran dRAX currently supports Kubernetes up to version 1.20. The following command installs specifically this version: sudo apt install -y kubelet = 1 .20.0-00 kubeadm = 1 .20.0-00 kubectl = 1 .20.0-00 sudo apt-mark hold kubelet kubeadm kubectl","title":"Install Kubernetes inside the VM"},{"location":"kubernetes-install/#configure-kubernetes","text":"To initialize the Kubernetes cluster, the IP address of the node needs to be fixed, i.e. if this IP changes, a full re-installation of Kubernetes will be required. This is generally the (primary) IP address of the network interface associated with the default gateway. From here on, this IP is referred to as $NODE_IP - this shell variable has been stored in the first page: NOTE : in this part these variables will be used. export NODE_IP # See Preperation paragraph for correct ip export POD_NETWORK This guide assumes we will use Flannel as the CNI-based Pod network for this Kubernetes instance, which uses the 10.244.0.0/16 subnet by default. We store it again as an environment variable for later use, and of course if you wish to use a different subnet, change the command accordingly: The following command initializes the cluster on this node: sudo kubeadm init --pod-network-cidr = $POD_NETWORK --apiserver-advertise-address = $NODE_IP If this succeeds, we should see information for joining other worker nodes to this cluster. We won't do that at this point, but it's a sign that the command completed successfully. To make kubectl work for our non-root user, run the following commands: mkdir -p \" $HOME /.kube\" sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown \" $( id -u ) : $( id -g ) \" \" $HOME /.kube/config\"","title":"Configure Kubernetes"},{"location":"kubernetes-install/#install-flannel","text":"Prepare the Manifest file: curl -sSOJ https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml sed -i '/net-conf.json/,/}/{ s#10.244.0.0/16#' \" $POD_NETWORK \" '#; }' kube-flannel.yml Apply the Manifest file: kubectl apply -f kube-flannel.yml","title":"Install Flannel"},{"location":"kubernetes-install/#enable-pod-scheduling","text":"By default, Kubernetes will not schedule Pods on the control-plane node for security reasons. As we're running with a single Node, we need to remove the node-role.kubernetes.io/master taint, meaning that the scheduler will then be able to schedule Pods on it. kubectl taint nodes --all node-role.kubernetes.io/master-","title":"Enable Pod Scheduling"},{"location":"kubernetes-install/#a-small-busybox-pod-for-testing","text":"It is very convenient (however optional) to test the Kubernetes installation with a simple busybox pod for instance to test your DNS resolution inside a pod. To do so create the following yaml file (/tmp/busybox.yaml): cat << EOF > /tmp/busybox.yaml apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: - name: busybox image: busybox:1.28 command: - sleep - \"3600\" imagePullPolicy: IfNotPresent restartPolicy: Always EOF Then you can create the pod: NOTE : --kubeconfig is optional here because $HOME/.kube/config is the default config file kubectl --kubeconfig $HOME /.kube/config create -f /tmp/busybox.yaml If all went well a new POD was created, you can verify this with the following command kubectl --kubeconfig $HOME /.kube/config get pods #NAME READY STATUS RESTARTS AGE #busybox 1/1 Running 21 21h In order to verify if your Kubernetes is working correctly you could try some simple commands using the busybox POD. For instance to verify your name resolution works do: kubectl exec -ti busybox -- nslookup mirrors.ubuntu.com #Server: 10.96.0.10 #Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local #Name: mirrors.ubuntu.com #Address 1: 91.189.89.32 bilimbi.canonical.com","title":"A small busybox pod for testing"},{"location":"kubernetes-install/#apendix-remove-a-full-kubernetes-installation","text":"On occasion, it may be deemed necessary to fully remove Kubernetes, for instance if for any reason your server IP address will change, then the advertised Kubernetes IP address will have to follow. THe following command help making sure the previous installation is cleared up: sudo kubeadm reset sudo apt-get purge kubeadm sudo apt-get purge kubectl sudo apt-get purge kubelet sudo apt-get purge kubernetes-cni sudo rm -rf ~/.kube sudo rm -rf /etc/cni/net.d sudo ip link delete cni0 sudo ip link delete flannel.1","title":"APENDIX : Remove a full Kubernetes installation"},{"location":"kubernetes-install-1-24/","text":"Kubernetes Installation (v1.24) \u00b6 This chapter will install Kubernetes, using Flannel for the CNI. This guide defaults to using Containerd as the container runtime. For more information on installing Kubernetes, see the official Kubernetes documentation . Disable Swap \u00b6 Kubernetes refuses to run if swap is enabled on the node, so we disable swap immediately and then also disable it following a reboot: sudo swapoff -a sudo sed -i '/\\sswap\\s/ s/^\\(.*\\)$/#\\1/g' /etc/fstab Install Kubernetes and suplemental libraries \u00b6 Add the Kubernetes APT repository: sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt update Accelleran dRAX currently supports Kubernetes up to version 1.24. The following command installs specifically this (1.24) version of k8s together with containerd end suplemental libraries: sudo apt-get install -y runc libc6 containerd kubelet = 1 .24.0-00 kubeadm = 1 .24.0-00 kubectl = 1 .24.0-00 sudo apt-mark hold kubelet kubeadm kubectl Configure Containerd \u00b6 sudo rm -rf /etc/containerd/config.toml sudo systemctl restart containerd.service sudo systemctl enable containerd.service sudo sysctl -p Letting IPTables See Bridged Traffic and IP Forwarding Enabling \u00b6 In order k8s nodes can see bringed traffic properly, iptables should be configured to allow bringed traffic together with enabled ip-forwarding. echo \"net.bridge.bridge-nf-call-iptables=1\" | sudo tee -a /etc/sysctl.conf echo \"net.ipv4.ip_forward=1\" | sudo tee -a /etc/sysctl.conf echo \"net.ipv6.conf.all.forwarding=1\" | sudo tee -a /etc/sysctl.conf sudo modprobe br_netfilter Configure Kubernetes \u00b6 To initialize the Kubernetes cluster, the IP address of the node needs to be fixed, i.e. if this IP changes, a full re-installation of Kubernetes will be required. This is generally the (primary) IP address of the network interface associated with the default gateway. From here on, this IP is referred to as $NODE_IP - we store it as an environment variable for later use: export NODE_IP = 1 .2.3.4 # replace 1.2.3.4 with the correct IP This guide assumes we will use Flannel as the CNI-based Pod network for this Kubernetes instance, which uses the 10.244.0.0/16 subnet by default. We store it again as an environment variable for later use, and of course if you wish to use a different subnet, change the command accordingly: export POD_NETWORK = 10 .244.0.0/16 The following command initializes the cluster on this node: sudo kubeadm init --pod-network-cidr = $POD_NETWORK --apiserver-advertise-address = $NODE_IP If this succeeds, we should see information for joining other worker nodes to this cluster. We won't do that at this point, but it's a sign that the command completed successfully. To make kubectl work for our non-root user, run the following commands: mkdir -p \" $HOME /.kube\" sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown \" $( id -u ) : $( id -g ) \" \" $HOME /.kube/config\" Install Flannel \u00b6 Prepare the Manifest file: curl -sSOJ https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml sed -i '/net-conf.json/,/}/{ s#10.244.0.0/16#' \" $POD_NETWORK \" '#; }' kube-flannel.yml Apply the Manifest file: kubectl apply -f kube-flannel.yml Enable Pod Scheduling \u00b6 By default, Kubernetes will not schedule Pods on the control-plane node for security reasons. As we're running with a single Node, we need to remove the node-role.kubernetes.io/control-plane- taint, meaning that the scheduler will then be able to schedule Pods on it. kubectl taint node --all node-role.kubernetes.io/control-plane- Deploy Longhorn as the Default Persistent Storage \u00b6 First create the longhorn-system namespace: kubectl create namespace longhorn-system Then add the longhorn repository to helm: helm repo add longhorn https://charts.longhorn.io Finally deploy Longhorn in \"longhorn-system\" namespace, with all replica counts set to 1 and UI exposed on port 32100 via NodePort instead of ClusterIp: helm install longhorn longhorn/longhorn --version 1 .3.1 \\ --set service.ui.type = NodePort, \\ service.ui.nodePort = 32100 , \\ persistence.defaultClassReplicaCount = 1 , \\ csi.attacherReplicaCount = 1 , \\ csi.provisionerReplicaCount = 1 , \\ csi.resizerReplicaCount = 1 , \\ csi.snapshotterReplicaCount = 1 , \\ defaultSettings.defaultReplicaCount = 1 \\ -n longhorn-system A small busybox pod for testing \u00b6 It is very convenient (however optional) to test the Kubernetes installation with a simple busybox pod for instance to test your DNS resolution inside a pod. To do so create the following yaml file (/tmp/busybox.yaml): cat << EOF > /tmp/busybox.yaml apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: - name: busybox image: busybox:1.28 command: - sleep - \"3600\" imagePullPolicy: IfNotPresent restartPolicy: Always EOF Then you can create the pod: NOTE : --kubeconfig is optional here because $HOME/.kube/config is the default config file kubectl --kubeconfig $HOME /.kube/config create -f /tmp/busybox.yaml If all went well a new POD was created, you can verify this with the following command kubectl --kubeconfig $HOME /.kube/config get pods #NAME READY STATUS RESTARTS AGE #busybox 1/1 Running 21 21h In order to verify if your Kubernetes is working correctly you could try some simple commands using the busybox POD. For instance to verify your name resolution works do: kubectl exec -ti busybox -- nslookup mirrors.ubuntu.com #Server: 10.96.0.10 #Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local #Name: mirrors.ubuntu.com #Address 1: 91.189.89.32 bilimbi.canonical.com Remove in full a Kubernetes installation \u00b6 On occasion, it may be deemed necessary to fully remove Kubernetes, for instance if for any reason your server IP address will change, then the advertised Kubernetes IP address will have to follow. THe following command help making sure the previous installation is cleared up: sudo kubeadm reset sudo apt-get purge kubeadm kubectl kubelet kubernetes-cni kube* sudo rm -rf ~/.kube sudo rm -rf /etc/cni/net.d sudo ip link delete cni0 sudo ip link delete flannel.1","title":"Kubernetes Installation (v1.24)"},{"location":"kubernetes-install-1-24/#kubernetes-installation-v124","text":"This chapter will install Kubernetes, using Flannel for the CNI. This guide defaults to using Containerd as the container runtime. For more information on installing Kubernetes, see the official Kubernetes documentation .","title":"Kubernetes Installation (v1.24)"},{"location":"kubernetes-install-1-24/#disable-swap","text":"Kubernetes refuses to run if swap is enabled on the node, so we disable swap immediately and then also disable it following a reboot: sudo swapoff -a sudo sed -i '/\\sswap\\s/ s/^\\(.*\\)$/#\\1/g' /etc/fstab","title":"Disable Swap"},{"location":"kubernetes-install-1-24/#install-kubernetes-and-suplemental-libraries","text":"Add the Kubernetes APT repository: sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt update Accelleran dRAX currently supports Kubernetes up to version 1.24. The following command installs specifically this (1.24) version of k8s together with containerd end suplemental libraries: sudo apt-get install -y runc libc6 containerd kubelet = 1 .24.0-00 kubeadm = 1 .24.0-00 kubectl = 1 .24.0-00 sudo apt-mark hold kubelet kubeadm kubectl","title":"Install Kubernetes and suplemental libraries"},{"location":"kubernetes-install-1-24/#configure-containerd","text":"sudo rm -rf /etc/containerd/config.toml sudo systemctl restart containerd.service sudo systemctl enable containerd.service sudo sysctl -p","title":"Configure Containerd"},{"location":"kubernetes-install-1-24/#letting-iptables-see-bridged-traffic-and-ip-forwarding-enabling","text":"In order k8s nodes can see bringed traffic properly, iptables should be configured to allow bringed traffic together with enabled ip-forwarding. echo \"net.bridge.bridge-nf-call-iptables=1\" | sudo tee -a /etc/sysctl.conf echo \"net.ipv4.ip_forward=1\" | sudo tee -a /etc/sysctl.conf echo \"net.ipv6.conf.all.forwarding=1\" | sudo tee -a /etc/sysctl.conf sudo modprobe br_netfilter","title":"Letting IPTables See Bridged Traffic and IP Forwarding Enabling"},{"location":"kubernetes-install-1-24/#configure-kubernetes","text":"To initialize the Kubernetes cluster, the IP address of the node needs to be fixed, i.e. if this IP changes, a full re-installation of Kubernetes will be required. This is generally the (primary) IP address of the network interface associated with the default gateway. From here on, this IP is referred to as $NODE_IP - we store it as an environment variable for later use: export NODE_IP = 1 .2.3.4 # replace 1.2.3.4 with the correct IP This guide assumes we will use Flannel as the CNI-based Pod network for this Kubernetes instance, which uses the 10.244.0.0/16 subnet by default. We store it again as an environment variable for later use, and of course if you wish to use a different subnet, change the command accordingly: export POD_NETWORK = 10 .244.0.0/16 The following command initializes the cluster on this node: sudo kubeadm init --pod-network-cidr = $POD_NETWORK --apiserver-advertise-address = $NODE_IP If this succeeds, we should see information for joining other worker nodes to this cluster. We won't do that at this point, but it's a sign that the command completed successfully. To make kubectl work for our non-root user, run the following commands: mkdir -p \" $HOME /.kube\" sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown \" $( id -u ) : $( id -g ) \" \" $HOME /.kube/config\"","title":"Configure Kubernetes"},{"location":"kubernetes-install-1-24/#install-flannel","text":"Prepare the Manifest file: curl -sSOJ https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml sed -i '/net-conf.json/,/}/{ s#10.244.0.0/16#' \" $POD_NETWORK \" '#; }' kube-flannel.yml Apply the Manifest file: kubectl apply -f kube-flannel.yml","title":"Install Flannel"},{"location":"kubernetes-install-1-24/#enable-pod-scheduling","text":"By default, Kubernetes will not schedule Pods on the control-plane node for security reasons. As we're running with a single Node, we need to remove the node-role.kubernetes.io/control-plane- taint, meaning that the scheduler will then be able to schedule Pods on it. kubectl taint node --all node-role.kubernetes.io/control-plane-","title":"Enable Pod Scheduling"},{"location":"kubernetes-install-1-24/#deploy-longhorn-as-the-default-persistent-storage","text":"First create the longhorn-system namespace: kubectl create namespace longhorn-system Then add the longhorn repository to helm: helm repo add longhorn https://charts.longhorn.io Finally deploy Longhorn in \"longhorn-system\" namespace, with all replica counts set to 1 and UI exposed on port 32100 via NodePort instead of ClusterIp: helm install longhorn longhorn/longhorn --version 1 .3.1 \\ --set service.ui.type = NodePort, \\ service.ui.nodePort = 32100 , \\ persistence.defaultClassReplicaCount = 1 , \\ csi.attacherReplicaCount = 1 , \\ csi.provisionerReplicaCount = 1 , \\ csi.resizerReplicaCount = 1 , \\ csi.snapshotterReplicaCount = 1 , \\ defaultSettings.defaultReplicaCount = 1 \\ -n longhorn-system","title":"Deploy Longhorn as the Default Persistent Storage"},{"location":"kubernetes-install-1-24/#a-small-busybox-pod-for-testing","text":"It is very convenient (however optional) to test the Kubernetes installation with a simple busybox pod for instance to test your DNS resolution inside a pod. To do so create the following yaml file (/tmp/busybox.yaml): cat << EOF > /tmp/busybox.yaml apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: - name: busybox image: busybox:1.28 command: - sleep - \"3600\" imagePullPolicy: IfNotPresent restartPolicy: Always EOF Then you can create the pod: NOTE : --kubeconfig is optional here because $HOME/.kube/config is the default config file kubectl --kubeconfig $HOME /.kube/config create -f /tmp/busybox.yaml If all went well a new POD was created, you can verify this with the following command kubectl --kubeconfig $HOME /.kube/config get pods #NAME READY STATUS RESTARTS AGE #busybox 1/1 Running 21 21h In order to verify if your Kubernetes is working correctly you could try some simple commands using the busybox POD. For instance to verify your name resolution works do: kubectl exec -ti busybox -- nslookup mirrors.ubuntu.com #Server: 10.96.0.10 #Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local #Name: mirrors.ubuntu.com #Address 1: 91.189.89.32 bilimbi.canonical.com","title":"A small busybox pod for testing"},{"location":"kubernetes-install-1-24/#remove-in-full-a-kubernetes-installation","text":"On occasion, it may be deemed necessary to fully remove Kubernetes, for instance if for any reason your server IP address will change, then the advertised Kubernetes IP address will have to follow. THe following command help making sure the previous installation is cleared up: sudo kubeadm reset sudo apt-get purge kubeadm kubectl kubelet kubernetes-cni kube* sudo rm -rf ~/.kube sudo rm -rf /etc/cni/net.d sudo ip link delete cni0 sudo ip link delete flannel.1","title":"Remove in full a Kubernetes installation"}]}